{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les Embeddings de mots\n",
    "\n",
    "J'ai entrain√© des embeddings en fran√ßais √† partir des donn√©es du corpus OSCAR <span class=\"badge badge-secondary\">([Su√°rez et al., 2019](#suarez-2019))</span> (886.47 M de phrases qui correspondent √† 21.66 B de mots). La taille du vocabulaire est de 2.9 M de mots. Les embeddings sont entrain√©s avec l'algorithme Word2Vec <span class=\"badge badge-secondary\">([Mikolov et al., 2013a](#mikolov-2013a), [2013b](#mikolov-2013b))</span>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# ‚ö†Ô∏è Execute only if running in Colab\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  IN_COLAB = True\n",
    "else:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "  !pip install gensim==3.8.1 scikit-learn==0.23.2 matplotlib==3.3.2\n",
    "  # !pip install matplotlib==3.1.3\n",
    "  # puis red√©marer l'environnement d'√©x√©cution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# IPython automatically reload all changed code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "\n",
    "!wget --no-check-certificate -r 'https://docs.google.com/uc?export=download&id=1qgd_9RDj_nOKeoCBHKAL_PvqT_d5E7OL' -O oscar.fr.300.model.zip\n",
    "!unzip oscar.fr.300.model.zip\n",
    "# !unzip oscar.fr.300.model.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget --no-check-certificate -r 'https://docs.google.com/uc?export=download&id=1qgd_9RDj_nOKeoCBHKAL_PvqT_d5E7OL' -O oscar.fr.300.model.zip\n",
    "# !unzip oscar.fr.300.model.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load(\"oscar.fr.300.model\")\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(w2v_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model = KeyedVectors.load_word2vec_format(\"oscar.fr.300.10k.model\")\n",
    "# w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On cr√©e un array avec les 10,000 premiers mots et on cr√©e le dictionaire de vocabulaire\n",
    "\n",
    "word_count = {k: w2v_model.wv.vocab[k].count for k in w2v_model.wv.vocab}\n",
    "word_count = Counter(word_count)\n",
    "word_count.most_common(10)\n",
    "\n",
    "idx2w = {i: w for (i, (w, f)) in enumerate(word_count.most_common(10000))}\n",
    "w2idx = {w: i for (i, (w, f)) in enumerate(word_count.most_common(10000))}\n",
    "\n",
    "\n",
    "embeddings_vectors = [w2v_model.wv[w] for (w, f) in word_count.most_common(10000)]\n",
    "word2vec_embeddings = np.vstack(embeddings_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word2vec_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv['roi'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv['roi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>üìù Exercice :</b> Le mot 'dotcteur' n'a pas d'embeddings associ√©, pourquoi ? Est-il possible de le rajouter ?</p>\n",
    "</div>\n",
    "<hr> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv['dotcteur']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>Exercice :</b> S'assurer que les embeddings sont bien normalis√©s (la norme de chauqe vecteur vaut 1).</p>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(w2v_model.wv['avion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration de l'espace de repr√©sentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>üìù Exercice :</b> Ecrire une fonction qui renvoie les n mots les plus similaires √©tant donn√© un query. Pour cela, on calculera la similarit√© cosinus entre le query et les autres mots. On affichera les n mots avec la similarit√© cosinus la plus √©lev√©e.</p>\n",
    "</div>\n",
    "<hr> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/most_similar.py\n",
    "def most_similar(query, word2vec_embeddings, w2idx, idx2w, topn=10):\n",
    "    # TODO compl√©ter la fonction qui renvoie les topn mots les plus proches du query\n",
    "    # dans le vocabulaire.\n",
    "    most_similar_words = []\n",
    "    return most_similar_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2idx.get(\"paris\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_embeddings[2480].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(word2vec_embeddings, word2vec_embeddings[2480])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(\"paris\", word2vec_embeddings, w2idx, idx2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"paris\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"Paris\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"cpu\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"voiture\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"1\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\":)\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\":(\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"ordinateur\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relations entre les mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/Cours%202%20-%20Embeddings/figures/w2v.png?raw=true\" width=\"1000\">\n",
    "\n",
    "Source: https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"roi\", \"femme\"], negative=[\"homme\"], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ca marche pour les relations masculin / f√©minin\n",
    "w2v_model.wv.most_similar(positive=[\"infirmi√®re\", \"homme\"], negative=[\"femme\"], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"Berlin\", \"France\"], negative=[\"Paris\"], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"EtatsUnis\", \"Paris\"], negative=[\"France\"], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"Chine\", \"Paris\"], negative=[\"France\"], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"Espagne\", \"Paris\"], negative=[\"France\"], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ca marche aussi pour les abr√©viations\n",
    "w2v_model.wv.most_similar(positive=[\"NY\", \"EtatsUnis\"], negative=[\"NewYork\"], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation avec le t-SNE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La m√©thode [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) (t-distributed stochastic neighbor embedding) <span class=\"badge badge-secondary\">([Maaten et al., 2008](#maaten-2008))</span> est une m√©thode statistique pour visualiser des donn√©es de grandes dimensions dans un espace en 2D ou 3D. La m√©thode s'appuie sur une r√©duction de dimension non lin√©aire. Elle cherche √† rapprocher la repr√©sentation en 2D des objets proches dans l'espace original tout en √©loignant les objets √©loign√©s. Pour cela on d√©finit une distribution de probabilit√© sur l'espace original en grande dimension, telle que les objets similaires sont associ√©s √† des probabilit√©s √©lev√©es et les objets diff√©rents sont associ√©s √† des probabilit√©s faibles. On d√©finit une distribution de probabilit√© aux probabilit√©s similaires dans l'espace de repr√©sentation. On cherche ensuite √† minimiser la distance entre les deux distributions vis √† vis des repr√©sentations dans l'espace en 2D ou 3D. On mesure la distance entre les deux distributions √† l'aide de la [Kullback‚ÄìLeibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) (KL divergence). La version originale de la m√©thode utilise la distance euclidienne pour mesurer la distance entre deux points. La t-SNE est souvent utilis√©e pour explorer les donn√©es et interpr√©ter des _clusters_. N√©anmoins les clusters constitu√©s d√©pendent en grande partie de la configuration utilis√©e et peuvent donc ne pas refl√©ter les sp√©cificit√©s de la distribution originale des donn√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Inline Figures with matplotlib\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word_emb_tsne = TSNE(perplexity=30).fit_transform(word2vec_embeddings[:1000])\n",
    "\n",
    "plt.figure(figsize=(40, 40))\n",
    "axis = plt.gca()\n",
    "np.set_printoptions(suppress=True)\n",
    "plt.scatter(word_emb_tsne[:, 0], word_emb_tsne[:, 1], marker=\".\", s=1)\n",
    "\n",
    "for idx in range(1000):\n",
    "    plt.annotate(idx2w[idx],\n",
    "                 xy=(word_emb_tsne[idx, 0], word_emb_tsne[idx, 1]),\n",
    "                 xytext=(0, 0), textcoords='offset points')\n",
    "plt.savefig(\"tsne.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>üìù Exercice :</b>Visualisez la projection des pays et leur capitales</p>\n",
    "</div>\n",
    "<hr>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# word_emb_tsne = TSNE(perplexity=30).fit_transform(word2vec_embeddings[:10000])\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "axis = plt.gca()\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "pays = ['Allemagne', 'France', 'Belgique', 'Madrid', 'Londres']\n",
    "capitales = ['Berlin', 'Paris', 'Bruxelles', 'Espagne', 'Angleterre']\n",
    "\n",
    "x = [word_emb_tsne[w2idx[w], 0] for w in pays] + [word_emb_tsne[w2idx[w], 0] for w in capitales]\n",
    "y = [word_emb_tsne[w2idx[w], 1] for w in pays] + [word_emb_tsne[w2idx[w], 1] for w in capitales]\n",
    "\n",
    "plt.scatter(x, y, marker=\".\", s=1)\n",
    "\n",
    "for p in pays:\n",
    "    plt.annotate(p,\n",
    "                 xy=(word_emb_tsne[w2idx[p], 0], word_emb_tsne[w2idx[p], 1]),\n",
    "                 xytext=(0, 0), textcoords='offset points')\n",
    "\n",
    "for c in capitales:\n",
    "    plt.annotate(c,\n",
    "                 xy=(word_emb_tsne[w2idx[c], 0], word_emb_tsne[w2idx[c], 1]),\n",
    "                 xytext=(0, 0), textcoords='offset points')\n",
    "\n",
    "for c, p in zip(pays, capitales):\n",
    "    plt.annotate(\"\",\n",
    "                 xy=(word_emb_tsne[w2idx[c], 0], word_emb_tsne[w2idx[c], 1]),\n",
    "                 xytext=(word_emb_tsne[w2idx[p], 0], word_emb_tsne[w2idx[p], 1]),\n",
    "                 arrowprops=dict(arrowstyle=\"->\"))\n",
    "\n",
    "plt.xlim(min(x) - 5, max(x) + 5)\n",
    "plt.ylim(min(y) - 5, max(y) + 5)\n",
    "plt.savefig(\"tsne-pays-capitales.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>üìù Exercice :</b>Visualisez la projection de verbes √† l'infinitif ou conjugu√©s</p>\n",
    "</div>\n",
    "<hr>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infinitif = ['manger', 'lancer', 'jouer', 'cuisiner', 'sortir']\n",
    "conjugaison = ['mange', 'lance', 'joue', 'cuisine', 'sors']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "<hr>\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>üìù Exercice :</b>Comment interpr√©tez vous la repr√©sentation du mot cuisine ?</p>\n",
    "</div>\n",
    "<hr>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö References\n",
    "\n",
    "> <div id=\"suarez-2019\">Su√°rez, Pedro Javier Ortiz, Beno√Æt Sagot, and Laurent Romary. <a href=https://hal.inria.fr/hal-02148693>Asynchronous pipeline for processing huge corpora on medium to low resource infrastructures.</a> 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7). Leibniz-Institut f√ºr Deutsche Sprache, 2019.</div>\n",
    "\n",
    "> <div id=\"mikolov-2013a\">Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean. <a href=http://arxiv.org/abs/1301.3781>Efficient Estimation of Word Representations in Vector Space.</a> ICLR (Workshop Poster) 2013.</div>\n",
    "\n",
    "> <div id=\"mikolov-2013b\">Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, Jeffrey Dean. <a href=https://hal.inria.fr/hal-02148693>Distributed Representations of Words and Phrases and their Compositionality.</a> NIPS 2013: 3111-3119.</div>\n",
    "\n",
    "> <div id=\"maaten-2008\">Van der Maaten, Laurens, and Geoffrey Hinton. <a href=https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf?fbclid=IwA>Visualizing data using t-SNE.</a> Journal of machine learning research 9.11 (2008).</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copyright 2021 Antoine SIMOULIN.\n",
    "\n",
    "<i>Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "Icons made by <a href=\"https://www.flaticon.com/authors/freepik\" title=\"Freepik\">Freepik</a>, <a href=\"https://www.flaticon.com/authors/pixel-perfect\" title=\"Pixel perfect\">Pixel perfect</a>, <a href=\"https://www.flaticon.com/authors/becris\" title=\"Becris\">Becris</a>, <a href=\"https://www.flaticon.com/authors/smashicons\" title=\"Smashicons\">Smashicons</a>, <a href=\"https://www.flaticon.com/authors/srip\" title=\"srip\">srip</a>, <a href=\"https://www.flaticon.com/authors/adib-sulthon\" title=\"Adib\">Adib</a>, <a href=\"https://www.flaticon.com/authors/flat-icons\" title=\"Flat Icons\">Flat Icons</a> and <a href=\"https://www.flaticon.com/authors/dinosoftlabs\" title=\"Pixel perfect\">DinosoftLabs</a> from <a href=\"https://www.flaticon.com/\" title=\"Flaticon\"> www.flaticon.com</a></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
