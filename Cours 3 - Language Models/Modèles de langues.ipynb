{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copyright 2021 Antoine Simoulin.\n",
    "\n",
    "<i>Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "Icons made by <a href=\"https://www.flaticon.com/authors/freepik\" title=\"Freepik\">Freepik</a>, <a href=\"https://www.flaticon.com/authors/pixel-perfect\" title=\"Pixel perfect\">Pixel perfect</a>, <a href=\"https://www.flaticon.com/authors/becris\" title=\"Becris\">Becris</a>, <a href=\"https://www.flaticon.com/authors/smashicons\" title=\"Smashicons\">Smashicons</a>, <a href=\"https://www.flaticon.com/authors/srip\" title=\"srip\">srip</a>, <a href=\"https://www.flaticon.com/authors/adib-sulthon\" title=\"Adib\">Adib</a>, <a href=\"https://www.flaticon.com/authors/flat-icons\" title=\"Flat Icons\">Flat Icons</a> and <a href=\"https://www.flaticon.com/authors/dinosoftlabs\" title=\"Pixel perfect\">DinosoftLabs</a> from <a href=\"https://www.flaticon.com/\" title=\"Flaticon\"> www.flaticon.com</a></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice Models de langue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Check environment\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  IN_COLAB = True\n",
    "else:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "  # ‚ö†Ô∏è Execute only if running in Colab\n",
    "  !pip install -q transformers==3.1.0\n",
    "  !pip install -q tensorflow==2.0.0\n",
    "  # then restart runtime environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1: les mod√®les de langues de types N-gram\n",
    "\n",
    "Dans ce premier exercice, nous allons impl√©menter un mod√®le de langue de type N-gram pour construire un **syst√®me d'auto-compl√©tion**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous l'avons vu en cours, les mod√®les de langues n-grams cherchent √† estimer la probabilit√© conditionnelle d'un mot $t$ dans la phrase √©tant donn√© les $n$ mots pr√©c√©dents $w_{t-1}, w_{t-2} \\cdots w_{t-n}$ : \n",
    "\n",
    "$$ P(w_t | w_{t-1}\\dots w_{t-n}) \\tag{1}$$\n",
    "\n",
    "On estime cette probabilit√© avec $\\hat{P}$ en comptant les occurrences des sequences de mots dans les donn√©es d'entrainement :\n",
    "\n",
    "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n)}{C(w_{t-1}\\dots w_{t-n})} \\tag{2} $$\n",
    "\n",
    "Avec $C(\\cdots)$ le nombre d'occurrences d'une s√©quence de mots donn√©e. En pratique, le d√©nominateur peut √™tre nul. On va ajouter un param√®tre de smoothing. On ajoute une constate $k$ au num√©rateur et $k \\times |V|$ au d√©nominateur avec $|V|$ la taille du vocabulaire. On a donc :\n",
    "\n",
    "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n) + k}{C(w_{t-1}\\dots w_{t-n}) + k|V|} \\tag{3} $$\n",
    "\n",
    "Si on a un n-grams qui n'apparait pas, l'√©quation (3) devient donc $\\frac{1}{|V|}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [['je', 'suis', 'en', 'vacances'],\n",
    "             ['je', 'vais', 'partir', '√†', 'la', 'r√©union'],\n",
    "             ['je', 'suis', 'en', 'r√©union'],\n",
    "             ['je', 'vais', 'partir', 'en', 'vacances']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = list(set(sentences[0] + sentences[1] + sentences[2] + sentences[3]))\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>üìù Exercice :</b> Ecrire une fonction qui g√©n√®re tous les n-grams d'une phrase avec n un param√®tre de la fonction.</p>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/ngrams.py\n",
    "\n",
    "def sentence_2_n_grams(sentences, n=3, start_token='<s>', end_token='</s>'):\n",
    "    \n",
    "    # TODO Complete function\n",
    "    \n",
    "    return n_grams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_counts = sentence_2_n_grams(sentences, 1)\n",
    "print(\"Uni-gram:\")\n",
    "print(unigram_counts)\n",
    "\n",
    "bigram_counts = sentence_2_n_grams(sentences, 2)\n",
    "print(\"\\nBi-gram:\")\n",
    "print(bigram_counts)\n",
    "\n",
    "trigram_counts = sentence_2_n_grams(sentences, 3)\n",
    "print(\"\\nTri-gram:\")\n",
    "print(trigram_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>üìù Exercice :</b> Ecrire une fonction qui calcule la probabilit√© d'un mot en fonction des ngrams pr√©c√©dents.</p>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/estimate_proba.py\n",
    "def estimate_probability(word, previous_n_gram,\n",
    "                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "    \n",
    "    # TODO, complete word probability\n",
    "\n",
    "    probability = 0\n",
    "\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_1 = \"je vais\"\n",
    "word_2 = \"partir\"\n",
    "tmp_prob = estimate_probability(word_2, word_1, bigram_counts, trigram_counts, len(unique_words), k=1)\n",
    "\n",
    "print(\"La probabilit√© du mot '{}' √©tant donn√© le pr√©c√©dent n-gram '{}' est : {:.3f}.\"\n",
    "      .format(word_2, word_1, tmp_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0,\n",
    "                           start_token='<s>', end_token='</s>', unk_token='<unk>'):\n",
    "    \n",
    "    # On ajoute end_token et unk_token to the vocabulary\n",
    "    # start_token ne peut pas apparaitre comme mot suivant donc pas besoin de l'ajouter\n",
    "    vocabulary = vocabulary + [end_token, unk_token]\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    \n",
    "    probabilities = {}\n",
    "    for word in vocabulary:\n",
    "        probability = estimate_probability(word, previous_n_gram, \n",
    "                                           n_gram_counts, n_plus1_gram_counts, \n",
    "                                           vocabulary_size, k=k)\n",
    "        probabilities[word] = probability\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word_proba = estimate_probabilities(\"je\", unigram_counts, bigram_counts, unique_words, k=1)\n",
    "\n",
    "for w, p in next_word_proba.items():\n",
    "    print(\"La probabilit√© du mot '{}' √©tant donn√© le pr√©c√©dent n-gram '{}' est : {:.3f}.\"\n",
    "          .format(w, 'je', p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_probabilities(\"en\", bigram_counts, trigram_counts, unique_words, k=1)\n",
    "\n",
    "for w, p in next_word_proba.items():\n",
    "    print(\"La probabilit√© du mot '{}' √©tant donn√© le pr√©c√©dent n-gram '{}' est : {:.3f}.\"\n",
    "          .format(w, 'en', p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_count_matrix(n_plus1_gram_counts, vocabulary,\n",
    "                      start_token='<s>', end_token='</s>', unk_token='<unk>'):\n",
    " \n",
    "    vocabulary = vocabulary + [end_token, unk_token]\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    \n",
    "    # obtain unique n-grams\n",
    "    n_grams = list(n_plus1_gram_counts.keys())\n",
    "    \n",
    "    row_index = {n_gram: i for i, n_gram in enumerate(n_grams)}\n",
    "    col_index = {word: j for j, word in enumerate(vocabulary)}\n",
    "    \n",
    "    nrow = len(n_grams)\n",
    "    ncol = len(vocabulary)\n",
    "    count_matrix = np.zeros((nrow, ncol))\n",
    "    \n",
    "    for n_plus1_gram, count in n_plus1_gram_counts.items():\n",
    "        n_gram = n_plus1_gram\n",
    "        word = n_plus1_gram.split()[-1]\n",
    "        if word not in vocabulary:\n",
    "            continue\n",
    "        i = row_index[n_gram]\n",
    "        j = col_index[word]\n",
    "        count_matrix[i, j] = count\n",
    "    \n",
    "    count_matrix = pd.DataFrame(count_matrix, index=[' '.join(ng.split()[0:-1]) for ng in n_grams], columns=vocabulary)\n",
    "    return count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [['je', 'suis', 'en', 'vacances'],\n",
    "             ['je', 'vais', 'partir', '√†', 'la', 'r√©union'],\n",
    "             ['je', 'suis', 'en', 'r√©union'],\n",
    "             ['je', 'vais', 'partir', 'en', 'vacances']]\n",
    "\n",
    "display(make_count_matrix(bigram_counts, unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show trigram counts\n",
    "display(make_count_matrix(trigram_counts, unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n",
    "    count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)\n",
    "    count_matrix += k\n",
    "    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n",
    "    return prob_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(make_probability_matrix(bigram_counts, unique_words, k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(make_probability_matrix(trigram_counts, unique_words, k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
    "    \n",
    "    n = len(list(n_gram_counts.keys())[0].split()) \n",
    "    previous_n_gram = ' '.join(previous_tokens.split()[-n:])\n",
    "    probabilities = estimate_probabilities(previous_n_gram,\n",
    "                                           n_gram_counts, n_plus1_gram_counts,\n",
    "                                           vocabulary, k=k)\n",
    "\n",
    "    suggestion = None\n",
    "    max_prob = 0\n",
    "    for word, prob in probabilities.items(): \n",
    "        if prob > max_prob: \n",
    "            suggestion = word            \n",
    "            max_prob = prob    \n",
    "    return suggestion, max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_tokens = \"je vais\"\n",
    "tmp_suggest1 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)\n",
    "print(f\"Pour les tokens 'je vais',la suggestion est le mot '{tmp_suggest1[0]}' avec une probabilit√© de {tmp_suggest1[1]:.4f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut calculer la perplexit√© pour √©valuer le mod√®le. Cette derni√®re est donn√©e par :\n",
    "\n",
    "$$ PP(W) =\\sqrt[N]{ \\prod_{t=n+1}^N \\frac{1}{P(w_t | w_{t-n} \\cdots w_{t-1})} } \\tag{4}$$\n",
    "\n",
    "Avec $N$ la longueur de la phrase et $n$ la taille des n-grams (par exemple 2 dans le cas des bigrams). On cherche √† minimiser la perplexit√© du mod√®le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0,\n",
    "                         start_token='<s>', end_token='</s>', unk_token='<unk>'):\n",
    "    \n",
    "    n = len(list(n_gram_counts.keys())[0].split()) \n",
    "    tokens = [start_token] + sentence + [end_token]\n",
    "    N = len(tokens)\n",
    "    \n",
    "    product_pi = 1.0\n",
    "  \n",
    "    for t in range(n, N): \n",
    "        n_gram = tokens[t-n:t]\n",
    "        word = tokens[t]\n",
    "        \n",
    "        probability = estimate_probability(word, ' '.join(n_gram), \n",
    "                                           n_gram_counts, n_plus1_gram_counts, \n",
    "                                           len(unique_words), k=1)\n",
    "        product_pi *= 1 / probability\n",
    "\n",
    "    perplexity = product_pi**(1/float(N))\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_train1 = calculate_perplexity(sentences[0],\n",
    "                                         unigram_counts, bigram_counts,\n",
    "                                         len(unique_words), k=1.0)\n",
    "print(f\"La perplexit√© pour la premi√®re phrase du corpus est : {perplexity_train1:.4f}.\")\n",
    "\n",
    "\n",
    "perplexity_train1 = calculate_perplexity(['Tu' ,'pars', 'ou', 'en', 'vacances', '?'],\n",
    "                                         unigram_counts, bigram_counts,\n",
    "                                         len(unique_words), k=1.0)\n",
    "print(f\"La perplexit√© pour la phrase test est : {perplexity_train1:.4f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2: les mod√®les de langues avec r√©seaux de neurones : GPT-2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'ai entrain√© un mod√®le GPT-2 <span class=\"badge badge-secondary\">([Radford et al., 2019](#radford-2019))</span> sur 50M de phrases extraites du corpus OSCAR <span class=\"badge badge-secondary\">([Su√°rez et al., 2019](#suarez-2019))</span>. Le mod√®le a ensuite √©t√© fine-tun√© (on a continu√© l'entrainement) sur le Tome 2 de Harry Potter. Ce mod√®le est une architecture de r√©seaux de neurones assez connu pour les mod√®les de langues. Il permet de g√©n√©rer du texte de mani√®re assez r√©aliste. On va utiliser la librairie `transformers` pour utiliser le mod√®le. **Vous devez r√©cup√©rer les poids du mod√®les sur le moodle (fichier french-gpt2-hp)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./french-gpt2-hp\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"./french-gpt2-hp\", \n",
    "                                          pad_token_id=tokenizer.eos_token_id, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(\n",
    "    \"Dans son mouvement, la queue du Basilic lui avait jet√© le Choixpeau magique √† la t√™te. \",\n",
    "    return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = model.generate(input_ids, max_length=50)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids, \n",
    "    max_length=200, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(\n",
    "    \"Assis un peu plus loin, Harry reconnut Gilderoy Lockhart, v√™tu d'une robe de sorcier bleu-vert.\",\n",
    "    return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids, \n",
    "    max_length=200, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö References\n",
    "\n",
    "> <div id=\"radford-2019\">Alec Radford, Jeffrey Wu, Rewon Child, David Luan and Dario Amodei. <a href=https://openai.com/blog/better-language-models/> Better Language Models and Their Implications.</a></div>\n",
    "\n",
    "> <div id=\"suarez-2019\">Su√°rez, Pedro Javier Ortiz, Beno√Æt Sagot, and Laurent Romary. <a href=https://hal.inria.fr/hal-02148693> Asynchronous pipeline for processing huge corpora on medium to low resource infrastructures.</a> 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7). Leibniz-Institut f√ºr Deutsche Sprache, 2019.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
