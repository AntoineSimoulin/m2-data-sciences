{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copyright 2021 Antoine Simoulin.\n",
    "\n",
    "<i>Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "Icons made by <a href=\"https://www.flaticon.com/authors/freepik\" title=\"Freepik\">Freepik</a>, <a href=\"https://www.flaticon.com/authors/pixel-perfect\" title=\"Pixel perfect\">Pixel perfect</a>, <a href=\"https://www.flaticon.com/authors/becris\" title=\"Becris\">Becris</a>, <a href=\"https://www.flaticon.com/authors/smashicons\" title=\"Smashicons\">Smashicons</a>, <a href=\"https://www.flaticon.com/authors/srip\" title=\"srip\">srip</a>, <a href=\"https://www.flaticon.com/authors/adib-sulthon\" title=\"Adib\">Adib</a>, <a href=\"https://www.flaticon.com/authors/flat-icons\" title=\"Flat Icons\">Flat Icons</a> and <a href=\"https://www.flaticon.com/authors/dinosoftlabs\" title=\"Pixel perfect\">DinosoftLabs</a> from <a href=\"https://www.flaticon.com/\" title=\"Flaticon\"> www.flaticon.com</a></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice Models de langue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Check environment\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  IN_COLAB = True\n",
    "else:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "  # ‚ö†Ô∏è Execute only if running in Colab\n",
    "  !pip install -q transformers==3.1.0\n",
    "  !pip install -q tensorflow==2.0.0\n",
    "  # then restart runtime environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1: les mod√®les de langues de types N-gram\n",
    "\n",
    "Dans ce premier exercice, nous allons impl√©menter un mod√®le de langue de type N-gram pour construire un **syst√®me d'auto-compl√©tion**. Ce type de syst√®me est utilis√© dans Google pour proposer de compl√©ter les queries de recherche ou pour la r√©daction des textos pour proposer le mot suivant par exemple.\n",
    "\n",
    "<img src = \"autocomplete.png\" style=\"width:500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous l'avons vu en cours, les mod√®les de langues n-grams cherchent √† estimer la probabilit√© conditionnelle d'un mot $t$ dans la phrase √©tant donn√© les $n$ mots pr√©c√©dents $w_{t-1}, w_{t-2} \\cdots w_{t-n}$ : \n",
    "\n",
    "$$ P(w_t | w_{t-1}\\dots w_{t-n}) \\tag{1}$$\n",
    "\n",
    "On estime cette probabilit√© avec $\\hat{P}$ en comptant les occurrences des sequences de mots dans les donn√©es d'entrainement :\n",
    "\n",
    "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n)}{C(w_{t-1}\\dots w_{t-n})} \\tag{2} $$\n",
    "\n",
    "Avec $C(\\cdots)$ le nombre d'occurrences d'une s√©quence de mots donn√©e. En pratique, le d√©nominateur peut √™tre nul. On va ajouter un param√®tre de smoothing. On ajoute une constate $k$ au num√©rateur et $k \\times |V|$ au d√©nominateur avec $|V|$ la taille du vocabulaire. On a donc :\n",
    "\n",
    "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n) + k}{C(w_{t-1}\\dots w_{t-n}) + k|V|} \\tag{3} $$\n",
    "\n",
    "Si on a un n-grams qui n'apparait pas, l'√©quation (3) devient donc $\\frac{1}{|V|}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [['je', 'suis', 'en', 'vacances'],\n",
    "             ['je', 'vais', 'partir', '√†', 'la', 'r√©union'],\n",
    "             ['je', 'suis', 'en', 'r√©union'],\n",
    "             ['je', 'vais', 'partir', 'en', 'vacances']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['je', 'r√©union', 'vacances', '√†', 'vais', 'suis', 'en', 'partir', 'la']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = list(set(sentences[0] + sentences[1] + sentences[2] + sentences[3]))\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>üìù Exercice :</b> Ecrire une fonction qui g√©n√®re tous les n-grams d'une phrase avec n un param√®tre de la fonction.</p>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/ngrams.py\n",
    "\n",
    "def sentence_2_n_grams(sentences, n=3, start_token='<s>', end_token='</s>'):\n",
    "    ngrams = []\n",
    "    for s in sentences:\n",
    "        tokens = [start_token] + s + [end_token]\n",
    "        ngrams += zip(*[tokens[i:] for i in range(n)])\n",
    "    return Counter([\" \".join(ngram) for ngram in ngrams])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uni-gram:\n",
      "Counter({'<s>': 4, 'je': 4, '</s>': 4, 'en': 3, 'suis': 2, 'vacances': 2, 'vais': 2, 'partir': 2, 'r√©union': 2, '√†': 1, 'la': 1})\n",
      "\n",
      "Bi-gram:\n",
      "Counter({'<s> je': 4, 'je suis': 2, 'suis en': 2, 'en vacances': 2, 'vacances </s>': 2, 'je vais': 2, 'vais partir': 2, 'r√©union </s>': 2, 'partir √†': 1, '√† la': 1, 'la r√©union': 1, 'en r√©union': 1, 'partir en': 1})\n",
      "\n",
      "Tri-gram:\n",
      "Counter({'<s> je suis': 2, 'je suis en': 2, 'en vacances </s>': 2, '<s> je vais': 2, 'je vais partir': 2, 'suis en vacances': 1, 'vais partir √†': 1, 'partir √† la': 1, '√† la r√©union': 1, 'la r√©union </s>': 1, 'suis en r√©union': 1, 'en r√©union </s>': 1, 'vais partir en': 1, 'partir en vacances': 1})\n"
     ]
    }
   ],
   "source": [
    "unigram_counts = sentence_2_n_grams(sentences, 1)\n",
    "print(\"Uni-gram:\")\n",
    "print(unigram_counts)\n",
    "\n",
    "bigram_counts = sentence_2_n_grams(sentences, 2)\n",
    "print(\"\\nBi-gram:\")\n",
    "print(bigram_counts)\n",
    "\n",
    "trigram_counts = sentence_2_n_grams(sentences, 3)\n",
    "print(\"\\nTri-gram:\")\n",
    "print(trigram_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>üìù Exercice :</b> Ecrire une fonction qui calcule la probabilit√© d'un mot en fonction des ngrams pr√©c√©dents.</p>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/estimate_proba.py\n",
    "def estimate_probability(word, previous_n_gram,\n",
    "                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "    denominator = n_gram_counts.get(previous_n_gram, 0)\n",
    "    denominator += k * vocabulary_size\n",
    "\n",
    "    numerator = n_plus1_gram_counts.get(previous_n_gram + ' ' + word, 0)\n",
    "    numerator += k\n",
    "\n",
    "    probability = numerator / denominator\n",
    "\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La probabilit√© du mot 'partir' √©tant donn√© le pr√©c√©dent n-gram 'je vais' est : 0.273.\n"
     ]
    }
   ],
   "source": [
    "word_1 = \"je vais\"\n",
    "word_2 = \"partir\"\n",
    "tmp_prob = estimate_probability(word_2, word_1, bigram_counts, trigram_counts, len(unique_words), k=1)\n",
    "\n",
    "print(\"La probabilit√© du mot '{}' √©tant donn√© le pr√©c√©dent n-gram '{}' est : {:.3f}.\"\n",
    "      .format(word_2, word_1, tmp_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0,\n",
    "                           start_token='<s>', end_token='</s>', unk_token='<unk>'):\n",
    "    \n",
    "    # On ajoute end_token et unk_token to the vocabulary\n",
    "    # start_token ne peut pas apparaitre comme mot suivant donc pas besoin de l'ajouter\n",
    "    vocabulary = vocabulary + [end_token, unk_token]\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    \n",
    "    probabilities = {}\n",
    "    for word in vocabulary:\n",
    "        probability = estimate_probability(word, previous_n_gram, \n",
    "                                           n_gram_counts, n_plus1_gram_counts, \n",
    "                                           vocabulary_size, k=k)\n",
    "        probabilities[word] = probability\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La probabilit√© du mot 'je' √©tant donn√© le pr√©c√©dent n-gram 'je' est : 0.067.\n",
      "La probabilit√© du mot 'r√©union' √©tant donn√© le pr√©c√©dent n-gram 'je' est : 0.067.\n",
      "La probabilit√© du mot 'vacances' √©tant donn√© le pr√©c√©dent n-gram 'je' est : 0.067.\n",
      "La probabilit√© du mot '√†' √©tant donn√© le pr√©c√©dent n-gram 'je' est : 0.067.\n",
      "La probabilit√© du mot 'vais' √©tant donn√© le pr√©c√©dent n-gram 'je' est : 0.200.\n",
      "La probabilit√© du mot 'suis' √©tant donn√© le pr√©c√©dent n-gram 'je' est : 0.200.\n",
      "La probabilit√© du mot 'en' √©tant donn√© le pr√©c√©dent n-gram 'je' est : 0.067.\n",
      "La probabilit√© du mot 'partir' √©tant donn√© le pr√©c√©dent n-gram 'je' est : 0.067.\n",
      "La probabilit√© du mot 'la' √©tant donn√© le pr√©c√©dent n-gram 'je' est : 0.067.\n",
      "La probabilit√© du mot '</s>' √©tant donn√© le pr√©c√©dent n-gram 'je' est : 0.067.\n",
      "La probabilit√© du mot '<unk>' √©tant donn√© le pr√©c√©dent n-gram 'je' est : 0.067.\n"
     ]
    }
   ],
   "source": [
    "next_word_proba = estimate_probabilities(\"je\", unigram_counts, bigram_counts, unique_words, k=1)\n",
    "\n",
    "for w, p in next_word_proba.items():\n",
    "    print(\"La probabilit√© du mot '{}' √©tant donn√© le pr√©c√©dent n-gram '{}' est : {:.3f}.\"\n",
    "          .format(w, 'je', p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La probabilit√© du mot 'je' √©tant donn√© le pr√©c√©dent n-gram 'en' est : 0.067.\n",
      "La probabilit√© du mot 'r√©union' √©tant donn√© le pr√©c√©dent n-gram 'en' est : 0.067.\n",
      "La probabilit√© du mot 'vacances' √©tant donn√© le pr√©c√©dent n-gram 'en' est : 0.067.\n",
      "La probabilit√© du mot '√†' √©tant donn√© le pr√©c√©dent n-gram 'en' est : 0.067.\n",
      "La probabilit√© du mot 'vais' √©tant donn√© le pr√©c√©dent n-gram 'en' est : 0.200.\n",
      "La probabilit√© du mot 'suis' √©tant donn√© le pr√©c√©dent n-gram 'en' est : 0.200.\n",
      "La probabilit√© du mot 'en' √©tant donn√© le pr√©c√©dent n-gram 'en' est : 0.067.\n",
      "La probabilit√© du mot 'partir' √©tant donn√© le pr√©c√©dent n-gram 'en' est : 0.067.\n",
      "La probabilit√© du mot 'la' √©tant donn√© le pr√©c√©dent n-gram 'en' est : 0.067.\n",
      "La probabilit√© du mot '</s>' √©tant donn√© le pr√©c√©dent n-gram 'en' est : 0.067.\n",
      "La probabilit√© du mot '<unk>' √©tant donn√© le pr√©c√©dent n-gram 'en' est : 0.067.\n"
     ]
    }
   ],
   "source": [
    "estimate_probabilities(\"en\", bigram_counts, trigram_counts, unique_words, k=1)\n",
    "\n",
    "for w, p in next_word_proba.items():\n",
    "    print(\"La probabilit√© du mot '{}' √©tant donn√© le pr√©c√©dent n-gram '{}' est : {:.3f}.\"\n",
    "          .format(w, 'en', p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_count_matrix(n_plus1_gram_counts, vocabulary,\n",
    "                      start_token='<s>', end_token='</s>', unk_token='<unk>'):\n",
    " \n",
    "    vocabulary = vocabulary + [end_token, unk_token]\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    \n",
    "    # obtain unique n-grams\n",
    "    n_grams = list(n_plus1_gram_counts.keys())\n",
    "    \n",
    "    row_index = {n_gram: i for i, n_gram in enumerate(n_grams)}\n",
    "    col_index = {word: j for j, word in enumerate(vocabulary)}\n",
    "    \n",
    "    nrow = len(n_grams)\n",
    "    ncol = len(vocabulary)\n",
    "    count_matrix = np.zeros((nrow, ncol))\n",
    "    \n",
    "    for n_plus1_gram, count in n_plus1_gram_counts.items():\n",
    "        n_gram = n_plus1_gram\n",
    "        word = n_plus1_gram.split()[-1]\n",
    "        if word not in vocabulary:\n",
    "            continue\n",
    "        i = row_index[n_gram]\n",
    "        j = col_index[word]\n",
    "        count_matrix[i, j] = count\n",
    "    \n",
    "    count_matrix = pd.DataFrame(count_matrix, index=[' '.join(ng.split()[0:-1]) for ng in n_grams], columns=vocabulary)\n",
    "    return count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>je</th>\n",
       "      <th>r√©union</th>\n",
       "      <th>vacances</th>\n",
       "      <th>√†</th>\n",
       "      <th>vais</th>\n",
       "      <th>suis</th>\n",
       "      <th>en</th>\n",
       "      <th>partir</th>\n",
       "      <th>la</th>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>je</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suis</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vacances</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>je</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vais</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partir</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>√†</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>la</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r√©union</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partir</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           je  r√©union  vacances    √†  vais  suis   en  partir   la  </s>  \\\n",
       "<s>       4.0      0.0       0.0  0.0   0.0   0.0  0.0     0.0  0.0   0.0   \n",
       "je        0.0      0.0       0.0  0.0   0.0   2.0  0.0     0.0  0.0   0.0   \n",
       "suis      0.0      0.0       0.0  0.0   0.0   0.0  2.0     0.0  0.0   0.0   \n",
       "en        0.0      0.0       2.0  0.0   0.0   0.0  0.0     0.0  0.0   0.0   \n",
       "vacances  0.0      0.0       0.0  0.0   0.0   0.0  0.0     0.0  0.0   2.0   \n",
       "je        0.0      0.0       0.0  0.0   2.0   0.0  0.0     0.0  0.0   0.0   \n",
       "vais      0.0      0.0       0.0  0.0   0.0   0.0  0.0     2.0  0.0   0.0   \n",
       "partir    0.0      0.0       0.0  1.0   0.0   0.0  0.0     0.0  0.0   0.0   \n",
       "√†         0.0      0.0       0.0  0.0   0.0   0.0  0.0     0.0  1.0   0.0   \n",
       "la        0.0      1.0       0.0  0.0   0.0   0.0  0.0     0.0  0.0   0.0   \n",
       "r√©union   0.0      0.0       0.0  0.0   0.0   0.0  0.0     0.0  0.0   2.0   \n",
       "en        0.0      1.0       0.0  0.0   0.0   0.0  0.0     0.0  0.0   0.0   \n",
       "partir    0.0      0.0       0.0  0.0   0.0   0.0  1.0     0.0  0.0   0.0   \n",
       "\n",
       "          <unk>  \n",
       "<s>         0.0  \n",
       "je          0.0  \n",
       "suis        0.0  \n",
       "en          0.0  \n",
       "vacances    0.0  \n",
       "je          0.0  \n",
       "vais        0.0  \n",
       "partir      0.0  \n",
       "√†           0.0  \n",
       "la          0.0  \n",
       "r√©union     0.0  \n",
       "en          0.0  \n",
       "partir      0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = [['je', 'suis', 'en', 'vacances'],\n",
    "             ['je', 'vais', 'partir', '√†', 'la', 'r√©union'],\n",
    "             ['je', 'suis', 'en', 'r√©union'],\n",
    "             ['je', 'vais', 'partir', 'en', 'vacances']]\n",
    "\n",
    "display(make_count_matrix(bigram_counts, unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>je</th>\n",
       "      <th>r√©union</th>\n",
       "      <th>vacances</th>\n",
       "      <th>√†</th>\n",
       "      <th>vais</th>\n",
       "      <th>suis</th>\n",
       "      <th>en</th>\n",
       "      <th>partir</th>\n",
       "      <th>la</th>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;s&gt; je</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>je suis</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suis en</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en vacances</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;s&gt; je</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>je vais</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vais partir</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partir √†</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>√† la</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>la r√©union</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suis en</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en r√©union</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vais partir</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partir en</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              je  r√©union  vacances    √†  vais  suis   en  partir   la  </s>  \\\n",
       "<s> je       0.0      0.0       0.0  0.0   0.0   2.0  0.0     0.0  0.0   0.0   \n",
       "je suis      0.0      0.0       0.0  0.0   0.0   0.0  2.0     0.0  0.0   0.0   \n",
       "suis en      0.0      0.0       1.0  0.0   0.0   0.0  0.0     0.0  0.0   0.0   \n",
       "en vacances  0.0      0.0       0.0  0.0   0.0   0.0  0.0     0.0  0.0   2.0   \n",
       "<s> je       0.0      0.0       0.0  0.0   2.0   0.0  0.0     0.0  0.0   0.0   \n",
       "je vais      0.0      0.0       0.0  0.0   0.0   0.0  0.0     2.0  0.0   0.0   \n",
       "vais partir  0.0      0.0       0.0  1.0   0.0   0.0  0.0     0.0  0.0   0.0   \n",
       "partir √†     0.0      0.0       0.0  0.0   0.0   0.0  0.0     0.0  1.0   0.0   \n",
       "√† la         0.0      1.0       0.0  0.0   0.0   0.0  0.0     0.0  0.0   0.0   \n",
       "la r√©union   0.0      0.0       0.0  0.0   0.0   0.0  0.0     0.0  0.0   1.0   \n",
       "suis en      0.0      1.0       0.0  0.0   0.0   0.0  0.0     0.0  0.0   0.0   \n",
       "en r√©union   0.0      0.0       0.0  0.0   0.0   0.0  0.0     0.0  0.0   1.0   \n",
       "vais partir  0.0      0.0       0.0  0.0   0.0   0.0  1.0     0.0  0.0   0.0   \n",
       "partir en    0.0      0.0       1.0  0.0   0.0   0.0  0.0     0.0  0.0   0.0   \n",
       "\n",
       "             <unk>  \n",
       "<s> je         0.0  \n",
       "je suis        0.0  \n",
       "suis en        0.0  \n",
       "en vacances    0.0  \n",
       "<s> je         0.0  \n",
       "je vais        0.0  \n",
       "vais partir    0.0  \n",
       "partir √†       0.0  \n",
       "√† la           0.0  \n",
       "la r√©union     0.0  \n",
       "suis en        0.0  \n",
       "en r√©union     0.0  \n",
       "vais partir    0.0  \n",
       "partir en      0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show trigram counts\n",
    "display(make_count_matrix(trigram_counts, unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n",
    "    count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)\n",
    "    count_matrix += k\n",
    "    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n",
    "    return prob_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>je</th>\n",
       "      <th>r√©union</th>\n",
       "      <th>vacances</th>\n",
       "      <th>√†</th>\n",
       "      <th>vais</th>\n",
       "      <th>suis</th>\n",
       "      <th>en</th>\n",
       "      <th>partir</th>\n",
       "      <th>la</th>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>je</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suis</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vacances</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>je</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vais</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partir</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>√†</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>la</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r√©union</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partir</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                je   r√©union  vacances         √†      vais      suis  \\\n",
       "<s>       0.333333  0.066667  0.066667  0.066667  0.066667  0.066667   \n",
       "je        0.076923  0.076923  0.076923  0.076923  0.076923  0.230769   \n",
       "suis      0.076923  0.076923  0.076923  0.076923  0.076923  0.076923   \n",
       "en        0.076923  0.076923  0.230769  0.076923  0.076923  0.076923   \n",
       "vacances  0.076923  0.076923  0.076923  0.076923  0.076923  0.076923   \n",
       "je        0.076923  0.076923  0.076923  0.076923  0.230769  0.076923   \n",
       "vais      0.076923  0.076923  0.076923  0.076923  0.076923  0.076923   \n",
       "partir    0.083333  0.083333  0.083333  0.166667  0.083333  0.083333   \n",
       "√†         0.083333  0.083333  0.083333  0.083333  0.083333  0.083333   \n",
       "la        0.083333  0.166667  0.083333  0.083333  0.083333  0.083333   \n",
       "r√©union   0.076923  0.076923  0.076923  0.076923  0.076923  0.076923   \n",
       "en        0.083333  0.166667  0.083333  0.083333  0.083333  0.083333   \n",
       "partir    0.083333  0.083333  0.083333  0.083333  0.083333  0.083333   \n",
       "\n",
       "                en    partir        la      </s>     <unk>  \n",
       "<s>       0.066667  0.066667  0.066667  0.066667  0.066667  \n",
       "je        0.076923  0.076923  0.076923  0.076923  0.076923  \n",
       "suis      0.230769  0.076923  0.076923  0.076923  0.076923  \n",
       "en        0.076923  0.076923  0.076923  0.076923  0.076923  \n",
       "vacances  0.076923  0.076923  0.076923  0.230769  0.076923  \n",
       "je        0.076923  0.076923  0.076923  0.076923  0.076923  \n",
       "vais      0.076923  0.230769  0.076923  0.076923  0.076923  \n",
       "partir    0.083333  0.083333  0.083333  0.083333  0.083333  \n",
       "√†         0.083333  0.083333  0.166667  0.083333  0.083333  \n",
       "la        0.083333  0.083333  0.083333  0.083333  0.083333  \n",
       "r√©union   0.076923  0.076923  0.076923  0.230769  0.076923  \n",
       "en        0.083333  0.083333  0.083333  0.083333  0.083333  \n",
       "partir    0.166667  0.083333  0.083333  0.083333  0.083333  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(make_probability_matrix(bigram_counts, unique_words, k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>je</th>\n",
       "      <th>r√©union</th>\n",
       "      <th>vacances</th>\n",
       "      <th>√†</th>\n",
       "      <th>vais</th>\n",
       "      <th>suis</th>\n",
       "      <th>en</th>\n",
       "      <th>partir</th>\n",
       "      <th>la</th>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;s&gt; je</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>je suis</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suis en</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en vacances</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;s&gt; je</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>je vais</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vais partir</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partir √†</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>√† la</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>la r√©union</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suis en</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en r√©union</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vais partir</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partir en</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   je   r√©union  vacances         √†      vais      suis  \\\n",
       "<s> je       0.076923  0.076923  0.076923  0.076923  0.076923  0.230769   \n",
       "je suis      0.076923  0.076923  0.076923  0.076923  0.076923  0.076923   \n",
       "suis en      0.083333  0.083333  0.166667  0.083333  0.083333  0.083333   \n",
       "en vacances  0.076923  0.076923  0.076923  0.076923  0.076923  0.076923   \n",
       "<s> je       0.076923  0.076923  0.076923  0.076923  0.230769  0.076923   \n",
       "je vais      0.076923  0.076923  0.076923  0.076923  0.076923  0.076923   \n",
       "vais partir  0.083333  0.083333  0.083333  0.166667  0.083333  0.083333   \n",
       "partir √†     0.083333  0.083333  0.083333  0.083333  0.083333  0.083333   \n",
       "√† la         0.083333  0.166667  0.083333  0.083333  0.083333  0.083333   \n",
       "la r√©union   0.083333  0.083333  0.083333  0.083333  0.083333  0.083333   \n",
       "suis en      0.083333  0.166667  0.083333  0.083333  0.083333  0.083333   \n",
       "en r√©union   0.083333  0.083333  0.083333  0.083333  0.083333  0.083333   \n",
       "vais partir  0.083333  0.083333  0.083333  0.083333  0.083333  0.083333   \n",
       "partir en    0.083333  0.083333  0.166667  0.083333  0.083333  0.083333   \n",
       "\n",
       "                   en    partir        la      </s>     <unk>  \n",
       "<s> je       0.076923  0.076923  0.076923  0.076923  0.076923  \n",
       "je suis      0.230769  0.076923  0.076923  0.076923  0.076923  \n",
       "suis en      0.083333  0.083333  0.083333  0.083333  0.083333  \n",
       "en vacances  0.076923  0.076923  0.076923  0.230769  0.076923  \n",
       "<s> je       0.076923  0.076923  0.076923  0.076923  0.076923  \n",
       "je vais      0.076923  0.230769  0.076923  0.076923  0.076923  \n",
       "vais partir  0.083333  0.083333  0.083333  0.083333  0.083333  \n",
       "partir √†     0.083333  0.083333  0.166667  0.083333  0.083333  \n",
       "√† la         0.083333  0.083333  0.083333  0.083333  0.083333  \n",
       "la r√©union   0.083333  0.083333  0.083333  0.166667  0.083333  \n",
       "suis en      0.083333  0.083333  0.083333  0.083333  0.083333  \n",
       "en r√©union   0.083333  0.083333  0.083333  0.166667  0.083333  \n",
       "vais partir  0.166667  0.083333  0.083333  0.083333  0.083333  \n",
       "partir en    0.083333  0.083333  0.083333  0.083333  0.083333  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(make_probability_matrix(trigram_counts, unique_words, k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
    "    \n",
    "    n = len(list(n_gram_counts.keys())[0].split()) \n",
    "    previous_n_gram = ' '.join(previous_tokens.split()[-n:])\n",
    "    probabilities = estimate_probabilities(previous_n_gram,\n",
    "                                           n_gram_counts, n_plus1_gram_counts,\n",
    "                                           vocabulary, k=k)\n",
    "\n",
    "    suggestion = None\n",
    "    max_prob = 0\n",
    "    for word, prob in probabilities.items(): \n",
    "        if prob > max_prob: \n",
    "            suggestion = word            \n",
    "            max_prob = prob    \n",
    "    return suggestion, max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pour les tokens 'je vais',la suggestion est le mot 'partir' avec une probabilit√© de 0.2308.\n"
     ]
    }
   ],
   "source": [
    "previous_tokens = \"je vais\"\n",
    "tmp_suggest1 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)\n",
    "print(f\"Pour les tokens 'je vais',la suggestion est le mot '{tmp_suggest1[0]}' avec une probabilit√© de {tmp_suggest1[1]:.4f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut calculer la perplexit√© pour √©valuer le mod√®le. Cette derni√®re est donn√©e par :\n",
    "\n",
    "$$ PP(W) =\\sqrt[N]{ \\prod_{t=n+1}^N \\frac{1}{P(w_t | w_{t-n} \\cdots w_{t-1})} } \\tag{4}$$\n",
    "\n",
    "Avec $N$ la longueur de la phrase et $n$ la taille des n-grams (par exemple 2 dans le cas des bigrams). On cherche √† minimiser la perplexit√© du mod√®le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0,\n",
    "                         start_token='<s>', end_token='</s>', unk_token='<unk>'):\n",
    "    \n",
    "    n = len(list(n_gram_counts.keys())[0].split()) \n",
    "    tokens = [start_token] + sentence + [end_token]\n",
    "    N = len(tokens)\n",
    "    \n",
    "    product_pi = 1.0\n",
    "  \n",
    "    for t in range(n, N): \n",
    "        n_gram = tokens[t-n:t]\n",
    "        word = tokens[t]\n",
    "        \n",
    "        probability = estimate_probability(word, ' '.join(n_gram), \n",
    "                                           n_gram_counts, n_plus1_gram_counts, \n",
    "                                           len(unique_words), k=1)\n",
    "        product_pi *= 1 / probability\n",
    "\n",
    "    perplexity = product_pi**(1/float(N))\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La perplexit√© pour la premi√®re phrase du corpus est : 2.9089.\n",
      "La perplexit√© pour la phrase test est : 6.6343.\n"
     ]
    }
   ],
   "source": [
    "perplexity_train1 = calculate_perplexity(sentences[0],\n",
    "                                         unigram_counts, bigram_counts,\n",
    "                                         len(unique_words), k=1.0)\n",
    "print(f\"La perplexit√© pour la premi√®re phrase du corpus est : {perplexity_train1:.4f}.\")\n",
    "\n",
    "\n",
    "perplexity_train1 = calculate_perplexity(['Tu' ,'pars', 'ou', 'en', 'vacances', '?'],\n",
    "                                         unigram_counts, bigram_counts,\n",
    "                                         len(unique_words), k=1.0)\n",
    "print(f\"La perplexit√© pour la phrase test est : {perplexity_train1:.4f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2: les mod√®les de langues avec r√©seaux de neurones : GPT-2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'ai entrain√© un mod√®le GPT-2 <span class=\"badge badge-secondary\">([Radford et al., 2019](#radford-2019))</span> sur 50M de phrases extraites du corpus OSCAR <span class=\"badge badge-secondary\">([Su√°rez et al., 2019](#suarez-2019))</span>. Le mod√®le a ensuite √©t√© fine-tun√© (on a continu√© l'entrainement) sur le Tome 2 de Harry Potter. Ce mod√®le est une architecture de r√©seaux de neurones assez connu pour les mod√®les de langues. Il permet de g√©n√©rer du texte de mani√®re assez r√©aliste. On va utiliser la librairie `transformers` pour utiliser le mod√®le. **Vous devez r√©cup√©rer les poids du mod√®les sur le moodle (fichier french-gpt2-hp)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "Some weights or buffers of the PyTorch model TFGPT2LMHeadModel were not initialized from the TF 2.0 model and are newly initialized: ['transformer.h.10.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.8.attn.bias', 'lm_head.weight', 'transformer.h.2.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.9.attn.bias', 'transformer.h.7.attn.bias', 'transformer.h.1.attn.bias', 'transformer.h.0.attn.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./french-gpt2-hp\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"./french-gpt2-hp\", \n",
    "                                          pad_token_id=tokenizer.eos_token_id, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(\n",
    "    \"Dans son mouvement, la queue du Basilic lui avait jet√© le Choixpeau magique √† la t√™te. \",\n",
    "    return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 2 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Dans son mouvement, la queue du Basilic lui avait jet√© le Choixpeau magique √† la t√™te.  ‚ÄîQu'est-ce que tu fais l√†?\n",
      "demanda Harry.\n",
      "‚ÄîQu'est-ce que tu fais l√†\n"
     ]
    }
   ],
   "source": [
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = model.generate(input_ids, max_length=50)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 2 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Dans son mouvement, la queue du Basilic lui avait jet√© le Choixpeau magique √† la t√™te.  ‚ÄîQu'est-ce qui se passe?\n",
      "demanda-t-il d'une voix aigu√´.\n",
      "‚ÄîQuoi?‚Äî‚Äì‚Äî‚Äìdemarre aussit√¥t de la foule, les yeux fix√©s sur le visage de Malefoy qui avait l'air de plus en plus livide, comme s'il n'avait pas eu le temps de prononcer le moindre mot...  Harry se pr√©cipita dans la salle commune des Gryffondor, √† c√¥t√© de Ron et de Hermione, mais il ne fut pas surpris de voir que le professeur McGonagall √©tait en train de dire quelque chose sur la Chambre des Secrets et qu'elle ne semblait pas convaincue que c'√©tait la meilleure chose √† faire...\n",
      "Il y eut un long silence, puis il se tourna vers Ron, le regard perdu dans ses pens√©es, et le silence qui r√©gnait autour de lui se r√©percut en\n"
     ]
    }
   ],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids, \n",
    "    max_length=200, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(\n",
    "    \"Assis un peu plus loin, Harry reconnut Gilderoy Lockhart, v√™tu d'une robe de sorcier bleu-vert.\",\n",
    "    return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 2 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Assis un peu plus loin, Harry reconnut Gilderoy Lockhart, v√™tu d'une robe de sorcier bleu-vert.\n",
      "‚ÄîQu'est-ce qu'il y a?\n",
      "demanda-t-il en s'effor√ßant de ne pas faire de bruit, mais il n'eut pas le temps de prononcer le moindre mot, et il se laissa tomber sur le sol humide et humide de la salle commune de Gryffondor, √† c√¥t√© de Ron et de Hermione qui le regardaient avec des yeux ronds et des cheveux boucl√©s qui lui tombaient sur les yeux. Mais il ne fut pas surpris de voir que le professeur McGonagall √©tait le seul √† l'avoir vu, alors que les autres √©l√®ves de Serpentard √©taient assis c√¥te √† c√¥te sur un banc de Quidditch, au fond duquel √©tait √©crit en lettres noires :  ‚ÄîViens, dit Harry, je t'ai dit que j'√©tais le plus grand sorcier de tous les temps\n"
     ]
    }
   ],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids, \n",
    "    max_length=200, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö References\n",
    "\n",
    "> <div id=\"radford-2019\">Alec Radford, Jeffrey Wu, Rewon Child, David Luan and Dario Amodei. <a href=https://openai.com/blog/better-language-models/> Better Language Models and Their Implications.</a></div>\n",
    "\n",
    "> <div id=\"suarez-2019\">Su√°rez, Pedro Javier Ortiz, Beno√Æt Sagot, and Laurent Romary. <a href=https://hal.inria.fr/hal-02148693> Asynchronous pipeline for processing huge corpora on medium to low resource infrastructures.</a> 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7). Leibniz-Institut f√ºr Deutsche Sprache, 2019.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
