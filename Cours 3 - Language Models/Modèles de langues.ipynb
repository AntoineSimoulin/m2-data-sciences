{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice Models de langue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install transformers==3.1.0\n",
    "# !pip install -q tensorflow==2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1: les modèles de langues de types N-gram\n",
    "\n",
    "Dans ce premier exercice, nous allons implémenter un modèle de langue de type N-gram pour construire un **système d'auto-complétion**. Ce type de système est utilisé dans Google pour proposer de compléter les queries de recherche ou pour la rédaction des textos pour proposer le mot suivant par exemple.\n",
    "\n",
    "<img src = \"autocomplete.png\" style=\"width:500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous l'avons vu en cours, les modèles de langues n-grams cherchent à estimer la probabilité conditionnelle d'un mot $t$ dans la phrase étant donné les $n$ mots précédents $w_{t-1}, w_{t-2} \\cdots w_{t-n}$ : \n",
    "\n",
    "$$ P(w_t | w_{t-1}\\dots w_{t-n}) \\tag{1}$$\n",
    "\n",
    "On estime cette probabilité avec $\\hat{P}$ en comptant les occurrences des sequences de mots dans les données d'entrainement :\n",
    "\n",
    "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n)}{C(w_{t-1}\\dots w_{t-n})} \\tag{2} $$\n",
    "\n",
    "Avec $C(\\cdots)$ le nombre d'occurrences d'une séquence de mots donnée. En pratique, le dénominateur peut être nul. On va ajouter un paramètre de smoothing. On ajoute une constate $k$ au numérateur et $k \\times |V|$ au dénominateur avec $|V|$ la taille du vocabulaire. On a donc :\n",
    "\n",
    "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n) + k}{C(w_{t-1}\\dots w_{t-n}) + k|V|} \\tag{3} $$\n",
    "\n",
    "Si on a un n-grams qui n'apparait pas, l'équation (3) devient donc $\\frac{1}{|V|}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [['je', 'suis', 'en', 'vacances'],\n",
    "             ['je', 'vais', 'partir', 'à', 'la', 'réunion'],\n",
    "             ['je', 'suis', 'en', 'réunion'],\n",
    "             ['je', 'vais', 'partir', 'en', 'vacances']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['je', 'réunion', 'vacances', 'à', 'vais', 'suis', 'en', 'partir', 'la']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = list(set(sentences[0] + sentences[1] + sentences[2] + sentences[3]))\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>Exercice :</b> Ecrire une fonction qui génère tous les n-grams d'une phrase avec n un paramètre de la fonction.</p>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/ngrams.py\n",
    "\n",
    "def sentence_2_n_grams(sentences, n=3, start_token='<s>', end_token='</s>'):\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uni-gram:\n",
      "Counter({'<s>': 4, 'je': 4, '</s>': 4, 'en': 3, 'suis': 2, 'vacances': 2, 'vais': 2, 'partir': 2, 'réunion': 2, 'à': 1, 'la': 1})\n",
      "\n",
      "Bi-gram:\n",
      "Counter({'<s> je': 4, 'je suis': 2, 'suis en': 2, 'en vacances': 2, 'vacances </s>': 2, 'je vais': 2, 'vais partir': 2, 'réunion </s>': 2, 'partir à': 1, 'à la': 1, 'la réunion': 1, 'en réunion': 1, 'partir en': 1})\n",
      "\n",
      "Tri-gram:\n",
      "Counter({'<s> je suis': 2, 'je suis en': 2, 'en vacances </s>': 2, '<s> je vais': 2, 'je vais partir': 2, 'suis en vacances': 1, 'vais partir à': 1, 'partir à la': 1, 'à la réunion': 1, 'la réunion </s>': 1, 'suis en réunion': 1, 'en réunion </s>': 1, 'vais partir en': 1, 'partir en vacances': 1})\n"
     ]
    }
   ],
   "source": [
    "unigram_counts = sentence_2_n_grams(sentences, 1)\n",
    "print(\"Uni-gram:\")\n",
    "print(unigram_counts)\n",
    "\n",
    "bigram_counts = sentence_2_n_grams(sentences, 2)\n",
    "print(\"\\nBi-gram:\")\n",
    "print(bigram_counts)\n",
    "\n",
    "trigram_counts = sentence_2_n_grams(sentences, 3)\n",
    "print(\"\\nTri-gram:\")\n",
    "print(trigram_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>Exercice :</b> Ecrire une fonction qui calcule la probabilité d'un mot en fonction des ngrams précédents.</p>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-17-0b28851b9844>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-0b28851b9844>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    denominator =\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/estimate_proba.py\n",
    "\n",
    "def estimate_probability(word, previous_n_gram,\n",
    "                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "    denominator = \n",
    "    \n",
    "    numerator =\n",
    "    \n",
    "    probability = numerator / denominator\n",
    "\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La probabilité du mot 'pars' étant donné le précédent n-gram 'je' est : 0.077.\n"
     ]
    }
   ],
   "source": [
    "word_1 = \"je\"\n",
    "word_2 = \"pars\"\n",
    "tmp_prob = estimate_probability(word_2, word_1, unigram_counts, bigram_counts, len(unique_words), k=1)\n",
    "\n",
    "print(\"La probabilité du mot '{}' étant donné le précédent n-gram '{}' est : {:.3f}.\"\n",
    "      .format(word_2, word_1, tmp_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0,\n",
    "                           start_token='<s>', end_token='</s>', unk_token='<unk>'):\n",
    "    \n",
    "    # On ajoute end_token et unk_token to the vocabulary\n",
    "    # start_token ne peut pas apparaitre comme mot suivant donc pas besoin de l'ajouter\n",
    "    vocabulary = vocabulary + [end_token, unk_token]\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    \n",
    "    probabilities = {}\n",
    "    for word in vocabulary:\n",
    "        probability = estimate_probability(word, previous_n_gram, \n",
    "                                           n_gram_counts, n_plus1_gram_counts, \n",
    "                                           vocabulary_size, k=k)\n",
    "        probabilities[word] = probability\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La probabilité du mot 'je' étant donné le précédent n-gram 'je' est : 0.067.\n",
      "La probabilité du mot 'réunion' étant donné le précédent n-gram 'je' est : 0.067.\n",
      "La probabilité du mot 'vacances' étant donné le précédent n-gram 'je' est : 0.067.\n",
      "La probabilité du mot 'à' étant donné le précédent n-gram 'je' est : 0.067.\n",
      "La probabilité du mot 'vais' étant donné le précédent n-gram 'je' est : 0.200.\n",
      "La probabilité du mot 'suis' étant donné le précédent n-gram 'je' est : 0.200.\n",
      "La probabilité du mot 'en' étant donné le précédent n-gram 'je' est : 0.067.\n",
      "La probabilité du mot 'partir' étant donné le précédent n-gram 'je' est : 0.067.\n",
      "La probabilité du mot 'la' étant donné le précédent n-gram 'je' est : 0.067.\n",
      "La probabilité du mot '</s>' étant donné le précédent n-gram 'je' est : 0.067.\n",
      "La probabilité du mot '<unk>' étant donné le précédent n-gram 'je' est : 0.067.\n"
     ]
    }
   ],
   "source": [
    "next_word_proba = estimate_probabilities(\"je\", unigram_counts, bigram_counts, unique_words, k=1)\n",
    "\n",
    "for w, p in next_word_proba.items():\n",
    "    print(\"La probabilité du mot '{}' étant donné le précédent n-gram '{}' est : {:.3f}.\"\n",
    "          .format(w, 'je', p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La probabilité du mot 'je' étant donné le précédent n-gram 'en' est : 0.067.\n",
      "La probabilité du mot 'réunion' étant donné le précédent n-gram 'en' est : 0.067.\n",
      "La probabilité du mot 'vacances' étant donné le précédent n-gram 'en' est : 0.067.\n",
      "La probabilité du mot 'à' étant donné le précédent n-gram 'en' est : 0.067.\n",
      "La probabilité du mot 'vais' étant donné le précédent n-gram 'en' est : 0.200.\n",
      "La probabilité du mot 'suis' étant donné le précédent n-gram 'en' est : 0.200.\n",
      "La probabilité du mot 'en' étant donné le précédent n-gram 'en' est : 0.067.\n",
      "La probabilité du mot 'partir' étant donné le précédent n-gram 'en' est : 0.067.\n",
      "La probabilité du mot 'la' étant donné le précédent n-gram 'en' est : 0.067.\n",
      "La probabilité du mot '</s>' étant donné le précédent n-gram 'en' est : 0.067.\n",
      "La probabilité du mot '<unk>' étant donné le précédent n-gram 'en' est : 0.067.\n"
     ]
    }
   ],
   "source": [
    "estimate_probabilities(\"en\", bigram_counts, trigram_counts, unique_words, k=1)\n",
    "\n",
    "for w, p in next_word_proba.items():\n",
    "    print(\"La probabilité du mot '{}' étant donné le précédent n-gram '{}' est : {:.3f}.\"\n",
    "          .format(w, 'en', p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_count_matrix(n_plus1_gram_counts, vocabulary,\n",
    "                      start_token='<s>', end_token='</s>', unk_token='<unk>'):\n",
    " \n",
    "    vocabulary = vocabulary + [end_token, unk_token]\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    \n",
    "    # obtain unique n-grams\n",
    "    n_grams = list(n_plus1_gram_counts.keys())\n",
    "    \n",
    "    row_index = {n_gram: i for i, n_gram in enumerate(n_grams)}\n",
    "    col_index = {word: j for j, word in enumerate(vocabulary)}\n",
    "    \n",
    "    nrow = len(n_grams)\n",
    "    ncol = len(vocabulary)\n",
    "    count_matrix = np.zeros((nrow, ncol))\n",
    "    \n",
    "    for n_plus1_gram, count in n_plus1_gram_counts.items():\n",
    "        n_gram = n_plus1_gram\n",
    "        word = n_plus1_gram.split()[-1]\n",
    "        if word not in vocabulary:\n",
    "            continue\n",
    "        i = row_index[n_gram]\n",
    "        j = col_index[word]\n",
    "        count_matrix[i, j] = count\n",
    "    \n",
    "    count_matrix = pd.DataFrame(count_matrix, index=[' '.join(ng.split()[0:-1]) for ng in n_grams], columns=vocabulary)\n",
    "    return count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>je</th>\n",
       "      <th>réunion</th>\n",
       "      <th>vacances</th>\n",
       "      <th>à</th>\n",
       "      <th>vais</th>\n",
       "      <th>suis</th>\n",
       "      <th>en</th>\n",
       "      <th>partir</th>\n",
       "      <th>la</th>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>je</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suis</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vacances</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>je</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vais</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partir</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>à</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>la</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>réunion</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partir</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           je  réunion  vacances    à  vais  suis   en  partir   la  </s>  \\\n",
       "<s>       4.0      0.0       0.0  0.0   0.0   0.0  0.0     0.0  0.0   0.0   \n",
       "je        0.0      0.0       0.0  0.0   0.0   2.0  0.0     0.0  0.0   0.0   \n",
       "suis      0.0      0.0       0.0  0.0   0.0   0.0  2.0     0.0  0.0   0.0   \n",
       "en        0.0      0.0       2.0  0.0   0.0   0.0  0.0     0.0  0.0   0.0   \n",
       "vacances  0.0      0.0       0.0  0.0   0.0   0.0  0.0     0.0  0.0   2.0   \n",
       "je        0.0      0.0       0.0  0.0   2.0   0.0  0.0     0.0  0.0   0.0   \n",
       "vais      0.0      0.0       0.0  0.0   0.0   0.0  0.0     2.0  0.0   0.0   \n",
       "partir    0.0      0.0       0.0  1.0   0.0   0.0  0.0     0.0  0.0   0.0   \n",
       "à         0.0      0.0       0.0  0.0   0.0   0.0  0.0     0.0  1.0   0.0   \n",
       "la        0.0      1.0       0.0  0.0   0.0   0.0  0.0     0.0  0.0   0.0   \n",
       "réunion   0.0      0.0       0.0  0.0   0.0   0.0  0.0     0.0  0.0   2.0   \n",
       "en        0.0      1.0       0.0  0.0   0.0   0.0  0.0     0.0  0.0   0.0   \n",
       "partir    0.0      0.0       0.0  0.0   0.0   0.0  1.0     0.0  0.0   0.0   \n",
       "\n",
       "          <unk>  \n",
       "<s>         0.0  \n",
       "je          0.0  \n",
       "suis        0.0  \n",
       "en          0.0  \n",
       "vacances    0.0  \n",
       "je          0.0  \n",
       "vais        0.0  \n",
       "partir      0.0  \n",
       "à           0.0  \n",
       "la          0.0  \n",
       "réunion     0.0  \n",
       "en          0.0  \n",
       "partir      0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = [['je', 'suis', 'en', 'vacances'],\n",
    "             ['je', 'vais', 'partir', 'à', 'la', 'réunion'],\n",
    "             ['je', 'suis', 'en', 'réunion'],\n",
    "             ['je', 'vais', 'partir', 'en', 'vacances']]\n",
    "\n",
    "display(make_count_matrix(bigram_counts, unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>je</th>\n",
       "      <th>réunion</th>\n",
       "      <th>vacances</th>\n",
       "      <th>à</th>\n",
       "      <th>vais</th>\n",
       "      <th>suis</th>\n",
       "      <th>en</th>\n",
       "      <th>partir</th>\n",
       "      <th>la</th>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;s&gt; je</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>je suis</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suis en</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en vacances</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;s&gt; je</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>je vais</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vais partir</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partir à</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>à la</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>la réunion</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suis en</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en réunion</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vais partir</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partir en</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              je  réunion  vacances    à  vais  suis   en  partir   la  </s>  \\\n",
       "<s> je       0.0      0.0       0.0  0.0   0.0   2.0  0.0     0.0  0.0   0.0   \n",
       "je suis      0.0      0.0       0.0  0.0   0.0   0.0  2.0     0.0  0.0   0.0   \n",
       "suis en      0.0      0.0       1.0  0.0   0.0   0.0  0.0     0.0  0.0   0.0   \n",
       "en vacances  0.0      0.0       0.0  0.0   0.0   0.0  0.0     0.0  0.0   2.0   \n",
       "<s> je       0.0      0.0       0.0  0.0   2.0   0.0  0.0     0.0  0.0   0.0   \n",
       "je vais      0.0      0.0       0.0  0.0   0.0   0.0  0.0     2.0  0.0   0.0   \n",
       "vais partir  0.0      0.0       0.0  1.0   0.0   0.0  0.0     0.0  0.0   0.0   \n",
       "partir à     0.0      0.0       0.0  0.0   0.0   0.0  0.0     0.0  1.0   0.0   \n",
       "à la         0.0      1.0       0.0  0.0   0.0   0.0  0.0     0.0  0.0   0.0   \n",
       "la réunion   0.0      0.0       0.0  0.0   0.0   0.0  0.0     0.0  0.0   1.0   \n",
       "suis en      0.0      1.0       0.0  0.0   0.0   0.0  0.0     0.0  0.0   0.0   \n",
       "en réunion   0.0      0.0       0.0  0.0   0.0   0.0  0.0     0.0  0.0   1.0   \n",
       "vais partir  0.0      0.0       0.0  0.0   0.0   0.0  1.0     0.0  0.0   0.0   \n",
       "partir en    0.0      0.0       1.0  0.0   0.0   0.0  0.0     0.0  0.0   0.0   \n",
       "\n",
       "             <unk>  \n",
       "<s> je         0.0  \n",
       "je suis        0.0  \n",
       "suis en        0.0  \n",
       "en vacances    0.0  \n",
       "<s> je         0.0  \n",
       "je vais        0.0  \n",
       "vais partir    0.0  \n",
       "partir à       0.0  \n",
       "à la           0.0  \n",
       "la réunion     0.0  \n",
       "suis en        0.0  \n",
       "en réunion     0.0  \n",
       "vais partir    0.0  \n",
       "partir en      0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show trigram counts\n",
    "display(make_count_matrix(trigram_counts, unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n",
    "    count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)\n",
    "    count_matrix += k\n",
    "    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n",
    "    return prob_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>je</th>\n",
       "      <th>réunion</th>\n",
       "      <th>vacances</th>\n",
       "      <th>à</th>\n",
       "      <th>vais</th>\n",
       "      <th>suis</th>\n",
       "      <th>en</th>\n",
       "      <th>partir</th>\n",
       "      <th>la</th>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>je</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suis</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vacances</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>je</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vais</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partir</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>à</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>la</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>réunion</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partir</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                je   réunion  vacances         à      vais      suis  \\\n",
       "<s>       0.333333  0.066667  0.066667  0.066667  0.066667  0.066667   \n",
       "je        0.076923  0.076923  0.076923  0.076923  0.076923  0.230769   \n",
       "suis      0.076923  0.076923  0.076923  0.076923  0.076923  0.076923   \n",
       "en        0.076923  0.076923  0.230769  0.076923  0.076923  0.076923   \n",
       "vacances  0.076923  0.076923  0.076923  0.076923  0.076923  0.076923   \n",
       "je        0.076923  0.076923  0.076923  0.076923  0.230769  0.076923   \n",
       "vais      0.076923  0.076923  0.076923  0.076923  0.076923  0.076923   \n",
       "partir    0.083333  0.083333  0.083333  0.166667  0.083333  0.083333   \n",
       "à         0.083333  0.083333  0.083333  0.083333  0.083333  0.083333   \n",
       "la        0.083333  0.166667  0.083333  0.083333  0.083333  0.083333   \n",
       "réunion   0.076923  0.076923  0.076923  0.076923  0.076923  0.076923   \n",
       "en        0.083333  0.166667  0.083333  0.083333  0.083333  0.083333   \n",
       "partir    0.083333  0.083333  0.083333  0.083333  0.083333  0.083333   \n",
       "\n",
       "                en    partir        la      </s>     <unk>  \n",
       "<s>       0.066667  0.066667  0.066667  0.066667  0.066667  \n",
       "je        0.076923  0.076923  0.076923  0.076923  0.076923  \n",
       "suis      0.230769  0.076923  0.076923  0.076923  0.076923  \n",
       "en        0.076923  0.076923  0.076923  0.076923  0.076923  \n",
       "vacances  0.076923  0.076923  0.076923  0.230769  0.076923  \n",
       "je        0.076923  0.076923  0.076923  0.076923  0.076923  \n",
       "vais      0.076923  0.230769  0.076923  0.076923  0.076923  \n",
       "partir    0.083333  0.083333  0.083333  0.083333  0.083333  \n",
       "à         0.083333  0.083333  0.166667  0.083333  0.083333  \n",
       "la        0.083333  0.083333  0.083333  0.083333  0.083333  \n",
       "réunion   0.076923  0.076923  0.076923  0.230769  0.076923  \n",
       "en        0.083333  0.083333  0.083333  0.083333  0.083333  \n",
       "partir    0.166667  0.083333  0.083333  0.083333  0.083333  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(make_probability_matrix(bigram_counts, unique_words, k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>je</th>\n",
       "      <th>réunion</th>\n",
       "      <th>vacances</th>\n",
       "      <th>à</th>\n",
       "      <th>vais</th>\n",
       "      <th>suis</th>\n",
       "      <th>en</th>\n",
       "      <th>partir</th>\n",
       "      <th>la</th>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;s&gt; je</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>je suis</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suis en</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en vacances</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;s&gt; je</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>je vais</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vais partir</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partir à</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>à la</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>la réunion</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suis en</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en réunion</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vais partir</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partir en</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   je   réunion  vacances         à      vais      suis  \\\n",
       "<s> je       0.076923  0.076923  0.076923  0.076923  0.076923  0.230769   \n",
       "je suis      0.076923  0.076923  0.076923  0.076923  0.076923  0.076923   \n",
       "suis en      0.083333  0.083333  0.166667  0.083333  0.083333  0.083333   \n",
       "en vacances  0.076923  0.076923  0.076923  0.076923  0.076923  0.076923   \n",
       "<s> je       0.076923  0.076923  0.076923  0.076923  0.230769  0.076923   \n",
       "je vais      0.076923  0.076923  0.076923  0.076923  0.076923  0.076923   \n",
       "vais partir  0.083333  0.083333  0.083333  0.166667  0.083333  0.083333   \n",
       "partir à     0.083333  0.083333  0.083333  0.083333  0.083333  0.083333   \n",
       "à la         0.083333  0.166667  0.083333  0.083333  0.083333  0.083333   \n",
       "la réunion   0.083333  0.083333  0.083333  0.083333  0.083333  0.083333   \n",
       "suis en      0.083333  0.166667  0.083333  0.083333  0.083333  0.083333   \n",
       "en réunion   0.083333  0.083333  0.083333  0.083333  0.083333  0.083333   \n",
       "vais partir  0.083333  0.083333  0.083333  0.083333  0.083333  0.083333   \n",
       "partir en    0.083333  0.083333  0.166667  0.083333  0.083333  0.083333   \n",
       "\n",
       "                   en    partir        la      </s>     <unk>  \n",
       "<s> je       0.076923  0.076923  0.076923  0.076923  0.076923  \n",
       "je suis      0.230769  0.076923  0.076923  0.076923  0.076923  \n",
       "suis en      0.083333  0.083333  0.083333  0.083333  0.083333  \n",
       "en vacances  0.076923  0.076923  0.076923  0.230769  0.076923  \n",
       "<s> je       0.076923  0.076923  0.076923  0.076923  0.076923  \n",
       "je vais      0.076923  0.230769  0.076923  0.076923  0.076923  \n",
       "vais partir  0.083333  0.083333  0.083333  0.083333  0.083333  \n",
       "partir à     0.083333  0.083333  0.166667  0.083333  0.083333  \n",
       "à la         0.083333  0.083333  0.083333  0.083333  0.083333  \n",
       "la réunion   0.083333  0.083333  0.083333  0.166667  0.083333  \n",
       "suis en      0.083333  0.083333  0.083333  0.083333  0.083333  \n",
       "en réunion   0.083333  0.083333  0.083333  0.166667  0.083333  \n",
       "vais partir  0.166667  0.083333  0.083333  0.083333  0.083333  \n",
       "partir en    0.083333  0.083333  0.083333  0.083333  0.083333  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(make_probability_matrix(trigram_counts, unique_words, k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
    "    \n",
    "    n = len(list(n_gram_counts.keys())[0].split()) \n",
    "    previous_n_gram = ' '.join(previous_tokens.split()[-n:])\n",
    "    probabilities = estimate_probabilities(previous_n_gram,\n",
    "                                           n_gram_counts, n_plus1_gram_counts,\n",
    "                                           vocabulary, k=k)\n",
    "\n",
    "    suggestion = None\n",
    "    max_prob = 0\n",
    "    for word, prob in probabilities.items(): \n",
    "        if prob > max_prob: \n",
    "            suggestion = word            \n",
    "            max_prob = prob    \n",
    "    return suggestion, max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pour les tokens 'je vais',la suggestion est le mot 'partir' avec une probabilité de 0.2308.\n"
     ]
    }
   ],
   "source": [
    "previous_tokens = \"je vais\"\n",
    "tmp_suggest1 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)\n",
    "print(f\"Pour les tokens 'je vais',la suggestion est le mot '{tmp_suggest1[0]}' avec une probabilité de {tmp_suggest1[1]:.4f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut calculer la perplexité pour évaluer le modèle. Cette dernière est donnée par :\n",
    "\n",
    "$$ PP(W) =\\sqrt[N]{ \\prod_{t=n+1}^N \\frac{1}{P(w_t | w_{t-n} \\cdots w_{t-1})} } \\tag{4}$$\n",
    "\n",
    "Avec $N$ la longueur de la phrase et $n$ la taille des n-grams (par exemple 2 dans le cas des bigrams). On cherche à minimiser la perplexité du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0,\n",
    "                         start_token='<s>', end_token='</s>', unk_token='<unk>'):\n",
    "    \n",
    "    n = len(list(n_gram_counts.keys())[0].split()) \n",
    "    tokens = [start_token] + sentence + [end_token]\n",
    "    N = len(tokens)\n",
    "    \n",
    "    product_pi = 1.0\n",
    "  \n",
    "    for t in range(n, N): \n",
    "        n_gram = tokens[t-n:t]\n",
    "        word = tokens[t]\n",
    "        \n",
    "        probability = estimate_probability(word, ' '.join(n_gram), \n",
    "                                           n_gram_counts, n_plus1_gram_counts, \n",
    "                                           len(unique_words), k=1)\n",
    "        product_pi *= 1 / probability\n",
    "\n",
    "    perplexity = product_pi**(1/float(N))\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La perplexité pour la première phrase du corpus est : 2.9089.\n",
      "La perplexité pour la phrase test est : 6.6343.\n"
     ]
    }
   ],
   "source": [
    "perplexity_train1 = calculate_perplexity(sentences[0],\n",
    "                                         unigram_counts, bigram_counts,\n",
    "                                         len(unique_words), k=1.0)\n",
    "print(f\"La perplexité pour la première phrase du corpus est : {perplexity_train1:.4f}.\")\n",
    "\n",
    "\n",
    "perplexity_train1 = calculate_perplexity(['Tu' ,'pars', 'ou', 'en', 'vacances', '?'],\n",
    "                                         unigram_counts, bigram_counts,\n",
    "                                         len(unique_words), k=1.0)\n",
    "print(f\"La perplexité pour la phrase test est : {perplexity_train1:.4f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2: les modèles de langues avec réseaux de neurones : GPT-2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'ai entrainé un modèle GPT-2 <span class=\"badge badge-secondary\">(Radford et al., 2019)</span> sur 50M de phrases extraites du corpus OSCAR <span class=\"badge badge-secondary\">(Suárez et al., 2019)</span>. Le modèle a ensuite été fine-tuné (on a continué l'entrainement) sur le Tome 2 de Harry Potter. Ce modèle est une architecture de réseaux de neurones assez connu pour les modèles de langues. Il permet de générer du texte de manière assez réaliste. On va utiliser la librairie `transformers` pour utiliser le modèle. **Vous devez récupérer les poids du modèles sur le moodle (fichier french-gpt2-hp)**.\n",
    "\n",
    "<span class=\"badge badge-secondary\">(Radford et al., 2019)</span> Alec Radford, Jeffrey Wu, Rewon Child, David Luan and Dario Amodei. \"Better Language Models and Their Implications.\"https://openai.com/blog/better-language-models/\n",
    "\n",
    "<span class=\"badge badge-secondary\">(Suárez et al., 2019)</span> Suárez, Pedro Javier Ortiz, Benoît Sagot, and Laurent Romary. \"Asynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\" 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7). Leibniz-Institut für Deutsche Sprache, 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "Some weights or buffers of the PyTorch model TFGPT2LMHeadModel were not initialized from the TF 2.0 model and are newly initialized: ['transformer.h.10.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.8.attn.bias', 'lm_head.weight', 'transformer.h.2.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.9.attn.bias', 'transformer.h.7.attn.bias', 'transformer.h.1.attn.bias', 'transformer.h.0.attn.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./french-gpt2-hp\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"./french-gpt2-hp\", \n",
    "                                          pad_token_id=tokenizer.eos_token_id, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(\n",
    "    \"Dans son mouvement, la queue du Basilic lui avait jeté le Choixpeau magique à la tête. \",\n",
    "    return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 2 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Dans son mouvement, la queue du Basilic lui avait jeté le Choixpeau magique à la tête.  —Qu'est-ce que tu fais là?\n",
      "demanda Harry.\n",
      "—Qu'est-ce que tu fais là\n"
     ]
    }
   ],
   "source": [
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = model.generate(input_ids, max_length=50)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 2 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Dans son mouvement, la queue du Basilic lui avait jeté le Choixpeau magique à la tête.  —Qu'est-ce qui se passe?\n",
      "demanda-t-il d'une voix aiguë.\n",
      "—Quoi?—–—–demarre aussitôt de la foule, les yeux fixés sur le visage de Malefoy qui avait l'air de plus en plus livide, comme s'il n'avait pas eu le temps de prononcer le moindre mot...  Harry se précipita dans la salle commune des Gryffondor, à côté de Ron et de Hermione, mais il ne fut pas surpris de voir que le professeur McGonagall était en train de dire quelque chose sur la Chambre des Secrets et qu'elle ne semblait pas convaincue que c'était la meilleure chose à faire...\n",
      "Il y eut un long silence, puis il se tourna vers Ron, le regard perdu dans ses pensées, et le silence qui régnait autour de lui se répercut en\n"
     ]
    }
   ],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids, \n",
    "    max_length=200, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(\n",
    "    \"Assis un peu plus loin, Harry reconnut Gilderoy Lockhart, vêtu d'une robe de sorcier bleu-vert.\",\n",
    "    return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 2 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Assis un peu plus loin, Harry reconnut Gilderoy Lockhart, vêtu d'une robe de sorcier bleu-vert.\n",
      "—Qu'est-ce qu'il y a?\n",
      "demanda-t-il en s'efforçant de ne pas faire de bruit, mais il n'eut pas le temps de prononcer le moindre mot, et il se laissa tomber sur le sol humide et humide de la salle commune de Gryffondor, à côté de Ron et de Hermione qui le regardaient avec des yeux ronds et des cheveux bouclés qui lui tombaient sur les yeux. Mais il ne fut pas surpris de voir que le professeur McGonagall était le seul à l'avoir vu, alors que les autres élèves de Serpentard étaient assis côte à côte sur un banc de Quidditch, au fond duquel était écrit en lettres noires :  —Viens, dit Harry, je t'ai dit que j'étais le plus grand sorcier de tous les temps\n"
     ]
    }
   ],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids, \n",
    "    max_length=200, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
