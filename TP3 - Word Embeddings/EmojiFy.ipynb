{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copyright 2021 Antoine Simoulin.\n",
    "\n",
    "<i>Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "Icons made by <a href=\"https://www.flaticon.com/authors/freepik\" title=\"Freepik\">Freepik</a>, <a href=\"https://www.flaticon.com/authors/pixel-perfect\" title=\"Pixel perfect\">Pixel perfect</a>, <a href=\"https://www.flaticon.com/authors/becris\" title=\"Becris\">Becris</a>, <a href=\"https://www.flaticon.com/authors/smashicons\" title=\"Smashicons\">Smashicons</a>, <a href=\"https://www.flaticon.com/authors/srip\" title=\"srip\">srip</a>, <a href=\"https://www.flaticon.com/authors/adib-sulthon\" title=\"Adib\">Adib</a>, <a href=\"https://www.flaticon.com/authors/flat-icons\" title=\"Flat Icons\">Flat Icons</a> and <a href=\"https://www.flaticon.com/authors/dinosoftlabs\" title=\"Pixel perfect\">DinosoftLabs</a> from <a href=\"https://www.flaticon.com/\" title=\"Flaticon\"> www.flaticon.com</a></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 3 : Words Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP3%20-%20Word%20Embeddings/tp3-header.png?raw=True\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va s'appuyer sur le corpus collect√© par <span class=\"badge badge-secondary\">([Panckhurst et al., 2016](#panckhurst-2016))</span> qui rassemble 88,000 sms collect√©s dans la r√©gion de Montpellier. Le corpus a √©t√© d√©-identifi√© (en particulier, les noms sont remplac√©s par [ _forename_ ]). Pour chaque sms, on a identifi√© les Emojis dans le texte.\n",
    "\n",
    "Il y avait beaucoup de type d'Emojis. Dans le TP, ils ont √©t√© simplifi√©s selon le tableau suivant. Tous les Emojis de la colonne `Emoji list` ont √©t√© remplac√© par l'emoji de la colonne `Generic`. Dans le TP les Emojis n'apparaissent pas dans le texte du sms car on cherche √† les pr√©dire.\n",
    "\n",
    "\n",
    "| Generic Emoji  | Emoji list                                                         |\n",
    "|:--------------:|:------------------------------------------------------------------:|\n",
    "| üòÉ             | '=P', ':)', ':P', '=)', ':p', ':d', ':-)', '=D', ':D', '^^'        |\n",
    "| üò≤             | ':O', 'o_o', ':o', ':&'                                            | \n",
    "| üòî             | '\"-.-'''\", '<_>', '-_-', \"--'\", \"-.-'\", '-.-', \"-.-''\", \"-\\_-'\"    | \n",
    "| üò†             | ':/', ':-/', ':-(', ':(', ':-<'                                    | \n",
    "| üòÜ             | '>.<', '¬§.¬§', '<>','><', '*.*', 'xd', 'XD', 'xD', 'x)',';)', ';-)' | \n",
    "| üòç             | '</3', '<3'                                                        | \n",
    " \n",
    "\n",
    "Finalement pour le TP, on a filtr√© le jeu de donn√©es pour ne conserver que les sms contenant qu'un seul Emoji. On a par ailleurs <i>down sampl√©</i> les classes majoritaires pour limiter le d√©s√©quilibre du jeu de donn√©es. En effet les sms avec un smiley üòÉ √©tait largement sur-repr√©sent√©s.\n",
    "\n",
    "<b>L'objet du TP est de pr√©dire l'√©moji associ√© √† chaque message. Pour cela on vectorisera le texte en utilisant les m√©thodes d'embeddings.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Check environment\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  IN_COLAB = True\n",
    "else:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "  # ‚ö†Ô∏è Execute only if running in Colab\n",
    "  !pip install -q scikit-learn==0.23.2 matplotlib==3.1.3 pandas==1.1.3 gensim==3.8.1 torch==1.6.0 torchvision==0.7.0\n",
    "  !pip install skorch==0.10.0\n",
    "  # then restart runtime environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "import os, sys\n",
    "\n",
    "# IPython automatically reload all changed code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Inline Figures with matplotlib\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import extrenal modules\n",
    "import urllib.request\n",
    "\n",
    "class_names = ['happy', 'joke', 'astonished', 'angry', 'bored', 'heart']\n",
    "repo_url = 'https://raw.githubusercontent.com/AntoineSimoulin/m2-data-sciences/master/'\n",
    "\n",
    "_ = urllib.request.urlretrieve(repo_url + 'src/plots.py', 'plots.py')\n",
    "\n",
    "if not os.path.exists('smileys'):\n",
    "    os.makedirs('smileys')\n",
    "\n",
    "for c in class_names:\n",
    "    _ = urllib.request.urlretrieve(\n",
    "        repo_url + 'TP3%20-%20Word%20Embeddings/smileys/{}.png'.format(c), \n",
    "        'smileys/{}.png'.format(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va utiliser les embeddings d√©j√† entrain√© que nous avons manipul√© au cours pr√©c√©dent. Pour limiter la taille du fichier d'embeddings, on a sauvegard√© que les `10,000` mots les plus fr√©quents. <b>Vous devez r√©cup√©rer le fichier d'embeddings aisni que le jeu de donn√©es directement sur le [Moodle](https://moodle.u-paris.fr/course/view.php?id=11048).</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = KeyedVectors.load_word2vec_format(\"oscar.fr.300.10k.model\")\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(w2v_model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On cr√©e un array avec les 10,000 premiers mots et on cr√©e le dictionaire de vocabulaire\n",
    "\n",
    "word_count = {k: w2v_model.vocab[k].count for k in w2v_model.vocab}\n",
    "word_count = Counter(word_count)\n",
    "word_count.most_common(10)\n",
    "\n",
    "idx2w = {i: w for (i, (w, f)) in enumerate(word_count.most_common(10000), 2)}\n",
    "idx2w[0] = 'unk'\n",
    "idx2w[1] = 'pad'\n",
    "w2idx = {w: i for (i, (w, f)) in enumerate(word_count.most_common(10000), 2)}\n",
    "w2idx['unk'] = 0\n",
    "w2idx['pad'] = 1\n",
    "\n",
    "embeddings_vectors = [w2v_model[w] for (w, f) in word_count.most_common(10000)]\n",
    "word2vec_embeddings = np.vstack(embeddings_vectors)\n",
    "word2vec_embeddings = np.concatenate((word2vec_embeddings, np.zeros_like(word2vec_embeddings[0:2])), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2idx['Oh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_embeddings[3664][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model['Oh'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('emojis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.loc[3, 'sms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['happy', 'joke', 'astonished', 'angry', 'bored', 'heart']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va utiliser la m√™me fonction de tokenization qui a √©t√© utilis√©e pour entrainer les embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_pattern = re.compile(r\"(\\->|(?::\\)|:-\\)|:\\(|:-\\(|;\\);-\\)|:-O|8-|:P|:D|:\\||:S|:\\$|:@|8o\\||\\+o\\(|\\(H\\)|\\(C\\)|\\(\\?\\))|(?:[\\d.,]+)|([^\\s\\w0-9])\\2*|(?:[\\w0-9\\.]+['‚Äô]?)(?<!\\.))\")\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = [groups[0] for groups in re.findall(token_pattern, str(text))]\n",
    "    tokens = [t.strip() for t in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['tokens'] = dataset['sms'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration de donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>üìù Exercice :</b> Observer la distribution des classes.</p>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>üìù Exercice :</b> Evaluer la proportion de tokens qui sont hors du vocabulaire des embeddings.</p>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les embeddings de mots permettent de repr√©senter chaque <i>token</i> par un vecteur. Pour obtenir un vecteur qui repr√©sente le sms, on va agr√©ger les diff√©rents mots du texte. On consid√©rera plusieurs fonctions d'agr√©gation : la somme, la moyenne, me maximum ou le minimum.\n",
    "\n",
    "En pratique nous verrons dans le dernier cours d'ouverture qu'il existe des m√©thodes plus √©volu√©es pour composer les mots de la phrase. N√©anmoins une simple fonction d'agr√©gation nous donnera d√©j√† une bonne <i>baseline</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP3%20-%20Word%20Embeddings/model.png?raw=True\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>üìù Exercice :</b> Ecrire une fonction qui permet de vectoriser un sms.</p>\n",
    "</div>\n",
    "<hr>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/vectorize_1.py\n",
    "\n",
    "def vectorize(tokens, agg_method='mean'):\n",
    "    \n",
    "    #TODO √† compl√©ter\n",
    "    \n",
    "    # associer chaque token √† son embedding. \n",
    "    # Attention, certains tokens peuvent ne pas √™tre dans le vocabulaire\n",
    "    \n",
    "    # Agr√©ger les repr√©sentations de chaque token.\n",
    "    # Le vecteur de sortie doit √™tre de taille (300, )\n",
    "    \n",
    "    if agg_method == 'mean':\n",
    "        sentence_embedding = \n",
    "    elif agg_method == 'max':\n",
    "        sentence_embedding = \n",
    "    elif agg_method == 'sum':\n",
    "        sentence_embedding = \n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize(dataset['tokens'][0], agg_method='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voudrait attribuer un poids moins important aux embeddings des mots moins caract√©ristiques. Pour √ßa, on voudrait pond√©rer la contribution des vecteurs de chaque mot en fonction de leur score TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP3%20-%20Word%20Embeddings/model-tfidf.png?raw=True\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>üìù Exercice :</b> Utiliser la pond√©ration TF-IDF pour pond√©rer chacun des vecteurs.</p>\n",
    "</div>\n",
    "<hr>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=lambda x: x,\n",
    "                                   lowercase=False)\n",
    "\n",
    "tfidf_vectorizer.fit(dataset['tokens'])\n",
    "\n",
    "w2idx_tfidf = {w: idx for (idx, w) in enumerate(tfidf_vectorizer.get_feature_names())}\n",
    "idx_tfidf2w = {idx: w for (idx, w) in enumerate(tfidf_vectorizer.get_feature_names())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/vectorize_2.py\n",
    "\n",
    "def vectorize(tokens, agg_method='mean', tfidf_vectorizer=None):\n",
    "    \n",
    "    #TODO √† compl√©ter\n",
    "    \n",
    "    if agg_method == 'mean':\n",
    "        sentence_embedding = \n",
    "    elif agg_method == 'max':\n",
    "        sentence_embedding = \n",
    "    elif agg_method == 'sum':\n",
    "        sentence_embedding = \n",
    "    elif agg_method == 'tfidf':\n",
    "        \n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [vectorize(sms) for sms in dataset['tokens']]\n",
    "X = np.array(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va int√©grer la fonction `vectorize` dans un module compatible avec les fonctions de `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>üìù Exercice :</b> Int√©grer votre fonction de vectorization dans la classe Vectorizer ci-dessous. Vous devez simoplement la copier/coller en repla√ßant tfidf_vectorizer par self.tfidf_vectorizer car c'est maintenant un attribut de la class</p>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 choses √† faire pour l'excercice sur la class Vectorizer :\n",
    "# copier votre fonction vectorize dans la class\n",
    "# ajouter l'argument self dans la fonction vectorize\n",
    "# supprimer l'argument tfidf_vectorizer de la fonction vectorize\n",
    "# remplacer toutes les occurences de agg_method par self.agg_method dans la fonction vectorize\n",
    "# supprimer l'argument agg_method de la fonction vectorize\n",
    "# remplacer toutes les occurences de w2idx_tfidf par self.w2idx_tfidf dans la fonction vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/vectorizer.py\n",
    "\n",
    "class Vectorizer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, agg_method='mean', normalize=False):\n",
    "        self.agg_method = agg_method\n",
    "        self.normalize = normalize\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(tokenizer=lambda x: x,\n",
    "                                                lowercase=False,\n",
    "                                                token_pattern=None)\n",
    "        \n",
    "    def vectorize(self, tokens):\n",
    "    \n",
    "        token_embeddings = []\n",
    "        vect_vide = np.zeros_like(embeddings_vectors[0])\n",
    "\n",
    "        # associer chaque token √† son embedding. \n",
    "        # Attention, certains tokens peuvent ne pas √™tre dans le vocabulaire\n",
    "        \n",
    "        # self.tfidf_vectorizer\n",
    "\n",
    "        for t in tokens:\n",
    "            if t in w2idx:\n",
    "                t_idx = w2idx[t]\n",
    "                token_embeddings.append(embeddings_vectors[t_idx])\n",
    "            else:\n",
    "                token_embeddings.append(vect_vide)\n",
    "\n",
    "        token_embeddings_arr = np.array(token_embeddings)\n",
    "\n",
    "        # Agr√©ger les repr√©sentations de chaque token.\n",
    "        # Le vecteur de sortie doit √™tre de taille (300, )\n",
    "\n",
    "        if self.agg_method == 'mean':\n",
    "            sentence_embedding = np.mean(token_embeddings_arr, axis=0)\n",
    "        elif self.agg_method == 'max':\n",
    "            sentence_embedding = np.max(token_embeddings_arr, axis=0)\n",
    "        elif self.agg_method == 'sum':\n",
    "            sentence_embedding = np.sum(token_embeddings_arr, axis=0)\n",
    "        elif self.agg_method == 'tfidf':\n",
    "            pass\n",
    "\n",
    "        return sentence_embedding\n",
    "\n",
    "    \n",
    "    def _vectorize(self, tokens):\n",
    "        return vectorize(tokens)\n",
    "    \n",
    "    def fit(self, X, y=None): \n",
    "        self.tfidf_vectorizer.fit(X['tokens'])\n",
    "        self.w2idx_tfidf = {w: idx for (idx, w) in enumerate(self.tfidf_vectorizer.get_feature_names())}\n",
    "        self.idx_tfidf2w = {idx: w for (idx, w) in enumerate(self.tfidf_vectorizer.get_feature_names())}\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None, eps=1e-12):\n",
    "        X = [self.vectorize(sms) for sms in X['tokens']]\n",
    "        X = np.array(X)\n",
    "\n",
    "        if self.normalize:\n",
    "            X = X / np.linalg.norm(X + eps, axis=1, keepdims=True)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer(agg_method='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On compare deux algorithmes de classification :  Une r√©gression logistique et un SVM ou l'on p√©nalise les classes majoritaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(\n",
    "    dataset, test_size=0.33, random_state=42)\n",
    "\n",
    "y_train = X_train[['happy', 'joke', 'astonished', 'angry', 'bored', 'heart']].astype(int).values\n",
    "y_train = [x.tolist().index(1) for x in y_train]\n",
    "\n",
    "y_test = X_test[['happy', 'joke', 'astonished', 'angry', 'bored', 'heart']].astype(int).values\n",
    "y_test = [x.tolist().index(1) for x in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogReg_pipeline = Pipeline([\n",
    "    ('vect', Vectorizer('tfidf')),\n",
    "    ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'))),\n",
    "])\n",
    "\n",
    "\n",
    "# Training logistic regression model on train data\n",
    "LogReg_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Infering data on test set\n",
    "prediction_LogReg = LogReg_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_pipeline = Pipeline([\n",
    "    ('vect', Vectorizer('tfidf')),\n",
    "    ('clf', OneVsRestClassifier(SVC(kernel='linear', \n",
    "                                    class_weight='balanced', # penalize\n",
    "                                    probability=True), n_jobs=-1))\n",
    "])\n",
    "\n",
    "\n",
    "SVC_pipeline.fit(X_train, y_train)\n",
    "prediction_SVC = SVC_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from plots import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test accuracy is {}'.format(accuracy_score(y_test, prediction_SVC)))\n",
    "print('Test ROC socre is {}'.format(roc_auc_score(np.eye(np.max(y_test) + 1)[y_test],\n",
    "                                                  SVC_pipeline.predict_proba(X_test), \n",
    "                                                  multi_class='ovo')))\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(y_test, prediction_SVC), \n",
    "                      classes=class_names, \n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test accuracy is {}'.format(accuracy_score(y_test, prediction_LogReg)))\n",
    "print('Test ROC socre is {}'.format(roc_auc_score(np.eye(np.max(y_test) + 1)[y_test],\n",
    "                                                  LogReg_pipeline.predict_proba(X_test), \n",
    "                                                  multi_class='ovo')))\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(y_test, prediction_LogReg), \n",
    "                      classes=class_names, \n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>üìù Exercice :</b> Quelle mesure de performance vous semble le plus adapt√©e pour ce cas d'usage ?</p>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>üìù Exercice :</b> Comparer les r√©sultats obtenus avec les deux algorithmes de classifications</p>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>üìù Exercice :</b> Comparer les diff√©rentes m√©thodes d'agr√©gation propos√©es. (Mean, Max, Sum, Moyenne pond√©r√©e par le TF-IDF)</p>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>üìù Exercice (Bonus) :</b> Comparer les r√©sultats obtenus avec un r√©seau de neurones r√©curent (RNN).</p>\n",
    "</div>\n",
    "<hr>\n",
    "\n",
    "Nous ferons une ouverture sur les r√©seaux de neurones et leur utilisation pour le texte lors de la derni√®re s√©ance. N√©anmoins, cela peut √™tre une bonne occasion de se familiariser avec leur utilisation. Nous allons utiliser la librairie [skorch](https://github.com/skorch-dev/skorch) qui est un wrapper de la librairie [PyTorch](https://pytorch.org/) compatible avec [scikit-learn](https://scikit-learn.org/). Cela permet en particulier de simplifier les aspects d'optimisation. Ici on utilise un r√©seau r√©curent de type LSTM <span class=\"badge badge-secondary\">([Cho and al., 2014](#cho-2014)</span>, <span class=\"badge badge-secondary\">[Hochreiter and Schmidhuber, 1997](#schmidhuber-1997))</span>. Les r√©seaux de neurones r√©currents mod√©lisent les phrases comme des s√©quences d‚Äôembeddings de mots. Ils traitent l‚Äôentr√©e s√©quentiellement. A chaque √©tape, le vecteur de sortie est calcul√© en fonction de l‚Äôembedding du mot courant et de l‚Äô√©tat cach√© pr√©c√©dent.\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP3%20-%20Word%20Embeddings/lstm.png?raw=True\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from skorch import NeuralNet, NeuralNetClassifier\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, n_classes, embeddings_weights, \n",
    "        hidden_dim=100, embedding_dim=300, dropout=0.5):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings_weights, sparse=True)\n",
    "        self.embeddings.weight.requires_grad = False\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.dense = nn.Linear(hidden_dim, n_classes)\n",
    "        self.dropout = dropout\n",
    "        self.drop = nn.Dropout(self.dropout)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X, X_len = X\n",
    "        X = self.embeddings(X)\n",
    "\n",
    "        # On utilise une m√©thode de pytorch pour tenir compte de la longueur des phrases\n",
    "        # et ainsi s'adapter au padding.\n",
    "        X_packed = pack_padded_sequence(X, X_len, batch_first=True, enforce_sorted=False)\n",
    "        X_packed, (h, c) = self.lstm(X_packed)# [1][0] # .transpose(0, 1)\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM\n",
    "        X, output_lengths = pad_packed_sequence(X_packed, batch_first=True)\n",
    "\n",
    "        # X = torch.sigmoid(X)\n",
    "        out = F.softmax(self.dense(h.squeeze()), dim=-1)\n",
    "        return out\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced', classes=range(len(class_names)), y=y_train)\n",
    "# On va donner un poids plus important aux classes minoritaires \n",
    "# mais pas proportionnel √† leur distribution pour ne pas trop les favoriser \n",
    "# au d√©triment de la pr√©cision globale\n",
    "class_weights = [1, 1, 1.3, 1, 1.3, 1]\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "net = NeuralNetClassifier( # NeuralNet\n",
    "    RNNClassifier(len(class_names), torch.tensor(word2vec_embeddings)),\n",
    "    max_epochs=10,\n",
    "    lr=0.001,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    criterion=torch.nn.NLLLoss,\n",
    "    criterion__weight=class_weights\n",
    ") \n",
    "\n",
    "sequences = [torch.tensor([w2idx.get(t, 0) for t in tokens]) for tokens in X_train['tokens']]\n",
    "sequences_length = torch.tensor([len(s) for s in sequences])\n",
    "# On \"pad\" les s√©quences pour qu'elles aient toutes la m√™me longueur.\n",
    "padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=1)\n",
    "\n",
    "net.fit((padded_sequences, sequences_length), torch.tensor(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_test = [torch.tensor([w2idx.get(t, 0) for t in tokens]) for tokens in X_test['tokens']]\n",
    "sequences_test_length = torch.tensor([len(s) for s in sequences_test])\n",
    "# On \"pad\" les s√©quences pour qu'elles aient toutes la m√™me longueur.\n",
    "padded_sequences_test = pad_sequence(sequences_test, batch_first=True, padding_value=1)\n",
    "prediction_LSTM = net.predict((padded_sequences_test, sequences_test_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test accuracy is {}'.format(accuracy_score(y_test, prediction_LSTM)))\n",
    "print('Test ROC socre is {}'.format(roc_auc_score(np.eye(np.max(y_test) + 1)[y_test],\n",
    "                                                  net.predict_proba((padded_sequences_test, sequences_test_length)), \n",
    "                                                  multi_class='ovo')))\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(y_test, prediction_LSTM), \n",
    "                      classes=class_names, \n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'utilisation des m√©thodes de down-sampling ou up-sampling peut s'av√©rer fastidieux (on va se priver de donn√©es ou en utiliser d'autres plusieurs fois. La s√©lection des donn√©es doit se faire pr√©cis√©mment pour ne pas impacter les capacit√©s de g√©n√©ralisation de l'algorithme). Nous avons pr√©f√©r√© ici utiliser un algorithme qui p√©nalise les classes majoritaires et une mesure d'erreur adapt√©e. Il existe un bon article de blog pour g√©rer les classes d√©s√©quilibr√©es : https://elitedatascience.com/imbalanced-classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut se faire une id√©e des limites et des points fort de l'algorithme en regardant des pr√©dictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humors = ['happy', 'astonished', 'bored', 'angry', 'joke', 'heart']\n",
    "meta_smiley = [b'\\xF0\\x9F\\x98\\x83'.decode(\"utf-8\"),\n",
    "                b'\\xF0\\x9F\\x98\\xB2'.decode(\"utf-8\"),\n",
    "                b'\\xF0\\x9F\\x98\\x94'.decode(\"utf-8\"), \n",
    "                b'\\xF0\\x9F\\x98\\xA0'.decode(\"utf-8\"),\n",
    "                b'\\xF0\\x9F\\x98\\x86'.decode(\"utf-8\"),\n",
    "                b'\\xF0\\x9F\\x98\\x8D'.decode(\"utf-8\")]\n",
    "humor_2_emoji = {h: ms for (h, ms) in zip(humors, meta_smiley)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    idx = np.random.randint(0, len(X_test))\n",
    "    \n",
    "    emojis = humor_2_emoji[class_names[prediction_SVC[idx]]]\n",
    "    \n",
    "    true_emojis = humor_2_emoji[class_names[y_test[idx]]]\n",
    "    print(X_test['sms'].values[idx], '(Pred)', emojis, '(True)', true_emojis, '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi essayer de visualiser plus globalement les repr√©sentations. Pour √ßa on peut utiliser des algorithmes de r√©duction de dimension pour visualiser nos donn√©es. On a d√©j√† parl√© de UMAP et t-SNE. De mani√®re intutive, l'algorithme projete les repr√©sentations dans un espace de plus faible dimension en s'efforcant de respecter les distances entre les points entre l'espace de d√©part et d'arriv√©e. Il permet de visualiser facilement les donn√©es. On va utiliser l'outil `Tensorboard` qui int√®gre les principales m√©thodes de r√©duction de dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_img = Image.open('./smileys/happy.png').convert('RGB')\n",
    "pil_img = pil_img.resize((100, 100)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smileys_images = [f for f in listdir('./smileys') if isfile(join('./smileys', f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_tb = {}\n",
    "for s in smileys_images:\n",
    "    pil_img = Image.open(os.path.join('smileys', s)).convert('RGB')\n",
    "    pil_img = pil_img.resize((25, 25)) \n",
    "    pil_to_tensor = transforms.ToTensor()(pil_img).unsqueeze_(0)\n",
    "    imgs_tb[Path(os.path.join('smileys', s)).stem] = pil_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_embeddings = SummaryWriter(log_dir=os.path.join(\"./tfb/\"))\n",
    "\n",
    "vectorizer = Vectorizer(agg_method='tfidf', normalize=True)\n",
    "emb_test = vectorizer.fit_transform(X_test)\n",
    "\n",
    "writer_embeddings.add_embedding(torch.tensor(emb_test),\n",
    "                                metadata=[(r, s, l) for (r, s, l) in zip(\n",
    "                                    X_test['sms'].values,\n",
    "                                    [humor_2_emoji[class_names[y]] for y in y_test],\n",
    "                                    [humor_2_emoji[class_names[y]] for y in prediction_SVC])\n",
    "                                ],\n",
    "                                label_img=torch.cat([imgs_tb[class_names[y]] for y in y_test]),\n",
    "                                metadata_header=['sms','label', 'prediction'],\n",
    "                                tag=\"SMS-EMB-CLS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour visualiser les repr√©sentations, lancer un tensorboard. Dans un terminal, se placer dans le dossier ou est √©x√©cut√© le notebook et ex√©cuter:\n",
    "\n",
    "```\n",
    "tensorboard --logdir ./tfb/\n",
    "```\n",
    "\n",
    "Dans **Colab** on va lancer le tensorboard directement dans le notebook en √©x√©cutant les cellules suivante :\n",
    "\n",
    "```\n",
    "%load_ext tensorboard\n",
    "```\n",
    "\n",
    "```\n",
    "%tensorboard --logdir ./tfb/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard import notebook\n",
    "notebook.list() # View open TensorBoard instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control TensorBoard display. If no port is provided, \n",
    "# the most recently launched TensorBoard is used\n",
    "notebook.display(port=6006, height=1000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>üìù Exercice :</b> Utiliser les m√©thodes UMAP, PCA et t-SNE pour projeter les donn√©es. Comparez les diff√©rentes m√©thodes de projections et interpr√©tez qualitativement les propri√©t√©s de vos repr√©sentations.</p>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La compatibilit√© entre Jupyter/Colab et Tensorboard est un parfois instable (c.f. https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks). Si vous √™tes sur Colab, vous pouvez t√©l√©charger le dossier directement sur votre ordinateur. T√©l√©chargez le .zip, sur votre ordinateur, dezip√© le.\n",
    "\n",
    "```\n",
    "!zip -r tfb.zip ./tfb/\n",
    "```\n",
    "\n",
    "Sur votre ordinateur, dans un terminal, se placer dans le dossier ou est le notebook et ex√©cuter:\n",
    "\n",
    "```\n",
    "tensorboard --logdir ./tfb/\n",
    "```\n",
    "\n",
    "Vous devriez avoir un visuel comme ci-dessous. Vous pouvez cliquer sur un sms et vous avez √† droite les sms les plus proches en terme de distance cosine comme nous l'avons fait pour word2vec. Par ailleurs chaque sms est repr√©sent√© par le smiley correspondant. Vous pouvez faire varier les m√©thodes de projection dans le panneau de gauche.\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP3%20-%20Word%20Embeddings/tfb-viz.png?raw=True\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö References\n",
    "\n",
    "> <div id=\"panckhurst-2016\">Panckhurst, Rachel, et al. <a href=https://hal.archives-ouvertes.fr/hal-01485560> 88milSMS. A corpus of authentic text messages in French.</a> Banque de corpus CoMeRe. Chanier T.(√©d)-Ortolang: Nancy (2016).</div>\n",
    "\n",
    "> <div id=\"schmidhuber-1997\">Sepp Hochreiter, J√ºrgen Schmidhuber. <a href=https://dl.acm.org/doi/10.1162/neco.1997.9.8.1735> Long Short-Term Memory.</a> Neural Comput. 9(8): 1735-1780 (1997).</div>\n",
    "\n",
    "> <div id=\"cho-2014\">Kyunghyun Cho, Bart van Merrienboer, √áaglar G√ºl√ßehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio: <a href=https://doi.org/10.3115/v1/d14-1179> Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.</a> EMNLP 2014: 1724-1734.</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
