{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Copyright 2021 Antoine SIMOULIN.**\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loi de Zipf et pré-traitements de textes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Contenu du notebook :\n",
    "* Premier contact avec méthodes de Scrapping\n",
    "* Introduction aux expressions régulières et aux opérations de pré-traitements du texte\n",
    "* Validation empirique de la loi de Zipf et sensibilisation à la distribution statistique des corpus"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%capture\n",
    "\n",
    "!pip install --upgrade beautifulsoup4"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from bs4 import BeautifulSoup          # Python parsing library\n",
    "from collections import Counter\n",
    "import nltk                            # NLP library\n",
    "nltk.download('gutenberg')           # Run at first use\n",
    "from nltk.corpus import gutenberg      \n",
    "from nltk.probability import FreqDist  \n",
    "import os \n",
    "import re                              # Regular Expression (Regex) in Python\n",
    "import requests\n",
    "import sys\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# IPython automatically reload all changed code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Inline Figures with matplotlib\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!test -f plots.py || wget -q https://raw.githubusercontent.com/AntoineSimoulin/m2-data-sciences/master/src/plots.py .\n",
    "!test -f split_sentences.py || wget -q https://raw.githubusercontent.com/AntoineSimoulin/m2-data-sciences/master/Cours%201%20-%20Mod%C3%A9lisation%20statistique%20du%20langage/solutions/split_sentences.py -P solutions\n",
    "!test -f tokenize.py || wget -q https://raw.githubusercontent.com/AntoineSimoulin/m2-data-sciences/master/Cours%201%20-%20Mod%C3%A9lisation%20statistique%20du%20langage/solutions/tokenize.py -P solutions\n",
    "\n",
    "sys.path.append('.')\n",
    "from plots import plot_word_counter, plot_zipf"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Littrerature française"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "On cherche à s'assurer de la validité de la loi de Zipf en français. Pour cela on va construire un corpus avec des romans de la littérature française. Dans la démonstration, on propose d'utiliser des romans de Victor Hugo et Marcel Proust mais vous pouvez choisir les auteurs de votre choix.\n",
    "\n",
    "On va récupérer les livres sur le site https://www.gutenberg.org/. Un projet qui rassemble des livres libres de droit. On récupère les tomes des Misérables et de A la recherche du temps perdu."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "book_links = [\n",
    "    # A la recherche du temps perdu\n",
    "    'https://www.gutenberg.org/files/2650/2650-h/2650-h.htm',     # Du côté de chez Swann\n",
    "    'https://www.gutenberg.org/files/2998/2998-h/2998-h.htm',     # À l'ombre des jeunes filles en fleurs Partie 1\n",
    "    'https://www.gutenberg.org/files/2999/2999-h/2999-h.htm',     # À l'ombre des jeunes filles en fleurs Partie 2\n",
    "    'https://www.gutenberg.org/files/3000/3000-h/3000-h.htm',     # À l'ombre des jeunes filles en fleurs Partie 3\n",
    "    'https://www.gutenberg.org/files/8946/8946-h/8946-h.htm',     # Le Côté de Guermantes Partie 1\n",
    "    'https://www.gutenberg.org/files/12999/12999-h/12999-h.htm',  # Le Côté de Guermantes Partie 2\n",
    "    'https://www.gutenberg.org/files/13743/13743-h/13743-h.htm',  # Le Côté de Guermantes Partie 3\n",
    "    'https://www.gutenberg.org/files/15288/15288-h/15288-h.htm',  # Sodome et Gomorrhe Partie 1\n",
    "    'https://www.gutenberg.org/files/15075/15075-h/15075-h.htm',  # Sodome et Gomorrhe Partie 2\n",
    "    'https://www.gutenberg.org/files/60720/60720-h/60720-h.htm',  # La Prisonnière \n",
    "    # Les misérables\n",
    "    'https://www.ebooksgratuits.com/html/hugo_les_miserables_fantine.html',\n",
    "    'https://www.ebooksgratuits.com/html/hugo_les_miserables_cosette.html',\n",
    "    'https://www.ebooksgratuits.com/html/hugo_les_miserables_marius.html',\n",
    "    'https://www.ebooksgratuits.com/html/hugo_les_miserables_idylle_plumet_epopee_st_denis.html',\n",
    "    'https://www.ebooksgratuits.com/html/hugo_les_miserables_idylle_plumet_epopee_st_denis.html',\n",
    "    'https://www.ebooksgratuits.com/html/hugo_les_miserables_jean_valjean.html'\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "On récupère le texte au format HTML à l'aide de la librairie Beautiful Soup. La page HTML est organisée en paragraphes qui suivent le découpage du livre.\n",
    "\n",
    "Attention, le scrapping n'est pas toujours autorisé, il est toujours primordial de s'assurer des licences et disposition légales concernant les données que l'on cherche à récupérer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "\n",
    "paragraphes_all = []\n",
    "\n",
    "for link in tqdm(book_links):\n",
    "  page = requests.get(link) \n",
    "  soup = BeautifulSoup(page.content, 'html.parser')\n",
    "      \n",
    "  paragraphes = soup.select('p', class_='MsoNormal', style='')\n",
    "  paragraphes = [p.get_text(strip=True) for p in paragraphes]\n",
    "  paragraphes = [' '.join(p.split()) for p in paragraphes]\n",
    "  paragraphes_all.extend(paragraphes)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Downloaded {:,d} books for a total of {:,d} paragraphs.\".format(len(book_links), len(paragraphes_all)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(paragraphes_all[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<hr>\n",
    "\n",
    "## Pré-traitements : les expressions régulières"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Chaque paragraphe est consitué d'un unique bloc de texte. En NLP, les corpus sont généralement séparés en phrases et enregistrés dans un fichier ou on retrouve une phrase par ligne."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Les expressions régulières sont un outil très puissant. Elles permetent de rechercher des informations sous une forme standardisée. Dans l'exemple suivant, on peut cherche à extraire les numéros de téléphones. On cherche donc une suite de 10 chiffres avec éventuellement des séparateurs entre les chiffres. On peut également chercher les dates, les adresses, les montants, températures ou tout autre motif. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sample_text = \"\"\"\n",
    "    Bonjour mon numéro de téléphone est le 04.56.55.33.66\n",
    "    Super le mien est 0392020302\n",
    "    Génial, je vous donne également mon tel : 03-02-02-12-89 et celui du bureau : +33 (0) 5 33 19 33 09\n",
    "    Est-ce que vous seriez disponible pour un rendez vous le 06/12/20 vers 13h ?\n",
    "    Les tickets coutent 13€95.\n",
    "\"\"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Les regex permettent de chercher des motifs dans le texte. Ces motifs sont décrits par des expressions standardisées très spécifiques. En python, on peut utiliser les regex à l'aide de la librairie `re`\n",
    "\n",
    "Par exemple, on peut chercher un chiffre dans le texte:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "digit_pattern = re.compile('\\d')\n",
    "\n",
    "digits = re.findall(digit_pattern, sample_text)\n",
    "print(digits)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "On peut également chercher l'ensemble des motifs ou l'on retrouve deux chiffres qui se suivent"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "digit_pattern = re.compile('\\d{2}')\n",
    "\n",
    "digits = re.findall(digit_pattern, sample_text)\n",
    "print(digits)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finalement on peut chercher l'ensemble des motifs ou on retrouve plusieurs chiffres qui se suivent"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "digit_pattern = re.compile('\\d+')\n",
    "# + pour capter si le motif apparait entre 1 et une infinité de fois\n",
    "# * pour capter si le motif apparait entre 0 et une infinité de fois\n",
    "\n",
    "digits = re.findall(digit_pattern, sample_text)\n",
    "print(digits)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "On peut également chercher des groupes plus complexes, par exemple un numéro de téléphone"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "telephone_pattern = re.compile('[\\d\\.\\-\\(\\) +]{5,}')\n",
    "# [] est l'équivalent de \"ou\" pour l'ensemble des motifs de la liste\n",
    "# {5,} est un quantifier plus précis que * ou +. \n",
    "# Ici, on cherche ce motif au moins 5 fois. \n",
    "# {,5} serait au maximum 5 et {5} exactement 5 fois\n",
    "\n",
    "telephone = re.findall(telephone_pattern, sample_text)\n",
    "print(telephone)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "La librairie `re` comprend d'autres méthodes que `re.search`. Par exemple la fonction de substitution `re.sub` qui permet de remplacer le motif par un autre. Un exemple ici poour procéder à une dé-anonymisation des données."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sample_text_anonymized = re.sub(telephone_pattern, ' XX.XX.XX.XX.XX ', sample_text)\n",
    "print(sample_text_anonymized)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "On pourra s'appuyer sur l'outil : https://regex101.com/. Un autre excellent site pour s'entrainer aux expresions régulières : https://alf.nu/RegexGolf et une cheat sheet : https://cheatography.com/davechild/cheat-sheets/regular-expressions/\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<hr>\n",
    "\n",
    "**Exercice 1.** Séparer les paragraphes en phrases."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def split_into_sentences(text):\n",
    "    #TODO à compléter\n",
    "    \n",
    "    return sentences"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# %load solutions/split_sentences.py"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<hr>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bookcorpus = []\n",
    "\n",
    "for p in paragraphes_all:\n",
    "    sentences = split_into_sentences(p)\n",
    "    bookcorpus.extend(sentences)\n",
    "    \n",
    "print(\"Extracted {:,d} sentences\".format(len(bookcorpus)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# save corpus\n",
    "!test -d ./data || mkdir ./data\n",
    "\n",
    "with open(os.path.join('.', 'data', 'miserables_temps_perdu.txt'), 'w') as f:\n",
    "    for s in bookcorpus:\n",
    "        f.write(s + '\\n')\n",
    "        \n",
    "print(\"Saved corpus.\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!cat ./data/miserables_temps_perdu.txt | wc -l"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load corpus\n",
    "with open(os.path.join('data', 'miserables_temps_perdu.txt'), 'r') as f:\n",
    "    sentences = f.readlines()\n",
    "    \n",
    "sentences = [l.strip() for l in sentences if l]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "On va maintenant séparer le corpus en tokens.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Exercice 2.** Effectuer la tokenization du corpus et créer le dictionnaire de vocabulaire"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def tokenize(txt):\n",
    "    \n",
    "    #TODO à compléter\n",
    "    \n",
    "    return tokens"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# %load solutions/tokenize.py"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<hr>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenized_corpus = [tokenize(s) for s in sentences]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n_tokens = sum([len(t) for t in tokenized_corpus])\n",
    "print(\"Corpus contains {:,d} tokens.\".format(n_tokens))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenized_corpus[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenized_corpus_flatten = [ll for l in tokenized_corpus for ll in l]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "assert len(tokenized_corpus_flatten) == n_tokens"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokens_counter = Counter(tokenized_corpus_flatten)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_word_counter(tokens_counter, 50)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_zipf(tokens_counter)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Jul"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "On compare la distribution des mots avec un autre auteur français : le rappeur [Jul](https://fr.wikipedia.org/wiki/Jul_(chanteur)) pour s'assurer que l'on retrouve une forme de distribution similaire. Les fréquences de mots ont été évaluées sur 521 chansons dont les paroles ont été scrappées sur le [AZLyrics](https://www.azlyrics.com/j/jul.html). Les données ont été traitées avec le même script que précédemment. On a directement sauvegardé les fréquences d'apparitions pour l'ensemble des mots du vocabulaire"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!test -f data/jul_freqs.txt || wget -q https://raw.githubusercontent.com/AntoineSimoulin/m2-data-sciences/master/Cours%201%20-%20Mod%C3%A9lisation%20statistique%20du%20langage/data/jul_freqs.txt -P data/"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokens_counter = Counter()\n",
    "\n",
    "with open(os.path.join('data', 'jul_freqs.txt'), 'r') as f:\n",
    "    for line in f:\n",
    "        k, v = line.strip().split('\\t')\n",
    "        tokens_counter[k] = int(v)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokens_counter.most_common(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_word_counter(tokens_counter, 50)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_zipf(tokens_counter)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# English Books"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "On compare avec la litératture anglaise en utilisant en particulier les oeuvres de Jane Austen, Shakespeare. Ces dernières sont accessibles directement en utilisant la librairie `NLTK`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fd = FreqDist()\n",
    "n_words = 0\n",
    "for text in gutenberg.fileids():\n",
    "    for word in gutenberg.words(text):\n",
    "        fd[word] += 1\n",
    "        n_words += 1\n",
    "\n",
    "ranks = []\n",
    "freqs = []\n",
    "for rank, word in enumerate(fd):\n",
    "    ranks.append(rank+1)\n",
    "    freqs.append(fd[word])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_zipf(fd)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_word_counter(fd, 50)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}