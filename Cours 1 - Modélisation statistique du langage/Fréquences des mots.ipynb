{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loi de Zipf et pré-traitements de textes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contenu du notebook :\n",
    "* Premier contact avec méthodes de Scrapping\n",
    "* Introduction aux expressions régulières et aux opérations de pré-traitements du texte\n",
    "* Validation empirique de la loi de Zipf et sensibilisation à la distribution statistique des corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup          # Python parsing library\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import nltk                            # NLP library\n",
    "# nltk.download('gutenberg')           # Run at first use\n",
    "from nltk.corpus import gutenberg      \n",
    "from nltk.probability import FreqDist   \n",
    "import re                              # Regular Expression (Regex) in Python\n",
    "import requests\n",
    "from tqdm import tqdm                  # progress bar\n",
    "\n",
    "# IPython automatically reload all changed code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Inline Figures with matplotlib\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git clone https://github.com/AntoineSimoulin/m2-data-sciences.git .\n",
    "import sys\n",
    "sys.path.append('./m2-data-sciences/src/')\n",
    "from plots import plot_word_counter, plot_zipf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Littrerature française"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On cherche à s'assurer de la validité de la loi de Zipf en français. Pour cela on va construire un corpusavec des romans de la littérature française. Dans la démonstration, on propose d'utiliser des romans de Victor Hugo et Marcel Proust mais vous pouvez choisir les auteurs de votre choix.\n",
    "\n",
    "On va récupérer les livres sur le site https://www.gutenberg.org/. Un projet qui rassemeble des livres libres de droit. On récupère les tomes des Misérables et de A la recherche du temps perdu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_links = [\n",
    "    # A la recherche du temps perdu\n",
    "    'https://www.gutenberg.org/files/2650/2650-h/2650-h.htm',     # Du côté de chez Swann\n",
    "    'https://www.gutenberg.org/files/2998/2998-h/2998-h.htm',     # À l'ombre des jeunes filles en fleurs Partie 1\n",
    "    'https://www.gutenberg.org/files/2999/2999-h/2999-h.htm',     # À l'ombre des jeunes filles en fleurs Partie 2\n",
    "    'https://www.gutenberg.org/files/3000/3000-h/3000-h.htm',     # À l'ombre des jeunes filles en fleurs Partie 3\n",
    "    'https://www.gutenberg.org/files/8946/8946-h/8946-h.htm',     # Le Côté de Guermantes Partie 1\n",
    "    'https://www.gutenberg.org/files/12999/12999-h/12999-h.htm',  # Le Côté de Guermantes Partie 2\n",
    "    'https://www.gutenberg.org/files/13743/13743-h/13743-h.htm',  # Le Côté de Guermantes Partie 3\n",
    "    'https://www.gutenberg.org/files/15288/15288-h/15288-h.htm',  # Sodome et Gomorrhe Partie 1\n",
    "    'https://www.gutenberg.org/files/15075/15075-h/15075-h.htm',  # Sodome et Gomorrhe Partie 2\n",
    "    'https://www.gutenberg.org/files/60720/60720-h/60720-h.htm',  # La Prisonnière \n",
    "    # Les misérables\n",
    "    'https://www.ebooksgratuits.com/html/hugo_les_miserables_fantine.html',\n",
    "    'https://www.ebooksgratuits.com/html/hugo_les_miserables_cosette.html',\n",
    "    'https://www.ebooksgratuits.com/html/hugo_les_miserables_marius.html',\n",
    "    'https://www.ebooksgratuits.com/html/hugo_les_miserables_idylle_plumet_epopee_st_denis.html',\n",
    "    'https://www.ebooksgratuits.com/html/hugo_les_miserables_idylle_plumet_epopee_st_denis.html',\n",
    "    'https://www.ebooksgratuits.com/html/hugo_les_miserables_jean_valjean.html'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère le texte au format HTML à l'aide de la librairie Beautiful Soup. La page HTML est organisée en paragraphe qui suivent le découpage du livre.\n",
    "\n",
    "Attention, le scrapping n'est pas toujours autorisé, il est toujours primordial de s'assurer des licences et disposition légales concernant les données que l'on cherche à récupérer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "page = requests.get(book_links[0]) \n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "paragraphes = soup.select('p', class_='MsoNormal', style='')\n",
    "paragraphes = [p.get_text(strip=True) for p in paragraphes]\n",
    "paragraphes = [' '.join(p.split()) for p in paragraphes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Downloaded {:,d} books for a total of {:,d} paragraphs.\".format(len(book_links), len(paragraphes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paragraphes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Pré-traitements : les expressions régulières"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaque paragraphe est consitué d'un unique bloc de texte. En NLP, les corpus sont généralement séparés en phrases et enregistrés dans un fichier ou on retrouve une phrase par ligne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les expressions régulières sont un outil très puissants qui permettent de rechercher des informations sous une forme standardisée. Dans l'exemple suivant, on peut cherche à extraire les numéros de téléphone. On cherche donc une suite de 10 chiffres avec éventuellement des séparateurs entre les chiffres. On peut également chercher les dates, les adresses, les montants, températures ou tout autre motif. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "    Bonjour mon numéro de téléphone est le 04.56.55.33.66\n",
    "    Super le mien est 0392020302\n",
    "    Génial, je vous donne également mon tel : 03-02-02-12-89 et celui du bureau : +33 (0) 5 33 19 33 09\n",
    "    Est-ce que vous seriez disponible pour un rendez vous le 06/12/20 vers 13h ?\n",
    "    Les tickets coutent 13€95.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les regex permettent de chercher des motifs dans le texte. Ces motifs sont décrits par des expressions standardisées très spécifiques. En python, on peut utiliser les regex à l'aide de la librairie `re`\n",
    "\n",
    "Par exemple, on peut chercher un chiffre dans le texte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_pattern = re.compile('\\d')\n",
    "\n",
    "digits = re.findall(digit_pattern, sample_text)\n",
    "print(digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut également chercher l'ensemble des motifs ou l'on retrouve deux chiffres d'affillées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_pattern = re.compile('\\d{2}')\n",
    "\n",
    "digits = re.findall(digit_pattern, sample_text)\n",
    "print(digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalement on peut chercher l'ensemble des motifs ou on retrouve plusieurs chiffres d'affilé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_pattern = re.compile('\\d+')\n",
    "# + pour capter si le motif apparait entre 1 et une infinité de fois\n",
    "# * pour capter si le motif apparait entre 0 et une infinité de fois\n",
    "\n",
    "digits = re.findall(digit_pattern, sample_text)\n",
    "print(digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut également chercher des groupes plus complexes, par exemple un numéro de téléphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telephone_pattern = re.compile('[\\d\\.\\-\\(\\) +]{5,}')\n",
    "# [] est l'équivalent de \"ou\" pour l'ensemble des motifs de la liste\n",
    "# {5,} est un quantifier plus précise que * ou +. \n",
    "# Ici, on cherche ce motif au moins 5 fois. \n",
    "# {,5} serait au maximum 5 et {5} exactement 5 fois\n",
    "\n",
    "telephone = re.findall(telephone_pattern, sample_text)\n",
    "print(telephone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librairie `re` comprend d'autres méthodes que `re.search`. Par exemple la fonction de substitution `re.sub` qui permet de remplacer le motif par un autre. Un exemple ici poour procéder à une dé-anonymisation des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text_anonymized = re.sub(telephone_pattern, ' XX.XX.XX.XX.XX ', sample_text)\n",
    "print(sample_text_anonymized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On pourra s'appuyer sur l'outil : https://regex101.com/. Un autre excellent site pour s'entrainer aux expresions régulières : https://alf.nu/RegexGolf et une cheat sheet : https://cheatography.com/davechild/cheat-sheets/regular-expressions/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**Exercice 1.** Séparer les paragraphes en phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sentences(text):\n",
    "    #TODO à compléter\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/split_sentences.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookcorpus = []\n",
    "\n",
    "for p in paragraphes:\n",
    "    sentences = split_into_sentences(p)\n",
    "    bookcorpus.extend(sentences)\n",
    "    \n",
    "print(\"Extracted {:,d} sentences\".format(len(bookcorpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save corpus\n",
    "with open('./data/miserables_temps_perdu.txt', 'w') as f:\n",
    "    for s in bookcorpus:\n",
    "        f.write(s + '\\n')\n",
    "        \n",
    "print(\"Saved corpus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./data/miserables.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus\n",
    "with open('./data/miserables_temps_perdu.txt', 'r') as f:\n",
    "    sentences = f.readlines()\n",
    "    \n",
    "sentences = [l.strip() for l in sentences if l]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant séparer le corpus en tokens.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Exercice 2.** Effectuer la tokenization du corpus et créer le dictionnaire de vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(txt):\n",
    "    \n",
    "    #TODO à compléter\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/tokenize.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [tokenize(s) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = sum([len(t) for t in tokenized_corpus])\n",
    "print(\"Corpus contains {:,d} tokens.\".format(n_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus_flatten = [ll for l in tokenized_corpus for ll in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(tokenized_corpus_flatten) == n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_counter = Counter(tokenized_corpus_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_counter(tokens_counter, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_zipf(tokens_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On compare la distribution des mots avec un autre auteur français : le rappeur [Jul](https://fr.wikipedia.org/wiki/Jul_(chanteur)) pour s'assurer que l'on retrouve une forme de distribution similaire. Les fréquences de mots ont été évaluées sur 521 chansons dont les paroles ont été scrappées sur le [AZLyrics](https://www.azlyrics.com/j/jul.html). Les données ont été traitées avec le même script que précédemment. On a directement sauvegardé les fréquences d'apparition pour l'ensemble des mots du vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_counter = Counter()\n",
    "\n",
    "with open('./data/jul_freqs.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        k, v = line.strip().split('\\t')\n",
    "        tokens_counter[k] = int(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_counter(tokens_counter, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_zipf(tokens_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On compare avec la litératture anglaise en utilisant en particulier les oeuvres de Jane Austen, Shakespeare. Ces dernières sont accessibles directement en utilisant la librairie `NLTK`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = FreqDist()\n",
    "n_words = 0\n",
    "for text in gutenberg.fileids():\n",
    "    for word in gutenberg.words(text):\n",
    "        fd[word] += 1\n",
    "        n_words += 1\n",
    "\n",
    "ranks = []\n",
    "freqs = []\n",
    "for rank, word in enumerate(fd):\n",
    "    ranks.append(rank+1)\n",
    "    freqs.append(fd[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_zipf(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_counter(fd, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}