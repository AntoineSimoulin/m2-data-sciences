{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Copyright 2021 Antoine SIMOULIN.**\n",
    "\n",
    "<i>Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "Icons made by <a href=\"https://www.flaticon.com/authors/freepik\" title=\"Freepik\">Freepik</a>, <a href=\"https://www.flaticon.com/authors/pixel-perfect\" title=\"Pixel perfect\">Pixel perfect</a>, <a href=\"https://www.flaticon.com/authors/becris\" title=\"Becris\">Becris</a>, <a href=\"https://www.flaticon.com/authors/smashicons\" title=\"Smashicons\">Smashicons</a>, <a href=\"https://www.flaticon.com/authors/srip\" title=\"srip\">srip</a>, <a href=\"https://www.flaticon.com/authors/adib-sulthon\" title=\"Adib\">Adib</a>, <a href=\"https://www.flaticon.com/authors/flat-icons\" title=\"Flat Icons\">Flat Icons</a> and <a href=\"https://www.flaticon.com/authors/dinosoftlabs\" title=\"Pixel perfect\">DinosoftLabs</a> from <a href=\"https://www.flaticon.com/\" title=\"Flaticon\"> www.flaticon.com</a></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2 - Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/figure2.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le <i>Topic Modeling</i> est une approche statistique qui permet de faire √©merger des topics abstraits d'un corpus de documents. \n",
    "Cette approche permet √©galement d'analyser la structure du corpus de documents en regroupant ceux qui pr√©sentent des topics similaires puis en analysant ces groupes, ou en analysant les caract√©ristiques des topics identifi√©s.\n",
    "\n",
    "La plupart des mod√®les de <i>Topic Modeling</i> s'appuient sur des hypoth√®ses de mod√©lisations similaires:\n",
    "* Chaque document est mod√©lis√© comme une distribution sur les _topics_ ;\n",
    "* Chaque _topic_ est mod√©lis√© comme une distribution sur les mots du vocabulaire.\n",
    "\n",
    "On a illustr√© cette mod√©lisation ci-dessous. Ainsi chaque document est repr√©sent√© par une distribution sur une variable latente (on dit aussi cach√©e), les _topics_. Ces derniers ne sont pas \"observ√©s\" : en pratique chaque document est d√©crit par une distribution sur les mots du vocabulaire. **L'objectif des mod√®les de _topics_ est donc de caract√©riser la forme de cette variable latente.** Nous allons voir plusieurs m√©thodes et mod√®les proposant cette caract√©risation.\n",
    "\n",
    "Ci-dessous, on a illustr√© l'intuition derri√®re cette mod√©lisation. Chaque document va contenir plusieurs _topics_, par exemple, les transports et les vacances. On retrouvera donc des mots caract√©ristiques de ces topic: \"avion\", \"plage\", \"cong√©s\" ... Des documents qui abordent des _topics_ proches contiendront donc un vocabulaire proche. Ainsi chaque _topic_ pourra √™tre caract√©ris√© par des mots saillants qui lui sont sp√©cifiques.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/lda-idee.png?raw=true\" width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# ‚ö†Ô∏è Execute only if running in Colab\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  IN_COLAB = True\n",
    "else:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "  !pip install -q scikit-learn==0.23.2 nltk==3.5 unidecode pysrt\n",
    "  !pip install --no-deps pyLDAvis==3.3.1\n",
    "  !pip install --no-deps funcy==1.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "import numpy as np\n",
    "import os\n",
    "from pyLDAvis import sklearn as sklearn_lda\n",
    "import pickle\n",
    "import pyLDAvis\n",
    "import pysrt\n",
    "import re\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "from tqdm.auto import tqdm\n",
    "import unidecode\n",
    "import urllib.request\n",
    "\n",
    "# IPython automatically reload all changed code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import extrenal modules\n",
    "\n",
    "repo_url = 'https://raw.githubusercontent.com/AntoineSimoulin/m2-data-sciences/master/'\n",
    "for season in range(1, 9):\n",
    "  dir = './data/S{:02d}'.format(season)\n",
    "  if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "    for episode in range(1, 11):\n",
    "      try:\n",
    "        _ = urllib.request.urlretrieve(\n",
    "            repo_url + 'TP2%20-%20Text%20Mining/sous-titres-got/S{:02d}/E{:02d}.srt'.format(season, episode), \n",
    "            './data/S{:02d}/E{:02d}.srt'.format(season, episode))\n",
    "      except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le mod√®le Latent Semantic Analysis (LSA) ([Landauer & Dumais, 1997](#landauer-dumais-1997)) cherche √† d√©composer la matrice de d√©composition des documents selon le vocabulaire en deux matrices : une matrice de d√©composition des documents selon les topics et une matrice de distribution des topics selon les mots du vocabulaires.\n",
    "\n",
    "On commencer donc par repr√©senter les documents selon une distribution sur le vocabulaire. Pour cela on utilise le Tf-Idf qui permet de repr√©senter chaque document du corpus comme une distribution sur le vocabulaire, en pratique, un vecteur de la taille du vocabulaire. On peut donc repr√©senter le corpus comme une matrice de taille $(M, V)$ avec $M$ le nombre de documents dans le corpus et $V$ la taille du vocabulaire. Cette repr√©sentation est illustr√©e ci-dessous. \n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/bow.png?raw=true\" width=\"500\">\n",
    "\n",
    "On va ensuite d√©composer la matrice en utilisant **d√©composition en valeurs singuli√®res** (en anglais Singular Value Decomposition, [SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition)). On peut interpr√©ter la SVD comme la g√©n√©ralisation de la diagonalisation d'une matrice normale a des matrices arbitraires. Ainsi une matrice $A$ de taille $n \\times m$ peut √™tre factoris√©e sous la forme $A = U \\Sigma V^T$, avec $U$ et $V$ des matrices orthogonales de tailles respectives $m \\times m$ et $n \\times n$ et $\\Sigma$ une martice rectangulaire diagonale de taille $m \\times n$. \n",
    "\n",
    "En pratique, il est peu commun de d√©composer compl√©tement la matrice, on utilise plut√¥t la <a href=\"https://en.wikipedia.org/wiki/Singular_value_decomposition#Truncated_SVD\"><i>Trucated Singular Value Decomposition</i></a> qui permet de ne calculer que les $t$ premi√®res valeurs singuli√®res. Dans ce cas, on ne consid√®re que les $t$ premi√®res colonnes de la matrice $U$ et les $t$ premi√®res lignes de la matrice $V$. On a ainsi :\n",
    "\n",
    "$$A_t = U_t \\Sigma_t V_t^T$$\n",
    "\n",
    "Avec $U_t$ de taille $m \\times t$ et $V_t$ de taille $n \\times t$. Cette d√©composition est illustr√©e ci-dessous.\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/svd-formule.png?raw=true\" width=\"500\">\n",
    "\n",
    "On illustre ci-dessous l'application de la d√©composition √† notre matrice Tf-Idf. La matrice $U_t$ apparait comme la matrice <i>document-topic</i> qui d√©finit chaque document comme une distribution de topic. La matrice $V_t$ apparait comme la matrice <i>terme-topic</i> qui d√©finit chaque topic comme une distribution sur le vocabulaire.\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/svd-illustration.png?raw=true\" width=\"500\">\n",
    "\n",
    "On peut aussi interpr√©ter le <i>Topic Modeling</i> comme une approche de r√©duction de dimension. En effet, la matrice Tf-Idf a plusieurs d√©fauts : Elle est de grande dimension (la taille du vocabulaire), elle est _sparse_ _i.e._ beaucoup d'entr√©es sont √† z√©ro, elle est tr√®s bruit√©e et les information sont redondantes selon plusieurs dimensions. La d√©composition permet ainsi de la factoriser. Les deux matrices r√©sultantes permetent d'utiliser la similarit√© cosinus pour comparer simplement des doccuments ou des mots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Latent Semantic Analysis (pLSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La LSA est une m√©thode tr√®s efficace. N√©anmoins en pratique, les topics r√©sultants sont parfois difficiles √† interpr√©ter. \n",
    "La m√©thode n√©cessite un corpus important pour obtenir des r√©sultats pertinents.\n",
    "\n",
    "La methode de Probabilistic Latent Semantic Analysis (pLSA) remplace ainsi la SVD par une approche probabiliste.\n",
    "Il s'agit d'un mod√®le **g√©n√©ratif**, qui permet de g√©n√©rer les documents que l'on observe. \n",
    "En pratique il permet de g√©n√©rer la matrice Bag-of-words qui repr√©sente le corpus. Le mod√®le ne tient donc pas compte de l'ordre des mots.\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/plda_principe.png?raw=true\" width=\"500\">\n",
    "\n",
    "\n",
    "Les mod√®les graphiques repr√©sentent les variables al√©atoies comme des noeuds. Les arcs entre les noeuds indiquent les variables potentiellement d√©pendantes. Les variables observ√©es sont gris√©es. Dans la figure ci-dessous, les noeuds $ X_{1,...,N}$ sont observ√©s alors que le noeud $Y$ est une variable latente. Dans cet exemple, les variables observ√©es d√©pendent de cette variable latente. Les rectangles synth√©tisent la r√©plication de plusieurs structures. Un rectangle r√©sume donc plusieurs variables $X_n$ avec $n \\in N$.\n",
    "\n",
    "La structure du graph d√©finie les d√©pendances conditionnelles entre l'ensemble des variables. Par exemple dans le graph ci-dessous, on a $p(Y,X_{1},...,X_{N})=p(Y)\\prod _{n=1}^{N}p(X_{n}|Y)$.\n",
    "\n",
    "<img src=\"./figures/graphical_model.png\" width=\"500\">\n",
    "\n",
    "Le fonctionnement du mod√®le est d√©taill√© selon la repr√©sentation graphique suivante :\n",
    "* Etant donn√© un document $d$, un topic $z$ est pr√©sent dans le document avec une probabilit√© $P(z|d)$. \n",
    "* Etant donn√© un topic $z$, un mot est g√©n√©r√© selon la probabilit√© conditionnelle $P(w|z)$.\n",
    "\n",
    "La probabilit√© jointe d'observer un mot dans un document s'exprime donc :\n",
    "\n",
    "$$P(D,W)=P(D)\\sum_ZP(Z|D)P(W|Z)$$\n",
    "\n",
    "Ici, $P(D)$, $P(Z|D)$ et $P(W|Z)$ sont des param√®tres du mod√®le. $P(D)$ peut √™tre  calcul√© directement √† partir du corpus.\n",
    "$P(Z|D)$ et $P(W|Z)$ sont mod√©lis√©s par des distributions multinomiales qui peuvent √™tre entrain√©s par la m√©thode [EM](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm).\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/plsa.png?raw=true\" width=\"500\">\n",
    "\n",
    "On peut intepr√©ter la probabilit√© selon la proc√©dure suivante : On commence par un document avec la probabilit√© $P(D)$, on g√©n√®re un _topic_ avec la probabilit√© $P(Z|D)$ puis on g√©n√®re un mot avec la probabilit√© $P(W|Z)$. En pratique, on apprend donc les param√®tres de notre mod√®les qui permetent d'expliquer qu mieux le corpus observ√© comme illustr√© ci-dessous.\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/plda_inference.png?raw=true\" width=\"500\">\n",
    "\n",
    "On peut aussi exprimer la probabilit√© jointe selon la d√©composition suivante :\n",
    "\n",
    "$$P(D,W)=\\sum_ZP(Z)P(D|Z)P(W|Z)$$\n",
    "\n",
    "Dans cette mod√©lisation, on commence par le _topic_ avec $P(Z)$ et on g√©n√®re ensuite ind√©pendamment le document avec $P(D|Z)$ et le mot avec $P(W|Z)$.\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/plda_process.png?raw=true\" width=\"500\">\n",
    "\n",
    "L'int√©r√™t de cette param√©trisation, c'est qu'elle fait appraitre un parall√®le avec la LSA.\n",
    "\n",
    "La probabilit√© du topic $P(Z)$ correspond √† la matrice diagonale  de la d√©composition en valeurs singuli√®ers. La probabilit√© d'un document en donction du topic $P(D|Z)$ correpond √† la matrice document-_topic_  $U$ et la probabilit√© d'un mot en fonction du _topic_ $P(W|Z)$ correpond √† la matricec terme-_topic_ $V$. Les deux approches pr√©sentent donc des similarit√©s, la pLSA apporte un traitement statistique des _topics_ et des mots par rapport √† la LSA.\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/plsa-formule.png?raw=true\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "La pLSA a un certain nombre dee contraintes :\n",
    "* Il n'y a pas de param√®tres pour mod√©liser $P(D)$, on ne peut donc pas assigner de probabilit√© √† de nouveaux documents\n",
    "* Le nombre de param√®tres grandit lin√©airement avec le nombre de documents dans le corpus, le mod√®le est donc sujet √† l'_overfitting_.\n",
    "\n",
    "En pratique, la pLSA  n'est donc pas souvent utilis√©e, on lui pr√©f√®re g√©n√©ralement la Latent Dirichlet Allocation (LDA) ([Blei et al., 2001](#blei-2001)). La LDA utilise des prior de dirichlet pour les distributions des documents selon les _topics_ et _topics_ selon les mots, ce qui lui donne de meilleures propri√©t√©s de g√©n√©ralisation: on peut g√©n√©raliser pour de nouveaux documents.\n",
    "\n",
    "\n",
    "### La distribution de Dirichlet\n",
    "\n",
    "La [distribution de Dirichlet](https://en.wikipedia.org/wiki/Dirichlet_distribution) est g√©n√©ralement not√©e $Dir(\\alpha)$. Il s'agit d'une famille de lois de probabilit√©s continues pour des variables al√©atoires multinomiales. Cette loi (ou encore distribution) est param√©tr√©e par le vecteur ${\\bf \\alpha}$ de nombres r√©els positifs. La taille du vecteur ${\\bf \\alpha}$ indique la dimension de la distribution. Ce type de distribution est souvent utilis√©e comme distribution √† priori dans les mod√®les Bay√©siens. Sans rentrer dans les d√©tails, voici quelques caraact√©ristiques de la distribution de Dirichlet :\n",
    "\n",
    "* La distribution est d√©finie sur un simplex de vecteurs positifs dont la somme est √©gale √† 1 \n",
    "* Sa densit√© est caract√©ris√©e par : $P(\\theta |{\\overrightarrow {\\alpha }})={\\frac {\\Gamma (\\Sigma _{i}\\alpha _{i})f}{\\Pi _{i}\\Gamma (\\alpha _{i})}}\\Pi _{i}\\theta _{i}^{\\alpha _{i}-1}$\n",
    "* En pratique, si toutes les dimensions de ${\\bf \\alpha}$ ont des valeurs similaires, la distribution est plus √©tal√©e. Elle devient plus concentr√©e pour des valeurs plus importantes de ${\\bf \\alpha}$.\n",
    "\n",
    "La distribution est illustr√©e ci-dessous pour des valeurs ${\\bf \\alpha}$ qui varient entre (6, 2, 2), (3, 7, 5), (6, 2, 6), (2, 3, 4).\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/lda-simplex.png?raw=true\" width=\"500\">\n",
    "Source: https://towardsdatascience.com/dirichlet-distribution-a82ab942a879\n",
    "\n",
    "Cette distribution a des avantages pratiques. En particulier, on s'attend √† ce que les documents du corpus contiennent un _topic_ \"majoritaire\". Ils ne sont pas g√©n√©r√©s selon une distribution 25% vacances, 25% sport, 25% √©lections, 25% transports mais plut√¥t des distributions du type 85% vacances, 5% sport, 5% √©lections, 5% transports. Ces distributions vont donc attribu√© un poids important √† un certain _topic_. C'est justement ce que permet de r√©aliser la distribution de Dirichlet avec de faibles valeurs de $\\alpha$.\n",
    "\n",
    "La repr√©sentation garphique du mod√®le est propos√©e ci-dessous. La LDA suppose le proc√©ssus g√©n√©ratif suivant pour chaque document $W$ dans le corpus $D$.\n",
    "\n",
    "\n",
    "> 1. On choisit $\\theta \\sim Dir(\\alpha )$\n",
    "> 2. Pour chaque document dans le corpus:\n",
    ">     * Pour chacun des $N$ mots $w_{n}$ dans le document :\n",
    ">        * on g√©n√®re un topic $z_{n}\\sim Multinomial(\\theta )$\n",
    ">        * on g√©n√®re un mot $w_{n}$ $p(w_{n}|z_{n},B)$ selon une loi multinimoale conditionn√©e par le topic $z_{n}$.\n",
    "\n",
    "\n",
    "Our goal here is to estimate parameters œÜ, Œ∏ to maximize p(w; Œ±, Œ≤). The main advantage of LDA over pLSA is that it generalizes well for unseen documents.\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/lda_graph.png?raw=true\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utilisation des librairies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va chercher √† analyser les th√®mes de la S√©rie Game Of Thrones. On utilise pour √ßa les sous-titres de l'ensemble des saisons qui ont √©t√© r√©cup√©r√©s sur le site https://www.sous-titres.eu/series/game_of_thrones.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subtitle_file_dict(subtitles_dir):\n",
    "    \"Retourne les chemins vers les fichiers de sous titres\"\n",
    "    subtitles_file_path = {}\n",
    "    for path, subdirs, files in os.walk(subtitles_dir):\n",
    "        for name in files:\n",
    "            episode_name = '_'.join([os.path.basename(path), name.split('.')[0]])\n",
    "            subtitles_file_path[episode_name] = os.path.join(path, name)\n",
    "    return subtitles_file_path\n",
    "\n",
    "def parse_srt_file(srt_file, encoding='iso-8859-1'):\n",
    "    \"Lit un ficher de sous titres au format rst\"\n",
    "    subs = pysrt.open(srt_file, encoding=encoding)\n",
    "    text = ' '.join([' '.join(sub.text.split('\\n')) for sub in subs])\n",
    "    return text\n",
    "\n",
    "def create_corpus(subtitles_file_path):\n",
    "    \"Cr√©er un corpus √† partir de tous les fichiers rst dans un dossier\"\n",
    "    corpus = []\n",
    "    for k, v in subtitles_file_path.items():\n",
    "        if v.endswith('srt'):\n",
    "            corpus.append(parse_srt_file(v))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitles_file_path = create_subtitle_file_dict('./data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_1_txt = parse_srt_file(subtitles_file_path['S01_E01'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(episode_1_txt[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = create_corpus(subtitles_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>üìù Exercice :</b> Nettoyer le corpus pour enlever les accents, mettre le texte en minuscule, enlever la ponctuation et les doubles espaces. Eventuellement pour le stemming.</p>\n",
    "</div>\n",
    "<hr> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/cleaning.py\n",
    "stemmer = FrenchStemmer()\n",
    "\n",
    "def clean_corpus(corpus):\n",
    "    # TODO √©crire fonction qui permet de nettoyer les documents du corpuss\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corpus = clean_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corpus[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corpus_split = []\n",
    "for episode in clean_corpus:\n",
    "    episode_words = episode.split()\n",
    "    i = 0\n",
    "    while i < len(episode_words):\n",
    "        clean_corpus_split.append(' '.join(episode_words[i:i+400]))\n",
    "        i+=400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_corpus_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    tokens = []\n",
    "    for sentence in corpus.split('\\n'):\n",
    "        tokens.append(nltk.word_tokenize(sentence))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_length = [len(x.split()) for x in clean_corpus_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(sentence_length), np.std(sentence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>üìù Exercice :</b> Vectorizer le corpus en utilisant la m√©thode Bag-Of-Words.</p>\n",
    "</div>\n",
    "<hr> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/vectorize.py\n",
    "\n",
    "count_data = # TODO √©crire fonction de vectorization du corpus pour avoir la repr√©sentation BoW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_corpus_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faire varier les param√®tres ci-dessous\n",
    "number_topics = 15\n",
    "number_words = 10\n",
    "\n",
    "# Create and fit the LDA model\n",
    "lda = LDA(n_components=number_topics, n_jobs=-1)\n",
    "lda.fit(count_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, count_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "LDAvis_data_filepath = os.path.join('./ldavis_prepared_'+str(number_topics))\n",
    "LDAvis_prepared = sklearn_lda.prepare(lda, count_data, count_vectorizer, mds='mmds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "    \n",
    "pyLDAvis.save_html(LDAvis_prepared, './ldavis_prepared_'+ str(number_topics) +'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>üìù Exercice :</b> Faire varier le param√®tre Lambda et justifier de son impact.</p>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>üìù Exercice :</b> Faire varier le pr√©processing,en particulier la stemmatization. Analyser l'impact sur l'analyse des clusters.</p>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>üìù Exercice :</b> Etudier l'impact des Stop Words sur les topics.</p>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "> <div id=\"landauer-dumais-1997\">Landauer, Thomas K. et al. <a href=http://lsa.colorado.edu/papers/dp1.LSAintro.pdf>An introduction to latent semantic analysis.</a> Discourse Processes 25 (1998): 259-284.</div>\n",
    "\n",
    "> <div id=\"blei-2001\"> David M. Blei, Andrew Y. Ng, Michael I. Jordan: <a href=https://ai.stanford.edu/~ang/papers/nips01-lda>Latent Dirichlet Allocation.</a> NIPS 2001: 601-608</div>\n",
    " \n",
    "> <div id=\"alghamdi-2001\"> Rubayyi Alghamdi and Khalid Alfalqi: <a href=http://dx.doi.org/10.14569/IJACSA.2015.060121>A Survey of Topic Modeling in Text Mining.</a> International Journal of Advanced Computer Science and Applications(IJACSA), 6(1), 2015</div>\n",
    "\n",
    "> <div id=\"sievert-2014\"> Sievert, Carson, and Kenneth Shirley. <a href=\"https://aclanthology.org/W14-3110.pdf\">LDAvis: A method for visualizing and interpreting topics.</a> Proceedings of the workshop on interactive language learning, visualization, and interfaces. 2014.\n",
    "\n",
    "> <div id=\"chuang-2012\"> Chuang, Jason, Christopher D. Manning, and Jeffrey Heer. <a href=\"https://dl.acm.org/doi/10.1145/2254556.2254572\">Termite: Visualization techniques for assessing textual topic models.</a> Proceedings of the international working conference on advanced visual interfaces. 2012."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
