{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Copyright 2021 Antoine SIMOULIN.**\n",
    "\n",
    "<i>Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "Icons made by <a href=\"https://www.flaticon.com/authors/freepik\" title=\"Freepik\">Freepik</a>, <a href=\"https://www.flaticon.com/authors/pixel-perfect\" title=\"Pixel perfect\">Pixel perfect</a>, <a href=\"https://www.flaticon.com/authors/becris\" title=\"Becris\">Becris</a>, <a href=\"https://www.flaticon.com/authors/smashicons\" title=\"Smashicons\">Smashicons</a>, <a href=\"https://www.flaticon.com/authors/srip\" title=\"srip\">srip</a>, <a href=\"https://www.flaticon.com/authors/adib-sulthon\" title=\"Adib\">Adib</a>, <a href=\"https://www.flaticon.com/authors/flat-icons\" title=\"Flat Icons\">Flat Icons</a> and <a href=\"https://www.flaticon.com/authors/dinosoftlabs\" title=\"Pixel perfect\">DinosoftLabs</a> from <a href=\"https://www.flaticon.com/\" title=\"Flaticon\"> www.flaticon.com</a></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2 - Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/figure2.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le <i>Topic Modeling</i> est une approche statistique qui permet de faire émerger des topics abstraits d'un corpus de documents. \n",
    "Cette approche permet également d'analyser la structure du corpus de documents en regroupant ceux qui présentent des topics similaires puis en analysant ces groupes, ou en analysant les caractéristiques des topics identifiés.\n",
    "\n",
    "La plupart des modèles de <i>Topic Modeling</i> s'appuient sur des hypothèses de modélisations similaires:\n",
    "* Chaque document est modélisé comme une distribution sur les _topics_ ;\n",
    "* Chaque _topic_ est modélisé comme une distribution sur les mots du vocabulaire.\n",
    "\n",
    "On a illustré cette modélisation ci-dessous. Ainsi chaque document est représenté par une distribution sur une variable latente (on dit aussi cachée), les _topics_. Ces derniers ne sont pas \"observés\" : en pratique chaque document est décrit par une distribution sur les mots du vocabulaire. **L'objectif des modèles de _topics_ est donc de caractériser la forme de cette variable latente.** Nous allons voir plusieurs méthodes et modèles proposant cette caractérisation.\n",
    "\n",
    "Ci-dessous, on a illustré l'intuition derrière cette modélisation. Chaque document va contenir plusieurs _topics_, par exemple, les transports et les vacances. On retrouvera donc des mots caractéristiques de ces topic: \"avion\", \"plage\", \"congés\" ... Des documents qui abordent des _topics_ proches contiendront donc un vocabulaire proche. Ainsi chaque _topic_ pourra être caractérisé par des mots saillants qui lui sont spécifiques.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/lda-idee.png?raw=true\" width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# ⚠️ Execute only if running in Colab\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  IN_COLAB = True\n",
    "else:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "  !pip install -q scikit-learn==0.23.2 nltk==3.5 unidecode pysrt\n",
    "  !pip install --no-deps pyLDAvis==3.3.1\n",
    "  !pip install --no-deps funcy==1.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "import numpy as np\n",
    "import os\n",
    "from pyLDAvis import sklearn as sklearn_lda\n",
    "import pickle\n",
    "import pyLDAvis\n",
    "import pysrt\n",
    "import re\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "from tqdm.auto import tqdm\n",
    "import unidecode\n",
    "import urllib.request\n",
    "\n",
    "# IPython automatically reload all changed code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import extrenal modules\n",
    "\n",
    "repo_url = 'https://raw.githubusercontent.com/AntoineSimoulin/m2-data-sciences/master/'\n",
    "for season in range(1, 9):\n",
    "  dir = './data/S{:02d}'.format(season)\n",
    "  if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "    for episode in range(1, 11):\n",
    "      try:\n",
    "        _ = urllib.request.urlretrieve(\n",
    "            repo_url + 'TP2%20-%20Text%20Mining/sous-titres-got/S{:02d}/E{:02d}.srt'.format(season, episode), \n",
    "            './data/S{:02d}/E{:02d}.srt'.format(season, episode))\n",
    "      except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle Latent Semantic Analysis (LSA) ([Landauer & Dumais, 1997](#landauer-dumais-1997)) cherche à décomposer la matrice de décomposition des documents selon le vocabulaire en deux matrices : une matrice de décomposition des documents selon les topics et une matrice de distribution des topics selon les mots du vocabulaires.\n",
    "\n",
    "On commencer donc par représenter les documents selon une distribution sur le vocabulaire. Pour cela on utilise le Tf-Idf qui permet de représenter chaque document du corpus comme une distribution sur le vocabulaire, en pratique, un vecteur de la taille du vocabulaire. On peut donc représenter le corpus comme une matrice de taille $(M, V)$ avec $M$ le nombre de documents dans le corpus et $V$ la taille du vocabulaire. Cette représentation est illustrée ci-dessous. \n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/bow.png?raw=true\" width=\"500\">\n",
    "\n",
    "On va ensuite décomposer la matrice en utilisant **décomposition en valeurs singulières** (en anglais Singular Value Decomposition, [SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition)). On peut interpréter la SVD comme la généralisation de la diagonalisation d'une matrice normale a des matrices arbitraires. Ainsi une matrice $A$ de taille $n \\times m$ peut être factorisée sous la forme $A = U \\Sigma V^T$, avec $U$ et $V$ des matrices orthogonales de tailles respectives $m \\times m$ et $n \\times n$ et $\\Sigma$ une martice rectangulaire diagonale de taille $m \\times n$. \n",
    "\n",
    "En pratique, il est peu commun de décomposer complétement la matrice, on utilise plutôt la <a href=\"https://en.wikipedia.org/wiki/Singular_value_decomposition#Truncated_SVD\"><i>Trucated Singular Value Decomposition</i></a> qui permet de ne calculer que les $t$ premières valeurs singulières. Dans ce cas, on ne considère que les $t$ premières colonnes de la matrice $U$ et les $t$ premières lignes de la matrice $V$. On a ainsi :\n",
    "\n",
    "$$A_t = U_t \\Sigma_t V_t^T$$\n",
    "\n",
    "Avec $U_t$ de taille $m \\times t$ et $V_t$ de taille $n \\times t$. Cette décomposition est illustrée ci-dessous.\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/svd-formule.png?raw=true\" width=\"500\">\n",
    "\n",
    "On illustre ci-dessous l'application de la décomposition à notre matrice Tf-Idf. La matrice $U_t$ apparait comme la matrice <i>document-topic</i> qui définit chaque document comme une distribution de topic. La matrice $V_t$ apparait comme la matrice <i>terme-topic</i> qui définit chaque topic comme une distribution sur le vocabulaire.\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/svd-illustration.png?raw=true\" width=\"500\">\n",
    "\n",
    "On peut aussi interpréter le <i>Topic Modeling</i> comme une approche de réduction de dimension. En effet, la matrice Tf-Idf a plusieurs défauts : Elle est de grande dimension (la taille du vocabulaire), elle est _sparse_ _i.e._ beaucoup d'entrées sont à zéro, elle est très bruitée et les information sont redondantes selon plusieurs dimensions. La décomposition permet ainsi de la factoriser. Les deux matrices résultantes permetent d'utiliser la similarité cosinus pour comparer simplement des doccuments ou des mots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Latent Semantic Analysis (pLSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La LSA est une méthode très efficace. Néanmoins en pratique, les topics résultants sont parfois difficiles à interpréter. \n",
    "La méthode nécessite un corpus important pour obtenir des résultats pertinents.\n",
    "\n",
    "La methode de Probabilistic Latent Semantic Analysis (pLSA) remplace ainsi la SVD par une approche probabiliste.\n",
    "Il s'agit d'un modèle **génératif**, qui permet de générer les documents que l'on observe. \n",
    "En pratique il permet de générer la matrice Bag-of-words qui représente le corpus. Le modèle ne tient donc pas compte de l'ordre des mots.\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/plda_principe.png?raw=true\" width=\"500\">\n",
    "\n",
    "\n",
    "Les modèles graphiques représentent les variables aléatoies comme des noeuds. Les arcs entre les noeuds indiquent les variables potentiellement dépendantes. Les variables observées sont grisées. Dans la figure ci-dessous, les noeuds $ X_{1,...,N}$ sont observés alors que le noeud $Y$ est une variable latente. Dans cet exemple, les variables observées dépendent de cette variable latente. Les rectangles synthétisent la réplication de plusieurs structures. Un rectangle résume donc plusieurs variables $X_n$ avec $n \\in N$.\n",
    "\n",
    "La structure du graph définie les dépendances conditionnelles entre l'ensemble des variables. Par exemple dans le graph ci-dessous, on a $p(Y,X_{1},...,X_{N})=p(Y)\\prod _{n=1}^{N}p(X_{n}|Y)$.\n",
    "\n",
    "<img src=\"./figures/graphical_model.png\" width=\"500\">\n",
    "\n",
    "Le fonctionnement du modèle est détaillé selon la représentation graphique suivante :\n",
    "* Etant donné un document $d$, un topic $z$ est présent dans le document avec une probabilité $P(z|d)$. \n",
    "* Etant donné un topic $z$, un mot est généré selon la probabilité conditionnelle $P(w|z)$.\n",
    "\n",
    "La probabilité jointe d'observer un mot dans un document s'exprime donc :\n",
    "\n",
    "$$P(D,W)=P(D)\\sum_ZP(Z|D)P(W|Z)$$\n",
    "\n",
    "Ici, $P(D)$, $P(Z|D)$ et $P(W|Z)$ sont des paramètres du modèle. $P(D)$ peut être  calculé directement à partir du corpus.\n",
    "$P(Z|D)$ et $P(W|Z)$ sont modélisés par des distributions multinomiales qui peuvent être entrainés par la méthode [EM](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm).\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/plsa.png?raw=true\" width=\"500\">\n",
    "\n",
    "On peut intepréter la probabilité selon la procédure suivante : On commence par un document avec la probabilité $P(D)$, on génère un _topic_ avec la probabilité $P(Z|D)$ puis on génère un mot avec la probabilité $P(W|Z)$. En pratique, on apprend donc les paramètres de notre modèles qui permetent d'expliquer qu mieux le corpus observé comme illustré ci-dessous.\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/plda_inference.png?raw=true\" width=\"500\">\n",
    "\n",
    "On peut aussi exprimer la probabilité jointe selon la décomposition suivante :\n",
    "\n",
    "$$P(D,W)=\\sum_ZP(Z)P(D|Z)P(W|Z)$$\n",
    "\n",
    "Dans cette modélisation, on commence par le _topic_ avec $P(Z)$ et on génère ensuite indépendamment le document avec $P(D|Z)$ et le mot avec $P(W|Z)$.\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/plda_process.png?raw=true\" width=\"500\">\n",
    "\n",
    "L'intérêt de cette paramétrisation, c'est qu'elle fait appraitre un parallèle avec la LSA.\n",
    "\n",
    "La probabilité du topic $P(Z)$ correspond à la matrice diagonale  de la décomposition en valeurs singulièers. La probabilité d'un document en donction du topic $P(D|Z)$ correpond à la matrice document-_topic_  $U$ et la probabilité d'un mot en fonction du _topic_ $P(W|Z)$ correpond à la matricec terme-_topic_ $V$. Les deux approches présentent donc des similarités, la pLSA apporte un traitement statistique des _topics_ et des mots par rapport à la LSA.\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/plsa-formule.png?raw=true\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "La pLSA a un certain nombre dee contraintes :\n",
    "* Il n'y a pas de paramètres pour modéliser $P(D)$, on ne peut donc pas assigner de probabilité à de nouveaux documents\n",
    "* Le nombre de paramètres grandit linéairement avec le nombre de documents dans le corpus, le modèle est donc sujet à l'_overfitting_.\n",
    "\n",
    "En pratique, la pLSA  n'est donc pas souvent utilisée, on lui préfère généralement la Latent Dirichlet Allocation (LDA) ([Blei et al., 2001](#blei-2001)). La LDA utilise des prior de dirichlet pour les distributions des documents selon les _topics_ et _topics_ selon les mots, ce qui lui donne de meilleures propriétés de généralisation: on peut généraliser pour de nouveaux documents.\n",
    "\n",
    "\n",
    "### La distribution de Dirichlet\n",
    "\n",
    "La [distribution de Dirichlet](https://en.wikipedia.org/wiki/Dirichlet_distribution) est généralement notée $Dir(\\alpha)$. Il s'agit d'une famille de lois de probabilités continues pour des variables aléatoires multinomiales. Cette loi (ou encore distribution) est paramétrée par le vecteur ${\\bf \\alpha}$ de nombres réels positifs. La taille du vecteur ${\\bf \\alpha}$ indique la dimension de la distribution. Ce type de distribution est souvent utilisée comme distribution à priori dans les modèles Bayésiens. Sans rentrer dans les détails, voici quelques caraactéristiques de la distribution de Dirichlet :\n",
    "\n",
    "* La distribution est définie sur un simplex de vecteurs positifs dont la somme est égale à 1 \n",
    "* Sa densité est caractérisée par : $P(\\theta |{\\overrightarrow {\\alpha }})={\\frac {\\Gamma (\\Sigma _{i}\\alpha _{i})f}{\\Pi _{i}\\Gamma (\\alpha _{i})}}\\Pi _{i}\\theta _{i}^{\\alpha _{i}-1}$\n",
    "* En pratique, si toutes les dimensions de ${\\bf \\alpha}$ ont des valeurs similaires, la distribution est plus étalée. Elle devient plus concentrée pour des valeurs plus importantes de ${\\bf \\alpha}$.\n",
    "\n",
    "La distribution est illustrée ci-dessous pour des valeurs ${\\bf \\alpha}$ qui varient entre (6, 2, 2), (3, 7, 5), (6, 2, 6), (2, 3, 4).\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/lda-simplex.png?raw=true\" width=\"500\">\n",
    "Source: https://towardsdatascience.com/dirichlet-distribution-a82ab942a879\n",
    "\n",
    "Cette distribution a des avantages pratiques. En particulier, on s'attend à ce que les documents du corpus contiennent un _topic_ \"majoritaire\". Ils ne sont pas générés selon une distribution 25% vacances, 25% sport, 25% élections, 25% transports mais plutôt des distributions du type 85% vacances, 5% sport, 5% élections, 5% transports. Ces distributions vont donc attribué un poids important à un certain _topic_. C'est justement ce que permet de réaliser la distribution de Dirichlet avec de faibles valeurs de $\\alpha$.\n",
    "\n",
    "La représentation garphique du modèle est proposée ci-dessous. La LDA suppose le procéssus génératif suivant pour chaque document $W$ dans le corpus $D$.\n",
    "\n",
    "\n",
    "> 1. On choisit $\\theta \\sim Dir(\\alpha )$\n",
    "> 2. Pour chaque document dans le corpus:\n",
    ">     * Pour chacun des $N$ mots $w_{n}$ dans le document :\n",
    ">        * on génère un topic $z_{n}\\sim Multinomial(\\theta )$\n",
    ">        * on génère un mot $w_{n}$ $p(w_{n}|z_{n},B)$ selon une loi multinimoale conditionnée par le topic $z_{n}$.\n",
    "\n",
    "\n",
    "Our goal here is to estimate parameters φ, θ to maximize p(w; α, β). The main advantage of LDA over pLSA is that it generalizes well for unseen documents.\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/lda_graph.png?raw=true\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utilisation des librairies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va chercher à analyser les thèmes de la Série Game Of Thrones. On utilise pour ça les sous-titres de l'ensemble des saisons qui ont été récupérés sur le site https://www.sous-titres.eu/series/game_of_thrones.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subtitle_file_dict(subtitles_dir):\n",
    "    \"Retourne les chemins vers les fichiers de sous titres\"\n",
    "    subtitles_file_path = {}\n",
    "    for path, subdirs, files in os.walk(subtitles_dir):\n",
    "        for name in files:\n",
    "            episode_name = '_'.join([os.path.basename(path), name.split('.')[0]])\n",
    "            subtitles_file_path[episode_name] = os.path.join(path, name)\n",
    "    return subtitles_file_path\n",
    "\n",
    "def parse_srt_file(srt_file, encoding='iso-8859-1'):\n",
    "    \"Lit un ficher de sous titres au format rst\"\n",
    "    subs = pysrt.open(srt_file, encoding=encoding)\n",
    "    text = ' '.join([' '.join(sub.text.split('\\n')) for sub in subs])\n",
    "    return text\n",
    "\n",
    "def create_corpus(subtitles_file_path):\n",
    "    \"Créer un corpus à partir de tous les fichiers rst dans un dossier\"\n",
    "    corpus = []\n",
    "    for k, v in subtitles_file_path.items():\n",
    "        if v.endswith('srt'):\n",
    "            corpus.append(parse_srt_file(v))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitles_file_path = create_subtitle_file_dict('./data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_1_txt = parse_srt_file(subtitles_file_path['S01_E01'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(episode_1_txt[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = create_corpus(subtitles_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>📝 Exercice :</b> Nettoyer le corpus pour enlever les accents, mettre le texte en minuscule, enlever la ponctuation et les doubles espaces. Eventuellement pour le stemming.</p>\n",
    "</div>\n",
    "<hr> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/cleaning.py\n",
    "stemmer = FrenchStemmer()\n",
    "\n",
    "def clean_corpus(corpus):\n",
    "    # TODO écrire fonction qui permet de nettoyer les documents du corpuss\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corpus = clean_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corpus[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corpus_split = []\n",
    "for episode in clean_corpus:\n",
    "    episode_words = episode.split()\n",
    "    i = 0\n",
    "    while i < len(episode_words):\n",
    "        clean_corpus_split.append(' '.join(episode_words[i:i+400]))\n",
    "        i+=400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_corpus_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    tokens = []\n",
    "    for sentence in corpus.split('\\n'):\n",
    "        tokens.append(nltk.word_tokenize(sentence))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_length = [len(x.split()) for x in clean_corpus_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(sentence_length), np.std(sentence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>📝 Exercice :</b> Vectorizer le corpus en utilisant la méthode Bag-Of-Words.</p>\n",
    "</div>\n",
    "<hr> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/vectorize.py\n",
    "\n",
    "count_data = # TODO écrire fonction de vectorization du corpus pour avoir la représentation BoW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_corpus_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faire varier les paramètres ci-dessous\n",
    "number_topics = 15\n",
    "number_words = 10\n",
    "\n",
    "# Create and fit the LDA model\n",
    "lda = LDA(n_components=number_topics, n_jobs=-1)\n",
    "lda.fit(count_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, count_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "LDAvis_data_filepath = os.path.join('./ldavis_prepared_'+str(number_topics))\n",
    "LDAvis_prepared = sklearn_lda.prepare(lda, count_data, count_vectorizer, mds='mmds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "    \n",
    "pyLDAvis.save_html(LDAvis_prepared, './ldavis_prepared_'+ str(number_topics) +'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>📝 Exercice :</b> Faire varier le paramètre Lambda et justifier de son impact.</p>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>📝 Exercice :</b> Faire varier le préprocessing,en particulier la stemmatization. Analyser l'impact sur l'analyse des clusters.</p>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>📝 Exercice :</b> Etudier l'impact des Stop Words sur les topics.</p>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "> <div id=\"landauer-dumais-1997\">Landauer, Thomas K. et al. <a href=http://lsa.colorado.edu/papers/dp1.LSAintro.pdf>An introduction to latent semantic analysis.</a> Discourse Processes 25 (1998): 259-284.</div>\n",
    "\n",
    "> <div id=\"blei-2001\"> David M. Blei, Andrew Y. Ng, Michael I. Jordan: <a href=https://ai.stanford.edu/~ang/papers/nips01-lda>Latent Dirichlet Allocation.</a> NIPS 2001: 601-608</div>\n",
    " \n",
    "> <div id=\"alghamdi-2001\"> Rubayyi Alghamdi and Khalid Alfalqi: <a href=http://dx.doi.org/10.14569/IJACSA.2015.060121>A Survey of Topic Modeling in Text Mining.</a> International Journal of Advanced Computer Science and Applications(IJACSA), 6(1), 2015</div>\n",
    "\n",
    "> <div id=\"sievert-2014\"> Sievert, Carson, and Kenneth Shirley. <a href=\"https://aclanthology.org/W14-3110.pdf\">LDAvis: A method for visualizing and interpreting topics.</a> Proceedings of the workshop on interactive language learning, visualization, and interfaces. 2014.\n",
    "\n",
    "> <div id=\"chuang-2012\"> Chuang, Jason, Christopher D. Manning, and Jeffrey Heer. <a href=\"https://dl.acm.org/doi/10.1145/2254556.2254572\">Termite: Visualization techniques for assessing textual topic models.</a> Proceedings of the international working conference on advanced visual interfaces. 2012."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
