{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2 - Exploration de topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/figure2.png?raw=true\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q scikit-learn==0.23.2 pyLDAvis nltk==3.5 unidecode pysrt\n",
    "\n",
    "# Only if running in colab\n",
    "# !git clone https://github.com/AntoineSimoulin/m2-data-sciences.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "La Latent Dirichlet Allocation (LDA) <span class=\"badge badge-secondary\">(Blei et al., 2001)</span> est une méthode de <b>topic modelling</b>. C'est l'une des méthodes les plus utilisées dans ce domaine. La LDA prend en input une collection de documents et cherche à identifier les topics ou thèmes spécifiques dans l'ensemble du corpus.\n",
    "\n",
    "<span class=\"badge badge-secondary\">(Blei et al., 2001)</span> David M. Blei, Andrew Y. Ng, Michael I. Jordan: Latent Dirichlet Allocation. NIPS 2001: 601-608\n",
    "\n",
    "Par exemple, on peut considérer le corpus suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './m2-data-sciences/TP2 - Text Mining'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Nous avons pris l'avion pour aller en vacances.\",\n",
    "    \"J'ai fait de la plongée en vacances.\"\n",
    "    \"Nous avons joué au foot hier matin.\",\n",
    "    \"J'ai pris le taxi.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_documents = len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif est d'identifier les thèmes ou <i>topics</i> qui correspondent à chaque phrase. Ici, on pourrait considérer des thèmes commes les <i>vacances</i>, le <i>sport</i> ou encore les <i>moyens de transports</i>.\n",
    "\n",
    "La LDA est un modèle <b>génératif</b> qui cherche à expliquer une observation (ici le corpus) en s'appuyant sur des variables latentes (les topics). En plus de l'exploration de thèmes, ce type de modèle peut être utilisé pour des tâches non supervisées, en particulier le clustering.\n",
    "\n",
    "La LDA décrit chaque document comme une distribution de **topics**, eux mêmes caractérisés par une **distribution de mots**. En pratique, on va introduire une **variable latente** et exprimer chaque document comme une distribution de cette variable. Cette variable sera elle même décrite comme une distribution sur le vocabulaire comme décrit ci-dessous. Ainsi chaque topic n'est pas décrit explicitement. Il doit être interprété en fonction de sa distribution sur les mots du vocabulaire.\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/lda-idee.png?raw=true\" width=\"1000\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "vectorizer.fit(documents)\n",
    "\n",
    "w2idx = {w: i for (i, w) in enumerate(vectorizer.get_feature_names())}\n",
    "idx2w = {i: w for (i, w) in enumerate(vectorizer.get_feature_names())}\n",
    "\n",
    "print(len(w2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = vectorizer.build_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [[t.lower() for t in tokenizer(d)] for d in documents]\n",
    "for doc in documents:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vectorizer.get_feature_names())\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"matrice d'occurence :\")\n",
    "\n",
    "n = np.zeros((len(documents), vocab_size), dtype=int)\n",
    "for (doc_idx, doc) in enumerate(documents):\n",
    "    words_idx, words_freq = np.unique(doc, return_counts=True)\n",
    "    for (w, f) in zip(words_idx, words_freq):\n",
    "        n[doc_idx][w2idx[w]] = f\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Modélisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La séance précédente, on a vu que l'on pouvait représenter les documents comme une distribution sur les mots. Les paramètres de cette distribution peuvent être calculés simplement en utilisant la fréquence des mots dans le document (le modèle Bag-Of-Word) ou alors en pondérant les mots en fonction de leur fréquence dans l'ensemble du corpus (modèle TF-IDF).\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/bow.png?raw=true\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les modèles génératifs\n",
    "\n",
    "La LDA est un modèle de probabilité génératif. Les modèles génératifs sont générallement développés suivant les 3 étapes suivantes :\n",
    "\n",
    "1. **Observations** Les données sont considérées comme des observations générées par le modèle. La notion de données inclue les variables latentes. Ces dernières représentent la structure thématique du corpus.\n",
    "2. **Apprentissage** On met à jour les variables du modèle en utilisant l'inférence à postériori\n",
    "3. **Inférence** On peut utiliser le modèle pour situer de nouvelles de données dans la structure de topics apprise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'originalité de la LDA réside dans sa modélisation de chaque document comme une distribution de topics. La plupart des modèles considérant que tous les mots d'un document sont issus du même topic. Ainsi chaque document peut être rattaché à plusieurs topics. Dans notre corpus exemple, la LDA pourrait générer une distribution du type :\n",
    "\n",
    "* Document 1 : 50% Topic \"Transports\" + 50% Topic \"Voyages\"\n",
    "* Document 2 : 50% Topic \"Sports\" + 50% Topic \"Voyages\"\n",
    "* Document 3 : 100% Topic \"Sports\"\n",
    "* Document 4 : 100% Topic \"Transports\"\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/lda2.png?raw=true\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notations\n",
    "\n",
    "Les modèles graphiques représentent les variables aléatoies comme des noeuds. Les arcs entre les noeuds indiquent les variables potentiellement dépendantes. Les variables observées sont grisées. Dans la figure ci-dessous, les noeuds $ X_{1,...,N}$ sont observés alors que le noeud $Y$ est une variable latente. Dans cet exemple, les variables observées dépendent de cette variable latente. Les rectangles synthétisent la réplication de plusieurs structures. Un rectangle résume donc plusieurs variables $X_n$ avec $n \\in N$.\n",
    "\n",
    "La structure du graph définie les dépendances conditionnelles entre l'ensemble des variables. Par exemple dans le graph ci-dessous, on a $p(Y,X_{1},...,X_{N})=p(Y)\\prod _{n=1}^{N}p(X_{n}|Y)$.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/graphical_model.png?raw=true\" width=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La distribution de Dirichlet\n",
    "\n",
    "\n",
    "La distribution de Dirichlet est généralement notée $Dir(\\alpha)$. Il s'agit d'une famille de lois de probabilités continues pour des variables aléatoires multinomiales. Cette loi (ou encore distribution) est paramétrée par le vecteur ${\\bf \\alpha}$ de nombres réels positifs. La taille du vecteur ${\\bf \\alpha}$ indique la dimension de la distribution. Ce type de distribution est souvent utilisée comme distribution à priori dans les modèles Bayésiens. Sans rentrer dans les détails, voici quelques caraactéristiques de la distribution de Dirichlet :\n",
    "\n",
    "* La distribution est définie sur un simplex de vecteurs positifs dont la somme est égale à 1 \n",
    "* Sa densité est caractérisée par : $P(\\theta |{\\overrightarrow {\\alpha }})={\\frac {\\Gamma (\\Sigma _{i}\\alpha _{i})f}{\\Pi _{i}\\Gamma (\\alpha _{i})}}\\Pi _{i}\\theta _{i}^{\\alpha _{i}-1}$\n",
    "* En pratique, si toutes les dimensions de ${\\bf \\alpha}$ ont des valeurs similaires, la distribution est plus étalée. Elle devient plus concentrée pour des valeurs plus importantes de ${\\bf \\alpha}$.\n",
    "\n",
    "La distribution est illustrée ci-dessous pour des valeurs ${\\bf \\alpha}$ qui varient entre (6, 2, 2), (3, 7, 5), (6, 2, 6), (2, 3, 4).\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/dirichlet.png?raw=true\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise les notations suivantes :\n",
    "\n",
    "* Les mots sont l'unité de base des données. Ils sont définis comme un élément d'un vocabulaire indexé par ${1,...,V}$. On représent chaque mot en utilisant des vecteurs one-hot dont toutes les composantes sont 0, sauf pour la composante qui correspond à l'index du mot, qui vaut 1.\n",
    "* Un document est une séquence de $N$ mots telle que $W=(w_{1},w_{2},...,w_{N})$ avec $w_{n}$ le $n$th mot de la séquence.\n",
    "* Un corpus est un ensemble de $M$ documents tels que $D=\\lbrace W_{1},W_{2},...,W_{M}\\rbrace$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le principe de la LDA\n",
    "\n",
    "La LDA est un modèle génératif. L'idée de base est que chaque document est représenté comme une distribution sur $k$ topics latents. Chaque topic est caractérisé par une distribution sur les mots. On appelle $\\beta$ la matrice de dimension $k*V$ qui représente la distribution des mots sur les $k$ topics. On a ainsi $\\beta _{ij}=p(w_{j}=1|z_{i}=1)$.\n",
    "\n",
    "La LDA suppose le procéssus génératif suivant pour chaque document $W$ dans le corpus $D$.\n",
    "\n",
    "\n",
    "> 1. On choisit $\\theta \\sim Dir(\\alpha )$\n",
    "> 2. Pour chaque document dans le corpus:\n",
    ">     * Pour chacun des $N$ mots $w_{n}$ dans le document :\n",
    ">        * on génère un topic $z_{n}\\sim Multinomial(\\theta )$\n",
    ">        * on génère un mot $w_{n}$ $p(w_{n}|z_{n},B)$ selon une loi multinimoale conditionnée par le topic $z_{n}$.\n",
    "\n",
    "\n",
    "Une variable aléatoire suivant une loi de dirichlet de dimension $k$ peut prendre des valeurs dans le $k-1$-simplex. Cet espace désigne les vecteurs $\\theta$ de dimension $k$ tels que $\\theta _{i}\\geq 0$ et $\\sum _{i=1}^{k}\\theta _{i}=1$\n",
    "\n",
    "Etant donné $\\alpha$ et $\\beta$, la probabilité jointe de $\\theta$, un ensemble de $K$ topics $Z$ et un ensemble de $N$ mots est donnée par : $$p(\\theta ,Z,W|\\alpha ,\\beta )=p(\\theta |\\alpha )\\prod _{n=1}^{N}p(z_{n}|\\theta )p(w_{n}|z_{n},\\beta ),$$\n",
    "\n",
    "Avec $p(z_{n}|\\theta )$ qui représente $\\theta _{i}$ pour chaque $i$ tel que $z_{i}^{n}=1$.\n",
    "\n",
    "On peut obtenir la distribution marginale du document en itérant sur les $\\theta$ et en sommant sur les $z$:\n",
    "\n",
    "$$p(W|\\alpha ,\\beta )=\\int p(\\theta |\\alpha ){\\big (}\\prod _{n=1}^{N}\\Sigma _{z_{n}}p(z_{n}|\\theta )p(w_{n}|z_{n},\\beta {\\big )}d\\theta$$\n",
    "\n",
    "Finalement, en prenant le produit des probabilitées marginales sur un chaque document, on obtient la probabilité du corpus :\n",
    "\n",
    "\n",
    "$$p(D|\\alpha ,\\beta )=\\prod _{d=1}^{M}\\int p(\\theta _{d}|\\alpha ){\\big (}\\prod _{n=1}^{N_{d}}\\Sigma _{z_{dn}}p(z_{dn}|\\theta _{d})p(w_{dn}|z_{dn},\\beta ){\\big )}d\\theta _{d}$$\n",
    "\n",
    "La représentation garphique du modèle est proposée ci-dessous.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/lda_graph.png?raw=true\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec les notations suivantes :\n",
    "* k — Le nombre de topics qui caractérisent un document\n",
    "* V — La taille du vocabulaire\n",
    "* M — Le nombre de documents\n",
    "* N — Le nombre de mots dans chaque document\n",
    "* w — Un mot dans un document représenté par un vecteur one-hot de taille V\n",
    "* **w** — Représentation d'un document, i.e. une collection de N vecteurs w de mots\n",
    "* D — Le corpus, i.e. une collection de M documents\n",
    "* z — Un topic parmi k. Un topic correspond à une distribution de mots. Par exemple, voyage = (0.3 avion, 0.4 plongée, 0.2 vacances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Apprendre les paramètres du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce modèle permet d'expliquer comment le corpus a été généré. Néanmoins en pratique on n'observe pas toutes les variables du modèle mais seulement la distribution des mots. En pratique on va \"inverser\" le modèle pour estimer les paramètres. Ce processus est appelé inférence à posteriori.\n",
    "\n",
    "On dispose d'un corpus de documents. On a fixé un nombre $k$ de topics. Pour apprendre la représentation des topics et des documents, il existe deux méthodes principales :\n",
    "\n",
    "* Le sampling de Gibbs\n",
    "* L'inférence variationelle\n",
    "\n",
    "Nous allons détailler le procéssus du sampling de Gibbs sur notre jeu de données exemple puis utiliser la librarie `sklearn` qui repose sur l'inférence variationelle pour expérimenter sur un jeu de données plus conséquent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# IPython automatically reload all changed code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le Sampling de Gibbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour fixer les paramètres des matrices, on cherche à maximiser la vraisemblance de nos données selon ce modèle. On utilise pour cela l'algorthime de **sampling de Gibbs**. Le sampling de Gibbs est un algorithme qui permet de sélectionner des distributions conditionelles dont la distribution des états converge vers la vraie distribution à terme. En pratique, on va mettre à jour itérativement les matrices $\\Theta$ et $\\beta$ pour maximiser la vraisemblance de nos données. Les itérations s'effectuent mot par mot en ajustant le topic assigné à chaque mot.  On fait l'hypothèse que l'on ne connait pas le topic assigné à chaque mot mais qu'on connait la correpondance de topic pour tous les autres mots dans le texte et on cherche à inférer le topic à assigner pour ce mot.\n",
    "\n",
    "\n",
    "D'un point de vue mathématique, on cherche la probabilité conditionnelle pour chaque mot d'être assigné à un topic, étant donné l'attribution des autres topics. On peut montrer que peut s'écrire :\n",
    "\n",
    "$$P(z_{i,d}=k|z_{:,d},w,\\alpha,\\beta) \\propto \\frac{n_{d,k}+\\beta_k}{\\sum_i^Kn_{d,i}+\\beta_i}v_{k,w_{d,n}}+\\alpha_{w_{d,n}}$$\n",
    "\n",
    "Avec :\n",
    "* $n_{d,k}$ : le nombre de fois ou le document $d$ utilise le topic $k$\n",
    "* $v_{k,w}$ : le nombre de fois ou le topic $k$ utilise le mot $w$\n",
    "* $\\alpha_k$ : le paramètre de dirichlet pour la distribution de topics par documents\n",
    "* $\\lambda_w$ : Le paramètre de dirichlet pour la distribution des mots par topic\n",
    "\n",
    "Il y a deux parties dans cette équation. D'abord on évalue la proportion de chaque topic dans un document. Puis on réparti l'attention des topics pour chaque mot. Les paramètres de dirichlet permette de régulariser quand $n_{d,k}$ ou $v_{k,w}$ sont trop proches de 0 et qu'il y a peu de chance qu'un mot choisise un topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  1. Pour chaque document et chaque mot, assigner un topic initial au hasard\n",
    "\n",
    "words_topic = [[np.random.randint(n_topics) for _ in range(len(d))] for d in documents]\n",
    "\n",
    "print(\"assignation de topic aléatoire pour le premier document\")\n",
    "for (w, t) in zip(documents[0], words_topic[0]):\n",
    "    print(\"{:>10} -> TOPIC {:1}\".format(w, t))\n",
    "    \n",
    "print(\"matrices d'assignation des topics :\")\n",
    "words_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $n_{d,k}$ : le nombre de fois ou le document $d$ utilise le topic $k$\n",
    "* $v_{k,w}$ : le nombre de fois ou le topic $k$ utilise le mot $w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Etant donné cette distribution de topic, calculer la distribution des documents\n",
    "#    en fonction des topics et des topics en fonction des mots\n",
    "\n",
    "document_topic_counts = np.zeros((len(documents), n_topics), dtype=int)\n",
    "for (doc_idx, topics) in enumerate(words_topic):\n",
    "    topics, topics_freq = np.unique(topics, return_counts=True)\n",
    "    for (t, f) in zip(topics, topics_freq):\n",
    "        document_topic_counts[doc_idx][t] = f\n",
    "print(\"Le nombre de fois ou le document  𝑑  utilise le topic  𝑘 :\")\n",
    "print(document_topic_counts, '\\n')\n",
    "\n",
    "topic_word_counts = np.zeros((n_topics, vocab_size + 1), dtype=int)\n",
    "for (topics, doc) in zip(words_topic, documents):\n",
    "    for (t, w_idx) in zip(topics, doc):\n",
    "        topic_word_counts[t][w2idx[w_idx]] += 1\n",
    "print(\"Le nombre de fois ou le topic  𝑘  utilise le mot  𝑤 :\")\n",
    "print(topic_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nombre de mots pour chaque topic:\n",
    "np.sum(topic_word_counts, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Pour affiner les distributions définies précédemments on va itérer sur l'ensemble des documents\n",
    "#    3.1 Pour chaque mot de chaque document :\n",
    "#        On calcule la probabilité conditionnelle du mot d'être assigné à un topic en utilisant la formule ci-dessus\n",
    "#        suivant cette probabilité, on réassigne le topic du mot considéré.\n",
    "#        En fait, on considère ici que seul la probabilité du mot considéré est à ajuster et que toutes les autres sont correctes.\n",
    "#        Cette version de l'algorithme est assez simplifié pour mieux comprendre le procéssus.\n",
    "\n",
    "eta = 0.01\n",
    "alpha = 50.\n",
    "\n",
    "\n",
    "for (doc_idx, doc) in enumerate(documents):\n",
    "    for (w_idx, w) in enumerate(doc):\n",
    "        \n",
    "        \n",
    "        current_topic = words_topic[doc_idx][w_idx]\n",
    "        \n",
    "        # Mise à jour des matrices de fréquences\n",
    "        document_topic_counts[doc_idx, current_topic] -= 1\n",
    "        topic_word_counts[current_topic, w2idx[w]] -= 1\n",
    "        \n",
    "        # Change le topic\n",
    "        topic_distribution = (topic_word_counts[:, w2idx[w]] + eta) * \\\n",
    "            (document_topic_counts[doc_idx, :] + alpha) / \\\n",
    "            (np.sum(topic_word_counts, 1) + 1e-12)\n",
    "\n",
    "        new_topic = np.random.multinomial(1, topic_distribution / topic_distribution.sum()).argmax()\n",
    "        \n",
    "        # Mise à jour des matrices de fréquences\n",
    "        document_topic_counts[doc_idx][new_topic] += 1\n",
    "        topic_word_counts[new_topic, w2idx[w]] += 1\n",
    "        words_topic[doc_idx][w_idx] = new_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topic_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. On va répéter cette transformation un certain nombre de fois afin de faire converger la distribution\n",
    "#    des topics\n",
    "\n",
    "n_iters = 10\n",
    "\n",
    "for _ in range(n_iters):\n",
    "    for (doc_idx, doc) in enumerate(documents):\n",
    "        for (w_idx, w) in enumerate(doc):\n",
    "\n",
    "\n",
    "            current_topic = words_topic[doc_idx][w_idx]\n",
    "\n",
    "            # Mise à jour des matrices de fréquences\n",
    "            document_topic_counts[doc_idx, current_topic] -= 1\n",
    "            topic_word_counts[current_topic, w2idx[w]] -= 1\n",
    "\n",
    "            # Change le topic\n",
    "            topic_distribution = (topic_word_counts[:, w2idx[w]] + eta) * \\\n",
    "                (document_topic_counts[doc_idx, :] + alpha) / \\\n",
    "                (np.sum(topic_word_counts, 1) + 1e-12)\n",
    "\n",
    "            new_topic = np.random.multinomial(1, topic_distribution / topic_distribution.sum()).argmax()\n",
    "\n",
    "            # Mise à jour des matrices de fréquences\n",
    "            document_topic_counts[doc_idx][new_topic] += 1\n",
    "            topic_word_counts[new_topic, w2idx[w]] += 1\n",
    "            words_topic[doc_idx][w_idx] = new_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topic_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalement on peut évaluer les matrices $\\Theta$ et $\\beta$ selon les formules suivantes :\n",
    "\n",
    "$$\\Theta_{k,t} = \\frac{n_{d,k}+\\eta_k}{\\sum_i^Kn_{d,i}+\\eta_k}$$\n",
    "\n",
    "$$\\beta_{m,k} = \\frac{v_{k}+\\alpha_k}{\\sum_i^Kn_{d,i}+\\alpha_k}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = (topic_word_counts + alpha) / (np.sum(topic_word_counts, 0) + alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 3\n",
    "\n",
    "for topic_idx in range(n_topics):\n",
    "    message = \"Topic #%d: \" % topic_idx\n",
    "    message += \" \".join([idx2w[i] for i in beta[topic_idx].argsort()[:-n_top_words - 1:-1][1:]])\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utilisation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "import pysrt\n",
    "import re\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va chercher à analyser les thèmes de la Série Game Of Thrones. On utilise pour ça les sous-titres de l'ensemble des saisons qui ont été récupérés sur le site https://www.sous-titres.eu/series/game_of_thrones.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subtitle_file_dict(subtitles_dir):\n",
    "    \"Retourne les chemins vers les fichiers de sous titres\"\n",
    "    subtitles_file_path = {}\n",
    "    for path, subdirs, files in os.walk(subtitles_dir):\n",
    "        for name in files:\n",
    "            episode_name = '_'.join([os.path.basename(path), name.split('.')[0]])\n",
    "            subtitles_file_path[episode_name] = os.path.join(path, name)\n",
    "    return subtitles_file_path\n",
    "\n",
    "def parse_srt_file(srt_file, encoding='iso-8859-1'):\n",
    "    \"Lit un ficher de sous titres au format rst\"\n",
    "    subs = pysrt.open(srt_file, encoding=encoding)\n",
    "    text = ' '.join([' '.join(sub.text.split('\\n')) for sub in subs])\n",
    "    return text\n",
    "\n",
    "def create_corpus(subtitles_file_path):\n",
    "    \"Créer un corpus à partir de tous les fichiers rst dans un dossier\"\n",
    "    corpus = []\n",
    "    for k, v in subtitles_file_path.items():\n",
    "        if v.endswith('srt'):\n",
    "            corpus.append(parse_srt_file(v))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitles_file_path = create_subtitle_file_dict(os.path.join(data_dir, 'sous-titres-got'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_1_txt = parse_srt_file(subtitles_file_path['S01_E01'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(episode_1_txt[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = create_corpus(subtitles_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>Exercice :</b> Nettoyer le corpus pour enlever les accents, mettre le texte en minuscule, enlever la ponctuation et les doubles espaces. Eventuellement pour le stemming </p>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la solutions est sauvegardée dans le répertoire solutions/cleaning.py\n",
    "\n",
    "stemmer = FrenchStemmer()\n",
    "\n",
    "def clean_corpus(corpus):\n",
    "    for i in range(len(corpus)):\n",
    "        corpus[i] = unidecode.unidecode(corpus[i])\n",
    "        corpus[i] = re.sub(r'[^\\w\\s]', ' ', corpus[i])\n",
    "        corpus[i] = corpus[i].lower()\n",
    "        corpus[i] = re.sub(r'\\s{2,}', ' ', corpus[i])\n",
    "        # corpus[i] = ' '.join([stemmer.stem(x) for x in corpus[i].split()])\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corpus = clean_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corpus[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corpus_split = []\n",
    "for episode in clean_corpus:\n",
    "    episode_words = episode.split()\n",
    "    i = 0\n",
    "    while i < len(episode_words):\n",
    "        clean_corpus_split.append(' '.join(episode_words[i:i+400]))\n",
    "        i+=400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_corpus_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    tokens = []\n",
    "    for sentence in corpus.split('\\n'):\n",
    "        tokens.append(nltk.word_tokenize(sentence))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_length = [len(x.split()) for x in clean_corpus_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(sentence_length), np.std(sentence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>Exercice :</b> Vectorizer le corpus en utilisant la méthode Bag-Of-Words.</p>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la solutions est sauvegardée dans le répertoire solutions/vectorize.py\n",
    "\n",
    "# Initialise the count vectorizer\n",
    "count_vectorizer = CountVectorizer(max_features=2000,\n",
    "                                   stop_words=STOP_WORDS,\n",
    "                                   max_df=0.9,\n",
    "                                   min_df=20)\n",
    "\n",
    "count_data = count_vectorizer.fit_transform(clean_corpus_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_corpus_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweak the two parameters below\n",
    "number_topics = 15\n",
    "number_words = 10\n",
    "\n",
    "# Create and fit the LDA model\n",
    "lda = LDA(n_components=number_topics, n_jobs=-1)\n",
    "lda.fit(count_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, count_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyLDAvis import sklearn as sklearn_lda\n",
    "import pickle\n",
    "import pyLDAvis\n",
    "\n",
    "LDAvis_data_filepath = os.path.join('./ldavis_prepared_'+str(number_topics))\n",
    "LDAvis_prepared = sklearn_lda.prepare(lda, count_data, count_vectorizer, mds='mmds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "    \n",
    "pyLDAvis.save_html(LDAvis_prepared, './ldavis_prepared_'+ str(number_topics) +'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-secondary\">(Sievert et al., 2014)</span> Sievert, Carson, and Kenneth Shirley. \"LDAvis: A method for visualizing and interpreting topics.\" Proceedings of the workshop on interactive language learning, visualization, and interfaces. 2014.\n",
    "\n",
    "<span class=\"badge badge-secondary\">(Chuang et al., 2012)</span> Chuang, Jason, Christopher D. Manning, and Jeffrey Heer. \"Termite: Visualization techniques for assessing textual topic models.\" Proceedings of the international working conference on advanced visual interfaces. 2012."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>Exercice :</b> Faire varier le paramètre Lambda et justifier de son impact.</p>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>Exercice :</b> Faire varier le préprocessing,en particulier la stemmatization. Analyser l'impact sur l'analyse des clusters.</p>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>Exercice :</b> Etudier l'impact des Stop Words sur les topics.</p>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources :\n",
    "* https://medium.com/analytics-vidhya/topic-modeling-using-lda-and-gibbs-sampling-explained-49d49b3d1045\n",
    "* https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158\n",
    "* https://wiki.ubc.ca/Course:CPSC522/Latent_Dirichlet_Allocation\n",
    "* http://www.arbylon.net/publications/text-est2.pdf\n",
    "\n",
    "Icons made by <a href=\"https://www.flaticon.com/authors/freepik\" title=\"Freepik\">Freepik</a> and <a href=\"https://www.flaticon.com/authors/pixel-perfect\" title=\"Pixel perfect\">Pixel perfect</a> from <a href=\"https://www.flaticon.com/\" title=\"Flaticon\"> www.flaticon.com</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
