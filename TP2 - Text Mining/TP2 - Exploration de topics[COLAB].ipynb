{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2 - Exploration de topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/figure2.png?raw=true\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q scikit-learn==0.23.2 pyLDAvis nltk==3.5 unidecode pysrt\n",
    "\n",
    "# Only if running in colab\n",
    "# !git clone https://github.com/AntoineSimoulin/m2-data-sciences.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "La Latent Dirichlet Allocation (LDA) <span class=\"badge badge-secondary\">(Blei et al., 2001)</span> est une m√©thode de <b>topic modelling</b>. C'est l'une des m√©thodes les plus utilis√©es dans ce domaine. La LDA prend en input une collection de documents et cherche √† identifier les topics ou th√®mes sp√©cifiques dans l'ensemble du corpus.\n",
    "\n",
    "<span class=\"badge badge-secondary\">(Blei et al., 2001)</span> David M. Blei, Andrew Y. Ng, Michael I. Jordan: Latent Dirichlet Allocation. NIPS 2001: 601-608\n",
    "\n",
    "Par exemple, on peut consid√©rer le corpus suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './m2-data-sciences/TP2 - Text Mining'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Nous avons pris l'avion pour aller en vacances.\",\n",
    "    \"J'ai fait de la plong√©e en vacances.\"\n",
    "    \"Nous avons jou√© au foot hier matin.\",\n",
    "    \"J'ai pris le taxi.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_documents = len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif est d'identifier les th√®mes ou <i>topics</i> qui correspondent √† chaque phrase. Ici, on pourrait consid√©rer des th√®mes commes les <i>vacances</i>, le <i>sport</i> ou encore les <i>moyens de transports</i>.\n",
    "\n",
    "La LDA est un mod√®le <b>g√©n√©ratif</b> qui cherche √† expliquer une observation (ici le corpus) en s'appuyant sur des variables latentes (les topics). En plus de l'exploration de th√®mes, ce type de mod√®le peut √™tre utilis√© pour des t√¢ches non supervis√©es, en particulier le clustering.\n",
    "\n",
    "La LDA d√©crit chaque document comme une distribution de **topics**, eux m√™mes caract√©ris√©s par une **distribution de mots**. En pratique, on va introduire une **variable latente** et exprimer chaque document comme une distribution de cette variable. Cette variable sera elle m√™me d√©crite comme une distribution sur le vocabulaire comme d√©crit ci-dessous. Ainsi chaque topic n'est pas d√©crit explicitement. Il doit √™tre interpr√©t√© en fonction de sa distribution sur les mots du vocabulaire.\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/lda-idee.png?raw=true\" width=\"1000\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "vectorizer.fit(documents)\n",
    "\n",
    "w2idx = {w: i for (i, w) in enumerate(vectorizer.get_feature_names())}\n",
    "idx2w = {i: w for (i, w) in enumerate(vectorizer.get_feature_names())}\n",
    "\n",
    "print(len(w2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = vectorizer.build_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [[t.lower() for t in tokenizer(d)] for d in documents]\n",
    "for doc in documents:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vectorizer.get_feature_names())\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"matrice d'occurence :\")\n",
    "\n",
    "n = np.zeros((len(documents), vocab_size), dtype=int)\n",
    "for (doc_idx, doc) in enumerate(documents):\n",
    "    words_idx, words_freq = np.unique(doc, return_counts=True)\n",
    "    for (w, f) in zip(words_idx, words_freq):\n",
    "        n[doc_idx][w2idx[w]] = f\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mod√©lisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La s√©ance pr√©c√©dente, on a vu que l'on pouvait repr√©senter les documents comme une distribution sur les mots. Les param√®tres de cette distribution peuvent √™tre calcul√©s simplement en utilisant la fr√©quence des mots dans le document (le mod√®le Bag-Of-Word) ou alors en pond√©rant les mots en fonction de leur fr√©quence dans l'ensemble du corpus (mod√®le TF-IDF).\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/bow.png?raw=true\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les mod√®les g√©n√©ratifs\n",
    "\n",
    "La LDA est un mod√®le de probabilit√© g√©n√©ratif. Les mod√®les g√©n√©ratifs sont g√©n√©rallement d√©velopp√©s suivant les 3 √©tapes suivantes :\n",
    "\n",
    "1. **Observations** Les donn√©es sont consid√©r√©es comme des observations g√©n√©r√©es par le mod√®le. La notion de donn√©es inclue les variables latentes. Ces derni√®res repr√©sentent la structure th√©matique du corpus.\n",
    "2. **Apprentissage** On met √† jour les variables du mod√®le en utilisant l'inf√©rence √† post√©riori\n",
    "3. **Inf√©rence** On peut utiliser le mod√®le pour situer de nouvelles de donn√©es dans la structure de topics apprise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'originalit√© de la LDA r√©side dans sa mod√©lisation de chaque document comme une distribution de topics. La plupart des mod√®les consid√©rant que tous les mots d'un document sont issus du m√™me topic. Ainsi chaque document peut √™tre rattach√© √† plusieurs topics. Dans notre corpus exemple, la LDA pourrait g√©n√©rer une distribution du type :\n",
    "\n",
    "* Document 1 : 50% Topic \"Transports\" + 50% Topic \"Voyages\"\n",
    "* Document 2 : 50% Topic \"Sports\" + 50% Topic \"Voyages\"\n",
    "* Document 3 : 100% Topic \"Sports\"\n",
    "* Document 4 : 100% Topic \"Transports\"\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/lda2.png?raw=true\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notations\n",
    "\n",
    "Les mod√®les graphiques repr√©sentent les variables al√©atoies comme des noeuds. Les arcs entre les noeuds indiquent les variables potentiellement d√©pendantes. Les variables observ√©es sont gris√©es. Dans la figure ci-dessous, les noeuds $ X_{1,...,N}$ sont observ√©s alors que le noeud $Y$ est une variable latente. Dans cet exemple, les variables observ√©es d√©pendent de cette variable latente. Les rectangles synth√©tisent la r√©plication de plusieurs structures. Un rectangle r√©sume donc plusieurs variables $X_n$ avec $n \\in N$.\n",
    "\n",
    "La structure du graph d√©finie les d√©pendances conditionnelles entre l'ensemble des variables. Par exemple dans le graph ci-dessous, on a $p(Y,X_{1},...,X_{N})=p(Y)\\prod _{n=1}^{N}p(X_{n}|Y)$.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/graphical_model.png?raw=true\" width=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La distribution de Dirichlet\n",
    "\n",
    "\n",
    "La distribution de Dirichlet est g√©n√©ralement not√©e $Dir(\\alpha)$. Il s'agit d'une famille de lois de probabilit√©s continues pour des variables al√©atoires multinomiales. Cette loi (ou encore distribution) est param√©tr√©e par le vecteur ${\\bf \\alpha}$ de nombres r√©els positifs. La taille du vecteur ${\\bf \\alpha}$ indique la dimension de la distribution. Ce type de distribution est souvent utilis√©e comme distribution √† priori dans les mod√®les Bay√©siens. Sans rentrer dans les d√©tails, voici quelques caraact√©ristiques de la distribution de Dirichlet :\n",
    "\n",
    "* La distribution est d√©finie sur un simplex de vecteurs positifs dont la somme est √©gale √† 1 \n",
    "* Sa densit√© est caract√©ris√©e par : $P(\\theta |{\\overrightarrow {\\alpha }})={\\frac {\\Gamma (\\Sigma _{i}\\alpha _{i})f}{\\Pi _{i}\\Gamma (\\alpha _{i})}}\\Pi _{i}\\theta _{i}^{\\alpha _{i}-1}$\n",
    "* En pratique, si toutes les dimensions de ${\\bf \\alpha}$ ont des valeurs similaires, la distribution est plus √©tal√©e. Elle devient plus concentr√©e pour des valeurs plus importantes de ${\\bf \\alpha}$.\n",
    "\n",
    "La distribution est illustr√©e ci-dessous pour des valeurs ${\\bf \\alpha}$ qui varient entre (6, 2, 2), (3, 7, 5), (6, 2, 6), (2, 3, 4).\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/dirichlet.png?raw=true\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise les notations suivantes :\n",
    "\n",
    "* Les mots sont l'unit√© de base des donn√©es. Ils sont d√©finis comme un √©l√©ment d'un vocabulaire index√© par ${1,...,V}$. On repr√©sent chaque mot en utilisant des vecteurs one-hot dont toutes les composantes sont 0, sauf pour la composante qui correspond √† l'index du mot, qui vaut 1.\n",
    "* Un document est une s√©quence de $N$ mots telle que $W=(w_{1},w_{2},...,w_{N})$ avec $w_{n}$ le $n$th mot de la s√©quence.\n",
    "* Un corpus est un ensemble de $M$ documents tels que $D=\\lbrace W_{1},W_{2},...,W_{M}\\rbrace$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le principe de la LDA\n",
    "\n",
    "La LDA est un mod√®le g√©n√©ratif. L'id√©e de base est que chaque document est repr√©sent√© comme une distribution sur $k$ topics latents. Chaque topic est caract√©ris√© par une distribution sur les mots. On appelle $\\beta$ la matrice de dimension $k*V$ qui repr√©sente la distribution des mots sur les $k$ topics. On a ainsi $\\beta _{ij}=p(w_{j}=1|z_{i}=1)$.\n",
    "\n",
    "La LDA suppose le proc√©ssus g√©n√©ratif suivant pour chaque document $W$ dans le corpus $D$.\n",
    "\n",
    "\n",
    "> 1. On choisit $\\theta \\sim Dir(\\alpha )$\n",
    "> 2. Pour chaque document dans le corpus:\n",
    ">     * Pour chacun des $N$ mots $w_{n}$ dans le document :\n",
    ">        * on g√©n√®re un topic $z_{n}\\sim Multinomial(\\theta )$\n",
    ">        * on g√©n√®re un mot $w_{n}$ $p(w_{n}|z_{n},B)$ selon une loi multinimoale conditionn√©e par le topic $z_{n}$.\n",
    "\n",
    "\n",
    "Une variable al√©atoire suivant une loi de dirichlet de dimension $k$ peut prendre des valeurs dans le $k-1$-simplex. Cet espace d√©signe les vecteurs $\\theta$ de dimension $k$ tels que $\\theta _{i}\\geq 0$ et $\\sum _{i=1}^{k}\\theta _{i}=1$\n",
    "\n",
    "Etant donn√© $\\alpha$ et $\\beta$, la probabilit√© jointe de $\\theta$, un ensemble de $K$ topics $Z$ et un ensemble de $N$ mots est donn√©e par : $$p(\\theta ,Z,W|\\alpha ,\\beta )=p(\\theta |\\alpha )\\prod _{n=1}^{N}p(z_{n}|\\theta )p(w_{n}|z_{n},\\beta ),$$\n",
    "\n",
    "Avec $p(z_{n}|\\theta )$ qui repr√©sente $\\theta _{i}$ pour chaque $i$ tel que $z_{i}^{n}=1$.\n",
    "\n",
    "On peut obtenir la distribution marginale du document en it√©rant sur les $\\theta$ et en sommant sur les $z$:\n",
    "\n",
    "$$p(W|\\alpha ,\\beta )=\\int p(\\theta |\\alpha ){\\big (}\\prod _{n=1}^{N}\\Sigma _{z_{n}}p(z_{n}|\\theta )p(w_{n}|z_{n},\\beta {\\big )}d\\theta$$\n",
    "\n",
    "Finalement, en prenant le produit des probabilit√©es marginales sur un chaque document, on obtient la probabilit√© du corpus :\n",
    "\n",
    "\n",
    "$$p(D|\\alpha ,\\beta )=\\prod _{d=1}^{M}\\int p(\\theta _{d}|\\alpha ){\\big (}\\prod _{n=1}^{N_{d}}\\Sigma _{z_{dn}}p(z_{dn}|\\theta _{d})p(w_{dn}|z_{dn},\\beta ){\\big )}d\\theta _{d}$$\n",
    "\n",
    "La repr√©sentation garphique du mod√®le est propos√©e ci-dessous.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/AntoineSimoulin/m2-data-sciences/blob/master/TP2%20-%20Text%20Mining/figures/lda_graph.png?raw=true\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec les notations suivantes :\n",
    "* k ‚Äî Le nombre de topics qui caract√©risent un document\n",
    "* V ‚Äî La taille du vocabulaire\n",
    "* M ‚Äî Le nombre de documents\n",
    "* N ‚Äî Le nombre de mots dans chaque document\n",
    "* w ‚Äî Un mot dans un document repr√©sent√© par un vecteur one-hot de taille V\n",
    "* **w** ‚Äî Repr√©sentation d'un document, i.e. une collection de N vecteurs w de mots\n",
    "* D ‚Äî Le corpus, i.e. une collection de M documents\n",
    "* z ‚Äî Un topic parmi k. Un topic correspond √† une distribution de mots. Par exemple, voyage = (0.3 avion, 0.4 plong√©e, 0.2 vacances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Apprendre les param√®tres du mod√®le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce mod√®le permet d'expliquer comment le corpus a √©t√© g√©n√©r√©. N√©anmoins en pratique on n'observe pas toutes les variables du mod√®le mais seulement la distribution des mots. En pratique on va \"inverser\" le mod√®le pour estimer les param√®tres. Ce processus est appel√© inf√©rence √† posteriori.\n",
    "\n",
    "On dispose d'un corpus de documents. On a fix√© un nombre $k$ de topics. Pour apprendre la repr√©sentation des topics et des documents, il existe deux m√©thodes principales :\n",
    "\n",
    "* Le sampling de Gibbs\n",
    "* L'inf√©rence variationelle\n",
    "\n",
    "Nous allons d√©tailler le proc√©ssus du sampling de Gibbs sur notre jeu de donn√©es exemple puis utiliser la librarie `sklearn` qui repose sur l'inf√©rence variationelle pour exp√©rimenter sur un jeu de donn√©es plus cons√©quent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# IPython automatically reload all changed code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le Sampling de Gibbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour fixer les param√®tres des matrices, on cherche √† maximiser la vraisemblance de nos donn√©es selon ce mod√®le. On utilise pour cela l'algorthime de **sampling de Gibbs**. Le sampling de Gibbs est un algorithme qui permet de s√©lectionner des distributions conditionelles dont la distribution des √©tats converge vers la vraie distribution √† terme. En pratique, on va mettre √† jour it√©rativement les matrices $\\Theta$ et $\\beta$ pour maximiser la vraisemblance de nos donn√©es. Les it√©rations s'effectuent mot par mot en ajustant le topic assign√© √† chaque mot.  On fait l'hypoth√®se que l'on ne connait pas le topic assign√© √† chaque mot mais qu'on connait la correpondance de topic pour tous les autres mots dans le texte et on cherche √† inf√©rer le topic √† assigner pour ce mot.\n",
    "\n",
    "\n",
    "D'un point de vue math√©matique, on cherche la probabilit√© conditionnelle pour chaque mot d'√™tre assign√© √† un topic, √©tant donn√© l'attribution des autres topics. On peut montrer que peut s'√©crire :\n",
    "\n",
    "$$P(z_{i,d}=k|z_{:,d},w,\\alpha,\\beta) \\propto \\frac{n_{d,k}+\\beta_k}{\\sum_i^Kn_{d,i}+\\beta_i}v_{k,w_{d,n}}+\\alpha_{w_{d,n}}$$\n",
    "\n",
    "Avec :\n",
    "* $n_{d,k}$ : le nombre de fois ou le document $d$ utilise le topic $k$\n",
    "* $v_{k,w}$ : le nombre de fois ou le topic $k$ utilise le mot $w$\n",
    "* $\\alpha_k$ : le param√®tre de dirichlet pour la distribution de topics par documents\n",
    "* $\\lambda_w$ : Le param√®tre de dirichlet pour la distribution des mots par topic\n",
    "\n",
    "Il y a deux parties dans cette √©quation. D'abord on √©value la proportion de chaque topic dans un document. Puis on r√©parti l'attention des topics pour chaque mot. Les param√®tres de dirichlet permette de r√©gulariser quand $n_{d,k}$ ou $v_{k,w}$ sont trop proches de 0 et qu'il y a peu de chance qu'un mot choisise un topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  1. Pour chaque document et chaque mot, assigner un topic initial au hasard\n",
    "\n",
    "words_topic = [[np.random.randint(n_topics) for _ in range(len(d))] for d in documents]\n",
    "\n",
    "print(\"assignation de topic al√©atoire pour le premier document\")\n",
    "for (w, t) in zip(documents[0], words_topic[0]):\n",
    "    print(\"{:>10} -> TOPIC {:1}\".format(w, t))\n",
    "    \n",
    "print(\"matrices d'assignation des topics :\")\n",
    "words_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $n_{d,k}$ : le nombre de fois ou le document $d$ utilise le topic $k$\n",
    "* $v_{k,w}$ : le nombre de fois ou le topic $k$ utilise le mot $w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Etant donn√© cette distribution de topic, calculer la distribution des documents\n",
    "#    en fonction des topics et des topics en fonction des mots\n",
    "\n",
    "document_topic_counts = np.zeros((len(documents), n_topics), dtype=int)\n",
    "for (doc_idx, topics) in enumerate(words_topic):\n",
    "    topics, topics_freq = np.unique(topics, return_counts=True)\n",
    "    for (t, f) in zip(topics, topics_freq):\n",
    "        document_topic_counts[doc_idx][t] = f\n",
    "print(\"Le nombre de fois ou le document  ùëë  utilise le topic  ùëò :\")\n",
    "print(document_topic_counts, '\\n')\n",
    "\n",
    "topic_word_counts = np.zeros((n_topics, vocab_size + 1), dtype=int)\n",
    "for (topics, doc) in zip(words_topic, documents):\n",
    "    for (t, w_idx) in zip(topics, doc):\n",
    "        topic_word_counts[t][w2idx[w_idx]] += 1\n",
    "print(\"Le nombre de fois ou le topic  ùëò  utilise le mot  ùë§ :\")\n",
    "print(topic_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nombre de mots pour chaque topic:\n",
    "np.sum(topic_word_counts, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Pour affiner les distributions d√©finies pr√©c√©demments on va it√©rer sur l'ensemble des documents\n",
    "#    3.1 Pour chaque mot de chaque document :\n",
    "#        On calcule la probabilit√© conditionnelle du mot d'√™tre assign√© √† un topic en utilisant la formule ci-dessus\n",
    "#        suivant cette probabilit√©, on r√©assigne le topic du mot consid√©r√©.\n",
    "#        En fait, on consid√®re ici que seul la probabilit√© du mot consid√©r√© est √† ajuster et que toutes les autres sont correctes.\n",
    "#        Cette version de l'algorithme est assez simplifi√© pour mieux comprendre le proc√©ssus.\n",
    "\n",
    "eta = 0.01\n",
    "alpha = 50.\n",
    "\n",
    "\n",
    "for (doc_idx, doc) in enumerate(documents):\n",
    "    for (w_idx, w) in enumerate(doc):\n",
    "        \n",
    "        \n",
    "        current_topic = words_topic[doc_idx][w_idx]\n",
    "        \n",
    "        # Mise √† jour des matrices de fr√©quences\n",
    "        document_topic_counts[doc_idx, current_topic] -= 1\n",
    "        topic_word_counts[current_topic, w2idx[w]] -= 1\n",
    "        \n",
    "        # Change le topic\n",
    "        topic_distribution = (topic_word_counts[:, w2idx[w]] + eta) * \\\n",
    "            (document_topic_counts[doc_idx, :] + alpha) / \\\n",
    "            (np.sum(topic_word_counts, 1) + 1e-12)\n",
    "\n",
    "        new_topic = np.random.multinomial(1, topic_distribution / topic_distribution.sum()).argmax()\n",
    "        \n",
    "        # Mise √† jour des matrices de fr√©quences\n",
    "        document_topic_counts[doc_idx][new_topic] += 1\n",
    "        topic_word_counts[new_topic, w2idx[w]] += 1\n",
    "        words_topic[doc_idx][w_idx] = new_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topic_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. On va r√©p√©ter cette transformation un certain nombre de fois afin de faire converger la distribution\n",
    "#    des topics\n",
    "\n",
    "n_iters = 10\n",
    "\n",
    "for _ in range(n_iters):\n",
    "    for (doc_idx, doc) in enumerate(documents):\n",
    "        for (w_idx, w) in enumerate(doc):\n",
    "\n",
    "\n",
    "            current_topic = words_topic[doc_idx][w_idx]\n",
    "\n",
    "            # Mise √† jour des matrices de fr√©quences\n",
    "            document_topic_counts[doc_idx, current_topic] -= 1\n",
    "            topic_word_counts[current_topic, w2idx[w]] -= 1\n",
    "\n",
    "            # Change le topic\n",
    "            topic_distribution = (topic_word_counts[:, w2idx[w]] + eta) * \\\n",
    "                (document_topic_counts[doc_idx, :] + alpha) / \\\n",
    "                (np.sum(topic_word_counts, 1) + 1e-12)\n",
    "\n",
    "            new_topic = np.random.multinomial(1, topic_distribution / topic_distribution.sum()).argmax()\n",
    "\n",
    "            # Mise √† jour des matrices de fr√©quences\n",
    "            document_topic_counts[doc_idx][new_topic] += 1\n",
    "            topic_word_counts[new_topic, w2idx[w]] += 1\n",
    "            words_topic[doc_idx][w_idx] = new_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topic_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalement on peut √©valuer les matrices $\\Theta$ et $\\beta$ selon les formules suivantes :\n",
    "\n",
    "$$\\Theta_{k,t} = \\frac{n_{d,k}+\\eta_k}{\\sum_i^Kn_{d,i}+\\eta_k}$$\n",
    "\n",
    "$$\\beta_{m,k} = \\frac{v_{k}+\\alpha_k}{\\sum_i^Kn_{d,i}+\\alpha_k}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = (topic_word_counts + alpha) / (np.sum(topic_word_counts, 0) + alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 3\n",
    "\n",
    "for topic_idx in range(n_topics):\n",
    "    message = \"Topic #%d: \" % topic_idx\n",
    "    message += \" \".join([idx2w[i] for i in beta[topic_idx].argsort()[:-n_top_words - 1:-1][1:]])\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utilisation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "import pysrt\n",
    "import re\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va chercher √† analyser les th√®mes de la S√©rie Game Of Thrones. On utilise pour √ßa les sous-titres de l'ensemble des saisons qui ont √©t√© r√©cup√©r√©s sur le site https://www.sous-titres.eu/series/game_of_thrones.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subtitle_file_dict(subtitles_dir):\n",
    "    \"Retourne les chemins vers les fichiers de sous titres\"\n",
    "    subtitles_file_path = {}\n",
    "    for path, subdirs, files in os.walk(subtitles_dir):\n",
    "        for name in files:\n",
    "            episode_name = '_'.join([os.path.basename(path), name.split('.')[0]])\n",
    "            subtitles_file_path[episode_name] = os.path.join(path, name)\n",
    "    return subtitles_file_path\n",
    "\n",
    "def parse_srt_file(srt_file, encoding='iso-8859-1'):\n",
    "    \"Lit un ficher de sous titres au format rst\"\n",
    "    subs = pysrt.open(srt_file, encoding=encoding)\n",
    "    text = ' '.join([' '.join(sub.text.split('\\n')) for sub in subs])\n",
    "    return text\n",
    "\n",
    "def create_corpus(subtitles_file_path):\n",
    "    \"Cr√©er un corpus √† partir de tous les fichiers rst dans un dossier\"\n",
    "    corpus = []\n",
    "    for k, v in subtitles_file_path.items():\n",
    "        if v.endswith('srt'):\n",
    "            corpus.append(parse_srt_file(v))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitles_file_path = create_subtitle_file_dict(os.path.join(data_dir, 'sous-titres-got'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_1_txt = parse_srt_file(subtitles_file_path['S01_E01'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(episode_1_txt[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = create_corpus(subtitles_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>Exercice :</b> Nettoyer le corpus pour enlever les accents, mettre le texte en minuscule, enlever la ponctuation et les doubles espaces. Eventuellement pour le stemming </p>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la solutions est sauvegard√©e dans le r√©pertoire solutions/cleaning.py\n",
    "\n",
    "stemmer = FrenchStemmer()\n",
    "\n",
    "def clean_corpus(corpus):\n",
    "    for i in range(len(corpus)):\n",
    "        corpus[i] = unidecode.unidecode(corpus[i])\n",
    "        corpus[i] = re.sub(r'[^\\w\\s]', ' ', corpus[i])\n",
    "        corpus[i] = corpus[i].lower()\n",
    "        corpus[i] = re.sub(r'\\s{2,}', ' ', corpus[i])\n",
    "        # corpus[i] = ' '.join([stemmer.stem(x) for x in corpus[i].split()])\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corpus = clean_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corpus[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corpus_split = []\n",
    "for episode in clean_corpus:\n",
    "    episode_words = episode.split()\n",
    "    i = 0\n",
    "    while i < len(episode_words):\n",
    "        clean_corpus_split.append(' '.join(episode_words[i:i+400]))\n",
    "        i+=400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_corpus_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    tokens = []\n",
    "    for sentence in corpus.split('\\n'):\n",
    "        tokens.append(nltk.word_tokenize(sentence))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_length = [len(x.split()) for x in clean_corpus_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(sentence_length), np.std(sentence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>Exercice :</b> Vectorizer le corpus en utilisant la m√©thode Bag-Of-Words.</p>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la solutions est sauvegard√©e dans le r√©pertoire solutions/vectorize.py\n",
    "\n",
    "# Initialise the count vectorizer\n",
    "count_vectorizer = CountVectorizer(max_features=2000,\n",
    "                                   stop_words=STOP_WORDS,\n",
    "                                   max_df=0.9,\n",
    "                                   min_df=20)\n",
    "\n",
    "count_data = count_vectorizer.fit_transform(clean_corpus_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_corpus_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweak the two parameters below\n",
    "number_topics = 15\n",
    "number_words = 10\n",
    "\n",
    "# Create and fit the LDA model\n",
    "lda = LDA(n_components=number_topics, n_jobs=-1)\n",
    "lda.fit(count_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, count_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyLDAvis import sklearn as sklearn_lda\n",
    "import pickle\n",
    "import pyLDAvis\n",
    "\n",
    "LDAvis_data_filepath = os.path.join('./ldavis_prepared_'+str(number_topics))\n",
    "LDAvis_prepared = sklearn_lda.prepare(lda, count_data, count_vectorizer, mds='mmds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "    \n",
    "pyLDAvis.save_html(LDAvis_prepared, './ldavis_prepared_'+ str(number_topics) +'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-secondary\">(Sievert et al., 2014)</span> Sievert, Carson, and Kenneth Shirley. \"LDAvis: A method for visualizing and interpreting topics.\" Proceedings of the workshop on interactive language learning, visualization, and interfaces. 2014.\n",
    "\n",
    "<span class=\"badge badge-secondary\">(Chuang et al., 2012)</span> Chuang, Jason, Christopher D. Manning, and Jeffrey Heer. \"Termite: Visualization techniques for assessing textual topic models.\" Proceedings of the international working conference on advanced visual interfaces. 2012."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>Exercice :</b> Faire varier le param√®tre Lambda et justifier de son impact.</p>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>Exercice :</b> Faire varier le pr√©processing,en particulier la stemmatization. Analyser l'impact sur l'analyse des clusters.</p>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>Exercice :</b> Etudier l'impact des Stop Words sur les topics.</p>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources :\n",
    "* https://medium.com/analytics-vidhya/topic-modeling-using-lda-and-gibbs-sampling-explained-49d49b3d1045\n",
    "* https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158\n",
    "* https://wiki.ubc.ca/Course:CPSC522/Latent_Dirichlet_Allocation\n",
    "* http://www.arbylon.net/publications/text-est2.pdf\n",
    "\n",
    "Icons made by <a href=\"https://www.flaticon.com/authors/freepik\" title=\"Freepik\">Freepik</a> and <a href=\"https://www.flaticon.com/authors/pixel-perfect\" title=\"Pixel perfect\">Pixel perfect</a> from <a href=\"https://www.flaticon.com/\" title=\"Flaticon\"> www.flaticon.com</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
